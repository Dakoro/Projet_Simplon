{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x45b12155c0a75667", "trace_id": "0xa9711bf23f91a0c5514d130f5d7e42ca"}, "parent_id": null, "start_time": 1724842076898542304, "end_time": 1724842078124375033, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"6b8996e8efd1402c8289463a2fe0f010\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"How were n-tuple systems first applied to game learning and what is their main goal?\"}", "mlflow.spanOutputs": "{\"result\": \" N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.\", \"source_documents\": [{\"page_content\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. N-Tuple Systems\", \"bboxes\": \"[[{'page': '3', 'x': '58.93', 'y': '715.94', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '70.36', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '82.31', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '94.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '106.22', 'h': '115.54', 'w': '8.64'}], [{'page': '3', 'x': '430.00', 'y': '106.22', 'h': '133.04', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '118.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '130.13', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '142.09', 'h': '103.38', 'w': '8.64'}], [{'page': '3', 'x': '418.61', 'y': '142.09', 'h': '144.43', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '154.04', 'h': '140.29', 'w': '8.64'}], [{'page': '3', 'x': '457.49', 'y': '154.04', 'h': '105.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '165.68', 'h': '152.14', 'w': '8.96'}], [{'page': '3', 'x': '468.84', 'y': '165.68', 'h': '94.20', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '177.95', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '189.91', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '201.55', 'h': '227.41', 'w': '8.96'}], [{'page': '3', 'x': '543.12', 'y': '201.86', 'h': '19.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '213.82', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '225.46', 'h': '224.53', 'w': '8.96'}], [{'page': '3', 'x': '539.71', 'y': '225.77', 'h': '16.79', 'w': '8.64'}], [{'page': '3', 'x': '559.72', 'y': '225.77', 'h': '3.32', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '237.73', 'h': '207.62', 'w': '8.64'}], [{'page': '3', 'x': '522.50', 'y': '237.73', 'h': '40.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '249.51', 'h': '251.06', 'w': '8.82'}, {'page': '3', 'x': '311.98', 'y': '261.46', 'h': '21.31', 'w': '8.59'}], [{'page': '3', 'x': '336.15', 'y': '261.64', 'h': '226.89', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '273.60', 'h': '129.27', 'w': '8.64'}, {'page': '3', 'x': '441.25', 'y': '271.93', 'h': '3.49', 'w': '6.05'}, {'page': '3', 'x': '449.18', 'y': '273.60', 'h': '113.86', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '285.55', 'h': '46.38', 'w': '8.64'}], [{'page': '3', 'x': '361.84', 'y': '285.55', 'h': '192.99', 'w': '8.64'}]]\", \"text\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"13\", \"_id\": \"69534c6a-07b2-4fd2-87b0-527fd47552b2\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"metadata\": {\"pages\": \"('1', '2')\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Introduction and Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '107.53', 'y': '641.95', 'h': '396.72', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '652.86', 'h': '277.34', 'w': '8.64'}], [{'page': '1', 'x': '388.46', 'y': '652.86', 'h': '115.54', 'w': '8.64'}, {'page': '1', 'x': '107.64', 'y': '663.77', 'h': '396.36', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '674.68', 'h': '396.00', 'w': '8.64'}, {'page': '1', 'x': '107.67', 'y': '685.59', 'h': '26.85', 'w': '8.64'}], [{'page': '1', 'x': '107.69', 'y': '701.80', 'h': '396.31', 'w': '8.82'}, {'page': '1', 'x': '108.00', 'y': '712.89', 'h': '271.65', 'w': '8.64'}], [{'page': '1', 'x': '382.14', 'y': '712.89', 'h': '122.11', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '75.48', 'h': '396.34', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '86.39', 'h': '56.44', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '86.39', 'h': '334.18', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '97.30', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '108.20', 'h': '104.76', 'w': '8.64'}]]\", \"text\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"22f24231-c49d-46f8-8f6b-90244defd1a7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"metadata\": {\"text\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '482.81', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '494.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '506.72', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '518.68', 'h': '48.70', 'w': '8.64'}], [{'page': '1', 'x': '363.71', 'y': '518.68', 'h': '199.32', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '530.63', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '542.59', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '554.54', 'h': '70.18', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9f1cb79f-9152-46c9-af5f-5eb2e6eb2e10\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"metadata\": {\"pages\": \"('5', '5')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '257.47', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '269.43', 'h': '46.22', 'w': '8.64'}], [{'page': '5', 'x': '99.80', 'y': '269.43', 'h': '200.22', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '281.38', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '293.34', 'h': '80.28', 'w': '8.64'}], [{'page': '5', 'x': '133.04', 'y': '293.34', 'h': '166.98', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '305.29', 'h': '187.80', 'w': '8.64'}], [{'page': '5', 'x': '239.50', 'y': '305.29', 'h': '60.52', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '317.25', 'h': '62.13', 'w': '8.64'}]]\", \"text\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"29fe50cd-f0a8-4a63-ad7d-2149f11b30a4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '4', 'x': '48.96', 'y': '394.21', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '405.85', 'h': '199.96', 'w': '8.96'}], [{'page': '4', 'x': '58.93', 'y': '418.54', 'h': '241.09', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '430.50', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '442.45', 'h': '153.73', 'w': '8.64'}], [{'page': '4', 'x': '205.94', 'y': '442.45', 'h': '94.08', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '454.41', 'h': '95.65', 'w': '8.64'}], [{'page': '4', 'x': '150.02', 'y': '454.09', 'h': '150.00', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '466.36', 'h': '251.05', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '478.00', 'h': '251.05', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '490.27', 'h': '223.43', 'w': '8.64'}]]\", \"text\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"4d604c9d-6f9d-4da1-b222-4df1eeae3b73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"metadata\": {\"text\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"SOFTENING SYMBOL GROUNDING\", \"bboxes\": \"[[{'page': '3', 'x': '108.00', 'y': '581.75', 'h': '302.26', 'w': '8.64'}], [{'page': '3', 'x': '413.23', 'y': '581.75', 'h': '90.77', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '592.49', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '602.91', 'h': '131.64', 'w': '8.96'}], [{'page': '3', 'x': '244.10', 'y': '603.23', 'h': '259.90', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '613.65', 'h': '350.00', 'w': '9.65'}], [{'page': '3', 'x': '462.50', 'y': '613.97', 'h': '41.50', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '624.71', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '635.45', 'h': '311.56', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"2\", \"para\": \"3\", \"_id\": \"dd7af29e-9b01-4006-8f52-f196daf712a1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"metadata\": {\"text\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '3', 'x': '321.94', 'y': '582.82', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '594.45', 'h': '251.06', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '606.73', 'h': '167.22', 'w': '8.64'}], [{'page': '3', 'x': '482.10', 'y': '606.73', 'h': '80.94', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '618.68', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '630.64', 'h': '177.02', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2dd8b318-243d-4eb5-b3b4-c9b7bf1cce30\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"metadata\": {\"text\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '186.00', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '197.95', 'h': '105.07', 'w': '8.64'}], [{'page': '5', 'x': '157.24', 'y': '197.95', 'h': '142.78', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '209.91', 'h': '156.81', 'w': '8.64'}], [{'page': '5', 'x': '209.51', 'y': '209.91', 'h': '90.51', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '221.50', 'h': '240.44', 'w': '9.01'}], [{'page': '5', 'x': '293.39', 'y': '221.87', 'h': '6.64', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '233.82', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '245.41', 'h': '198.89', 'w': '9.01'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"f9dbab16-ca23-4ced-aec9-96f0c89b23f5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0x22d82acc3fb8d98c", "trace_id": "0xa9711bf23f91a0c5514d130f5d7e42ca"}, "parent_id": "0x45b12155c0a75667", "start_time": 1724842076902625600, "end_time": 1724842076927443280, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"6b8996e8efd1402c8289463a2fe0f010\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"How were n-tuple systems first applied to game learning and what is their main goal?\"", "mlflow.spanOutputs": "[{\"page_content\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. N-Tuple Systems\", \"bboxes\": \"[[{'page': '3', 'x': '58.93', 'y': '715.94', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '70.36', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '82.31', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '94.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '106.22', 'h': '115.54', 'w': '8.64'}], [{'page': '3', 'x': '430.00', 'y': '106.22', 'h': '133.04', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '118.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '130.13', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '142.09', 'h': '103.38', 'w': '8.64'}], [{'page': '3', 'x': '418.61', 'y': '142.09', 'h': '144.43', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '154.04', 'h': '140.29', 'w': '8.64'}], [{'page': '3', 'x': '457.49', 'y': '154.04', 'h': '105.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '165.68', 'h': '152.14', 'w': '8.96'}], [{'page': '3', 'x': '468.84', 'y': '165.68', 'h': '94.20', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '177.95', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '189.91', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '201.55', 'h': '227.41', 'w': '8.96'}], [{'page': '3', 'x': '543.12', 'y': '201.86', 'h': '19.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '213.82', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '225.46', 'h': '224.53', 'w': '8.96'}], [{'page': '3', 'x': '539.71', 'y': '225.77', 'h': '16.79', 'w': '8.64'}], [{'page': '3', 'x': '559.72', 'y': '225.77', 'h': '3.32', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '237.73', 'h': '207.62', 'w': '8.64'}], [{'page': '3', 'x': '522.50', 'y': '237.73', 'h': '40.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '249.51', 'h': '251.06', 'w': '8.82'}, {'page': '3', 'x': '311.98', 'y': '261.46', 'h': '21.31', 'w': '8.59'}], [{'page': '3', 'x': '336.15', 'y': '261.64', 'h': '226.89', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '273.60', 'h': '129.27', 'w': '8.64'}, {'page': '3', 'x': '441.25', 'y': '271.93', 'h': '3.49', 'w': '6.05'}, {'page': '3', 'x': '449.18', 'y': '273.60', 'h': '113.86', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '285.55', 'h': '46.38', 'w': '8.64'}], [{'page': '3', 'x': '361.84', 'y': '285.55', 'h': '192.99', 'w': '8.64'}]]\", \"text\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"13\", \"_id\": \"69534c6a-07b2-4fd2-87b0-527fd47552b2\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"metadata\": {\"pages\": \"('1', '2')\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Introduction and Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '107.53', 'y': '641.95', 'h': '396.72', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '652.86', 'h': '277.34', 'w': '8.64'}], [{'page': '1', 'x': '388.46', 'y': '652.86', 'h': '115.54', 'w': '8.64'}, {'page': '1', 'x': '107.64', 'y': '663.77', 'h': '396.36', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '674.68', 'h': '396.00', 'w': '8.64'}, {'page': '1', 'x': '107.67', 'y': '685.59', 'h': '26.85', 'w': '8.64'}], [{'page': '1', 'x': '107.69', 'y': '701.80', 'h': '396.31', 'w': '8.82'}, {'page': '1', 'x': '108.00', 'y': '712.89', 'h': '271.65', 'w': '8.64'}], [{'page': '1', 'x': '382.14', 'y': '712.89', 'h': '122.11', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '75.48', 'h': '396.34', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '86.39', 'h': '56.44', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '86.39', 'h': '334.18', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '97.30', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '108.20', 'h': '104.76', 'w': '8.64'}]]\", \"text\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"22f24231-c49d-46f8-8f6b-90244defd1a7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"metadata\": {\"text\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '482.81', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '494.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '506.72', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '518.68', 'h': '48.70', 'w': '8.64'}], [{'page': '1', 'x': '363.71', 'y': '518.68', 'h': '199.32', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '530.63', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '542.59', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '554.54', 'h': '70.18', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9f1cb79f-9152-46c9-af5f-5eb2e6eb2e10\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"metadata\": {\"pages\": \"('5', '5')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '257.47', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '269.43', 'h': '46.22', 'w': '8.64'}], [{'page': '5', 'x': '99.80', 'y': '269.43', 'h': '200.22', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '281.38', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '293.34', 'h': '80.28', 'w': '8.64'}], [{'page': '5', 'x': '133.04', 'y': '293.34', 'h': '166.98', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '305.29', 'h': '187.80', 'w': '8.64'}], [{'page': '5', 'x': '239.50', 'y': '305.29', 'h': '60.52', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '317.25', 'h': '62.13', 'w': '8.64'}]]\", \"text\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"29fe50cd-f0a8-4a63-ad7d-2149f11b30a4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '4', 'x': '48.96', 'y': '394.21', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '405.85', 'h': '199.96', 'w': '8.96'}], [{'page': '4', 'x': '58.93', 'y': '418.54', 'h': '241.09', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '430.50', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '442.45', 'h': '153.73', 'w': '8.64'}], [{'page': '4', 'x': '205.94', 'y': '442.45', 'h': '94.08', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '454.41', 'h': '95.65', 'w': '8.64'}], [{'page': '4', 'x': '150.02', 'y': '454.09', 'h': '150.00', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '466.36', 'h': '251.05', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '478.00', 'h': '251.05', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '490.27', 'h': '223.43', 'w': '8.64'}]]\", \"text\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"4d604c9d-6f9d-4da1-b222-4df1eeae3b73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"metadata\": {\"text\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"SOFTENING SYMBOL GROUNDING\", \"bboxes\": \"[[{'page': '3', 'x': '108.00', 'y': '581.75', 'h': '302.26', 'w': '8.64'}], [{'page': '3', 'x': '413.23', 'y': '581.75', 'h': '90.77', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '592.49', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '602.91', 'h': '131.64', 'w': '8.96'}], [{'page': '3', 'x': '244.10', 'y': '603.23', 'h': '259.90', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '613.65', 'h': '350.00', 'w': '9.65'}], [{'page': '3', 'x': '462.50', 'y': '613.97', 'h': '41.50', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '624.71', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '635.45', 'h': '311.56', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"2\", \"para\": \"3\", \"_id\": \"dd7af29e-9b01-4006-8f52-f196daf712a1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"metadata\": {\"text\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '3', 'x': '321.94', 'y': '582.82', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '594.45', 'h': '251.06', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '606.73', 'h': '167.22', 'w': '8.64'}], [{'page': '3', 'x': '482.10', 'y': '606.73', 'h': '80.94', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '618.68', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '630.64', 'h': '177.02', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2dd8b318-243d-4eb5-b3b4-c9b7bf1cce30\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"metadata\": {\"text\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '186.00', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '197.95', 'h': '105.07', 'w': '8.64'}], [{'page': '5', 'x': '157.24', 'y': '197.95', 'h': '142.78', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '209.91', 'h': '156.81', 'w': '8.64'}], [{'page': '5', 'x': '209.51', 'y': '209.91', 'h': '90.51', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '221.50', 'h': '240.44', 'w': '9.01'}], [{'page': '5', 'x': '293.39', 'y': '221.87', 'h': '6.64', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '233.82', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '245.41', 'h': '198.89', 'w': '9.01'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"f9dbab16-ca23-4ced-aec9-96f0c89b23f5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0xb517cd14212f4cb5", "trace_id": "0xa9711bf23f91a0c5514d130f5d7e42ca"}, "parent_id": "0x45b12155c0a75667", "start_time": 1724842076928361094, "end_time": 1724842078123588005, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"6b8996e8efd1402c8289463a2fe0f010\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. N-Tuple Systems\", \"bboxes\": \"[[{'page': '3', 'x': '58.93', 'y': '715.94', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '70.36', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '82.31', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '94.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '106.22', 'h': '115.54', 'w': '8.64'}], [{'page': '3', 'x': '430.00', 'y': '106.22', 'h': '133.04', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '118.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '130.13', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '142.09', 'h': '103.38', 'w': '8.64'}], [{'page': '3', 'x': '418.61', 'y': '142.09', 'h': '144.43', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '154.04', 'h': '140.29', 'w': '8.64'}], [{'page': '3', 'x': '457.49', 'y': '154.04', 'h': '105.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '165.68', 'h': '152.14', 'w': '8.96'}], [{'page': '3', 'x': '468.84', 'y': '165.68', 'h': '94.20', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '177.95', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '189.91', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '201.55', 'h': '227.41', 'w': '8.96'}], [{'page': '3', 'x': '543.12', 'y': '201.86', 'h': '19.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '213.82', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '225.46', 'h': '224.53', 'w': '8.96'}], [{'page': '3', 'x': '539.71', 'y': '225.77', 'h': '16.79', 'w': '8.64'}], [{'page': '3', 'x': '559.72', 'y': '225.77', 'h': '3.32', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '237.73', 'h': '207.62', 'w': '8.64'}], [{'page': '3', 'x': '522.50', 'y': '237.73', 'h': '40.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '249.51', 'h': '251.06', 'w': '8.82'}, {'page': '3', 'x': '311.98', 'y': '261.46', 'h': '21.31', 'w': '8.59'}], [{'page': '3', 'x': '336.15', 'y': '261.64', 'h': '226.89', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '273.60', 'h': '129.27', 'w': '8.64'}, {'page': '3', 'x': '441.25', 'y': '271.93', 'h': '3.49', 'w': '6.05'}, {'page': '3', 'x': '449.18', 'y': '273.60', 'h': '113.86', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '285.55', 'h': '46.38', 'w': '8.64'}], [{'page': '3', 'x': '361.84', 'y': '285.55', 'h': '192.99', 'w': '8.64'}]]\", \"text\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"13\", \"_id\": \"69534c6a-07b2-4fd2-87b0-527fd47552b2\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"metadata\": {\"pages\": \"('1', '2')\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Introduction and Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '107.53', 'y': '641.95', 'h': '396.72', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '652.86', 'h': '277.34', 'w': '8.64'}], [{'page': '1', 'x': '388.46', 'y': '652.86', 'h': '115.54', 'w': '8.64'}, {'page': '1', 'x': '107.64', 'y': '663.77', 'h': '396.36', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '674.68', 'h': '396.00', 'w': '8.64'}, {'page': '1', 'x': '107.67', 'y': '685.59', 'h': '26.85', 'w': '8.64'}], [{'page': '1', 'x': '107.69', 'y': '701.80', 'h': '396.31', 'w': '8.82'}, {'page': '1', 'x': '108.00', 'y': '712.89', 'h': '271.65', 'w': '8.64'}], [{'page': '1', 'x': '382.14', 'y': '712.89', 'h': '122.11', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '75.48', 'h': '396.34', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '86.39', 'h': '56.44', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '86.39', 'h': '334.18', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '97.30', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '108.20', 'h': '104.76', 'w': '8.64'}]]\", \"text\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"22f24231-c49d-46f8-8f6b-90244defd1a7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"metadata\": {\"text\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '482.81', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '494.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '506.72', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '518.68', 'h': '48.70', 'w': '8.64'}], [{'page': '1', 'x': '363.71', 'y': '518.68', 'h': '199.32', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '530.63', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '542.59', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '554.54', 'h': '70.18', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9f1cb79f-9152-46c9-af5f-5eb2e6eb2e10\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"metadata\": {\"pages\": \"('5', '5')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '257.47', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '269.43', 'h': '46.22', 'w': '8.64'}], [{'page': '5', 'x': '99.80', 'y': '269.43', 'h': '200.22', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '281.38', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '293.34', 'h': '80.28', 'w': '8.64'}], [{'page': '5', 'x': '133.04', 'y': '293.34', 'h': '166.98', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '305.29', 'h': '187.80', 'w': '8.64'}], [{'page': '5', 'x': '239.50', 'y': '305.29', 'h': '60.52', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '317.25', 'h': '62.13', 'w': '8.64'}]]\", \"text\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"29fe50cd-f0a8-4a63-ad7d-2149f11b30a4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '4', 'x': '48.96', 'y': '394.21', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '405.85', 'h': '199.96', 'w': '8.96'}], [{'page': '4', 'x': '58.93', 'y': '418.54', 'h': '241.09', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '430.50', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '442.45', 'h': '153.73', 'w': '8.64'}], [{'page': '4', 'x': '205.94', 'y': '442.45', 'h': '94.08', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '454.41', 'h': '95.65', 'w': '8.64'}], [{'page': '4', 'x': '150.02', 'y': '454.09', 'h': '150.00', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '466.36', 'h': '251.05', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '478.00', 'h': '251.05', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '490.27', 'h': '223.43', 'w': '8.64'}]]\", \"text\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"4d604c9d-6f9d-4da1-b222-4df1eeae3b73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"metadata\": {\"text\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"SOFTENING SYMBOL GROUNDING\", \"bboxes\": \"[[{'page': '3', 'x': '108.00', 'y': '581.75', 'h': '302.26', 'w': '8.64'}], [{'page': '3', 'x': '413.23', 'y': '581.75', 'h': '90.77', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '592.49', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '602.91', 'h': '131.64', 'w': '8.96'}], [{'page': '3', 'x': '244.10', 'y': '603.23', 'h': '259.90', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '613.65', 'h': '350.00', 'w': '9.65'}], [{'page': '3', 'x': '462.50', 'y': '613.97', 'h': '41.50', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '624.71', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '635.45', 'h': '311.56', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"2\", \"para\": \"3\", \"_id\": \"dd7af29e-9b01-4006-8f52-f196daf712a1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"metadata\": {\"text\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '3', 'x': '321.94', 'y': '582.82', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '594.45', 'h': '251.06', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '606.73', 'h': '167.22', 'w': '8.64'}], [{'page': '3', 'x': '482.10', 'y': '606.73', 'h': '80.94', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '618.68', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '630.64', 'h': '177.02', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2dd8b318-243d-4eb5-b3b4-c9b7bf1cce30\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"metadata\": {\"text\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '186.00', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '197.95', 'h': '105.07', 'w': '8.64'}], [{'page': '5', 'x': '157.24', 'y': '197.95', 'h': '142.78', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '209.91', 'h': '156.81', 'w': '8.64'}], [{'page': '5', 'x': '209.51', 'y': '209.91', 'h': '90.51', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '221.50', 'h': '240.44', 'w': '9.01'}], [{'page': '5', 'x': '293.39', 'y': '221.87', 'h': '6.64', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '233.82', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '245.41', 'h': '198.89', 'w': '9.01'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"f9dbab16-ca23-4ced-aec9-96f0c89b23f5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"How were n-tuple systems first applied to game learning and what is their main goal?\"}", "mlflow.spanOutputs": "{\"output_text\": \" N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0xa714cea0f8708161", "trace_id": "0xa9711bf23f91a0c5514d130f5d7e42ca"}, "parent_id": "0xb517cd14212f4cb5", "start_time": 1724842076929606669, "end_time": 1724842078123315660, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"6b8996e8efd1402c8289463a2fe0f010\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"How were n-tuple systems first applied to game learning and what is their main goal?\", \"context\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\\n\\nWhile self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\\n\\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\\n\\nIn this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\\n\\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\\n\\nThe hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\\n\\nThis FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\\n\\nEither problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\\n\\nThe goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\\n\\nWe use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\"}", "mlflow.spanOutputs": "{\"text\": \" N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.\"}"}, "events": [{"name": "text", "timestamp": 1724842076929705, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nN-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\n\nWhile self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\n\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\n\nIn this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\n\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\n\nThe hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\n\nThis FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\u03bb) with arbitrary \u03bb \u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\n\nEither problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\n\nThe goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\n\nWe use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\n\nQuestion: How were n-tuple systems first applied to game learning and what is their main goal?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x382552f37d5f630e", "trace_id": "0xa9711bf23f91a0c5514d130f5d7e42ca"}, "parent_id": "0xa714cea0f8708161", "start_time": 1724842076930259733, "end_time": 1724842078123012962, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"6b8996e8efd1402c8289463a2fe0f010\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nN-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\\n\\nWhile self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\\n\\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\\n\\nIn this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\\n\\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\\n\\nThe hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\\n\\nThis FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\\n\\nEither problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\\n\\nThe goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\\n\\nWe use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\\n\\nQuestion: How were n-tuple systems first applied to game learning and what is their main goal?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 1227, \"completion_tokens\": 67, \"total_tokens\": 1294}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"How were n-tuple systems first applied to game learning and what is their main goal?\"}", "response": "{\"result\": \" N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.\", \"source_documents\": [{\"page_content\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. N-Tuple Systems\", \"bboxes\": \"[[{'page': '3', 'x': '58.93', 'y': '715.94', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '70.36', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '82.31', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '94.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '106.22', 'h': '115.54', 'w': '8.64'}], [{'page': '3', 'x': '430.00', 'y': '106.22', 'h': '133.04', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '118.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '130.13', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '142.09', 'h': '103.38', 'w': '8.64'}], [{'page': '3', 'x': '418.61', 'y': '142.09', 'h': '144.43', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '154.04', 'h': '140.29', 'w': '8.64'}], [{'page': '3', 'x': '457.49', 'y': '154.04', 'h': '105.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '165.68', 'h': '152.14', 'w': '8.96'}], [{'page': '3', 'x': '468.84', 'y': '165.68', 'h': '94.20', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '177.95', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '189.91', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '201.55', 'h': '227.41', 'w': '8.96'}], [{'page': '3', 'x': '543.12', 'y': '201.86', 'h': '19.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '213.82', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '225.46', 'h': '224.53', 'w': '8.96'}], [{'page': '3', 'x': '539.71', 'y': '225.77', 'h': '16.79', 'w': '8.64'}], [{'page': '3', 'x': '559.72', 'y': '225.77', 'h': '3.32', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '237.73', 'h': '207.62', 'w': '8.64'}], [{'page': '3', 'x': '522.50', 'y': '237.73', 'h': '40.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '249.51', 'h': '251.06', 'w': '8.82'}, {'page': '3', 'x': '311.98', 'y': '261.46', 'h': '21.31', 'w': '8.59'}], [{'page': '3', 'x': '336.15', 'y': '261.64', 'h': '226.89', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '273.60', 'h': '129.27', 'w': '8.64'}, {'page': '3', 'x': '441.25', 'y': '271.93', 'h': '3.49', 'w': '6.05'}, {'page': '3', 'x': '449.18', 'y': '273.60', 'h': '113.86', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '285.55', 'h': '46.38', 'w': '8.64'}], [{'page': '3', 'x': '361.84', 'y': '285.55', 'h': '192.99', 'w': '8.64'}]]\", \"text\": \"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"13\", \"_id\": \"69534c6a-07b2-4fd2-87b0-527fd47552b2\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"metadata\": {\"pages\": \"('1', '2')\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Introduction and Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '107.53', 'y': '641.95', 'h': '396.72', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '652.86', 'h': '277.34', 'w': '8.64'}], [{'page': '1', 'x': '388.46', 'y': '652.86', 'h': '115.54', 'w': '8.64'}, {'page': '1', 'x': '107.64', 'y': '663.77', 'h': '396.36', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '674.68', 'h': '396.00', 'w': '8.64'}, {'page': '1', 'x': '107.67', 'y': '685.59', 'h': '26.85', 'w': '8.64'}], [{'page': '1', 'x': '107.69', 'y': '701.80', 'h': '396.31', 'w': '8.82'}, {'page': '1', 'x': '108.00', 'y': '712.89', 'h': '271.65', 'w': '8.64'}], [{'page': '1', 'x': '382.14', 'y': '712.89', 'h': '122.11', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '75.48', 'h': '396.34', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '86.39', 'h': '56.44', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '86.39', 'h': '334.18', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '97.30', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '108.20', 'h': '104.76', 'w': '8.64'}]]\", \"text\": \"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"22f24231-c49d-46f8-8f6b-90244defd1a7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"metadata\": {\"text\": \"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '482.81', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '494.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '506.72', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '518.68', 'h': '48.70', 'w': '8.64'}], [{'page': '1', 'x': '363.71', 'y': '518.68', 'h': '199.32', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '530.63', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '542.59', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '554.54', 'h': '70.18', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9f1cb79f-9152-46c9-af5f-5eb2e6eb2e10\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"metadata\": {\"pages\": \"('5', '5')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '257.47', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '269.43', 'h': '46.22', 'w': '8.64'}], [{'page': '5', 'x': '99.80', 'y': '269.43', 'h': '200.22', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '281.38', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '293.34', 'h': '80.28', 'w': '8.64'}], [{'page': '5', 'x': '133.04', 'y': '293.34', 'h': '166.98', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '305.29', 'h': '187.80', 'w': '8.64'}], [{'page': '5', 'x': '239.50', 'y': '305.29', 'h': '60.52', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '317.25', 'h': '62.13', 'w': '8.64'}]]\", \"text\": \"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"29fe50cd-f0a8-4a63-ad7d-2149f11b30a4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '4', 'x': '48.96', 'y': '394.21', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '405.85', 'h': '199.96', 'w': '8.96'}], [{'page': '4', 'x': '58.93', 'y': '418.54', 'h': '241.09', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '430.50', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '442.45', 'h': '153.73', 'w': '8.64'}], [{'page': '4', 'x': '205.94', 'y': '442.45', 'h': '94.08', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '454.41', 'h': '95.65', 'w': '8.64'}], [{'page': '4', 'x': '150.02', 'y': '454.09', 'h': '150.00', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '466.36', 'h': '251.05', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '478.00', 'h': '251.05', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '490.27', 'h': '223.43', 'w': '8.64'}]]\", \"text\": \"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\\u03bb) with arbitrary \\u03bb \\u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"4d604c9d-6f9d-4da1-b222-4df1eeae3b73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"metadata\": {\"text\": \"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \\u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"SOFTENING SYMBOL GROUNDING\", \"bboxes\": \"[[{'page': '3', 'x': '108.00', 'y': '581.75', 'h': '302.26', 'w': '8.64'}], [{'page': '3', 'x': '413.23', 'y': '581.75', 'h': '90.77', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '592.49', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '602.91', 'h': '131.64', 'w': '8.96'}], [{'page': '3', 'x': '244.10', 'y': '603.23', 'h': '259.90', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '613.65', 'h': '350.00', 'w': '9.65'}], [{'page': '3', 'x': '462.50', 'y': '613.97', 'h': '41.50', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '624.71', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '635.45', 'h': '311.56', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"2\", \"para\": \"3\", \"_id\": \"dd7af29e-9b01-4006-8f52-f196daf712a1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"metadata\": {\"text\": \"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. TD Learning and FARL\", \"bboxes\": \"[[{'page': '3', 'x': '321.94', 'y': '582.82', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '594.45', 'h': '251.06', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '606.73', 'h': '167.22', 'w': '8.64'}], [{'page': '3', 'x': '482.10', 'y': '606.73', 'h': '80.94', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '618.68', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '630.64', 'h': '177.02', 'w': '8.64'}]]\", \"pages\": \"('3', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2dd8b318-243d-4eb5-b3b4-c9b7bf1cce30\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"metadata\": {\"text\": \"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Common Settings\", \"bboxes\": \"[[{'page': '5', 'x': '58.93', 'y': '186.00', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '197.95', 'h': '105.07', 'w': '8.64'}], [{'page': '5', 'x': '157.24', 'y': '197.95', 'h': '142.78', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '209.91', 'h': '156.81', 'w': '8.64'}], [{'page': '5', 'x': '209.51', 'y': '209.91', 'h': '90.51', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '221.50', 'h': '240.44', 'w': '9.01'}], [{'page': '5', 'x': '293.39', 'y': '221.87', 'h': '6.64', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '233.82', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '245.41', 'h': '198.89', 'w': '9.01'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"f9dbab16-ca23-4ced-aec9-96f0c89b23f5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}