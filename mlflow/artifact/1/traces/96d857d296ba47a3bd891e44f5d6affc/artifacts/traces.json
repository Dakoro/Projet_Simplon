{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0xf167308fc1d68292", "trace_id": "0x486dcfb360429241b62450df369905dd"}, "parent_id": null, "start_time": 1724841660337583304, "end_time": 1724841661370802975, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"96d857d296ba47a3bd891e44f5d6affc\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\"}", "mlflow.spanOutputs": "{\"result\": \" The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \\u2264 7 when using alpha-beta search and the Minimax algorithm.\", \"source_documents\": [{\"page_content\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"III. EXPERIMENTAL SETUP\", \"bboxes\": \"[[{'page': '4', 'x': '321.94', 'y': '619.92', 'h': '241.09', 'w': '8.82'}, {'page': '4', 'x': '311.98', 'y': '632.06', 'h': '98.21', 'w': '8.64'}], [{'page': '4', 'x': '414.24', 'y': '632.06', 'h': '148.79', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '644.01', 'h': '34.60', 'w': '8.64'}], [{'page': '4', 'x': '350.29', 'y': '643.69', 'h': '152.20', 'w': '8.96'}, {'page': '4', 'x': '502.50', 'y': '642.12', 'h': '7.94', 'w': '6.12'}, {'page': '4', 'x': '514.66', 'y': '644.01', 'h': '48.38', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '655.65', 'h': '194.38', 'w': '8.96'}, {'page': '4', 'x': '506.36', 'y': '654.07', 'h': '6.77', 'w': '6.12'}, {'page': '4', 'x': '516.47', 'y': '655.97', 'h': '46.56', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '667.92', 'h': '80.20', 'w': '8.64'}]]\", \"text\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"8c21f839-c634-4e86-9ea7-b4e44fbf28bc\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games\", \"section_title\": \"The Approximation Guarantee\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '410.49', 'h': '190.29', 'w': '9.96'}], [{'page': '7', 'x': '301.08', 'y': '410.49', 'h': '73.15', 'w': '9.96'}, {'page': '7', 'x': '377.04', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '386.40', 'y': '410.49', 'h': '62.26', 'w': '9.96'}, {'page': '7', 'x': '451.44', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '460.92', 'y': '410.95', 'h': '43.14', 'w': '9.10'}]]\", \"text\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.01795.pdf\", \"section_number\": \"5.1\", \"para\": \"1\", \"_id\": \"0548ecd0-5330-4a0b-a7ad-d48853603e96\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"metadata\": {\"pages\": \"('22', '22')\", \"paper_title\": \"From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains\", \"section_title\": \"Algorithm 2 Logistic Bernoulli Factory\", \"bboxes\": \"[[{'page': '22', 'x': '145.68', 'y': '482.37', 'h': '170.46', 'w': '9.96'}, {'page': '22', 'x': '334.32', 'y': '480.65', 'h': '3.97', 'w': '6.97'}, {'page': '22', 'x': '330.36', 'y': '487.16', 'h': '6.12', 'w': '4.42'}, {'page': '22', 'x': '330.36', 'y': '488.35', 'h': '19.81', 'w': '8.15'}, {'page': '22', 'x': '351.84', 'y': '482.37', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '382.92', 'y': '482.37', 'h': '94.67', 'w': '10.33'}, {'page': '22', 'x': '133.80', 'y': '496.53', 'h': '273.60', 'w': '9.96'}], [{'page': '22', 'x': '411.00', 'y': '496.53', 'h': '66.61', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '508.41', 'h': '190.98', 'w': '10.33'}, {'page': '22', 'x': '336.84', 'y': '505.99', 'h': '7.06', 'w': '6.40'}, {'page': '22', 'x': '336.84', 'y': '513.91', 'h': '12.85', 'w': '6.40'}, {'page': '22', 'x': '351.96', 'y': '508.41', 'h': '107.88', 'w': '10.33'}], [{'page': '22', 'x': '463.68', 'y': '508.41', 'h': '13.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '520.41', 'h': '343.62', 'w': '9.96'}, {'page': '22', 'x': '168.60', 'y': '532.41', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '199.68', 'y': '532.41', 'h': '277.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '547.77', 'h': '17.28', 'w': '9.96'}], [{'page': '22', 'x': '154.92', 'y': '547.77', 'h': '322.54', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '559.77', 'h': '326.49', 'w': '9.96'}]]\", \"text\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1912.09229.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"c49ea3dc-7fc6-44c3-8787-5a4d3197ab9c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"metadata\": {\"text\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"paper_title\": \"MULTI-STRUCTURAL GAMES AND BEYOND\", \"section_title\": \"The Syntactic Game\", \"bboxes\": \"[[{'page': '29', 'x': '90.00', 'y': '219.82', 'h': '121.87', 'w': '10.91'}, {'page': '29', 'x': '211.92', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '214.80', 'y': '219.82', 'h': '11.96', 'w': '10.91'}, {'page': '29', 'x': '227.16', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '229.92', 'y': '219.82', 'h': '16.44', 'w': '10.91'}, {'page': '29', 'x': '246.72', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '257.16', 'y': '219.82', 'h': '202.46', 'w': '10.91'}], [{'page': '29', 'x': '464.52', 'y': '219.82', 'h': '57.55', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '233.02', 'h': '4.92', 'w': '10.91'}, {'page': '29', 'x': '95.28', 'y': '231.26', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '101.04', 'y': '233.02', 'h': '420.90', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '245.98', 'h': '26.54', 'w': '10.91'}], [{'page': '29', 'x': '122.64', 'y': '245.98', 'h': '399.67', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '258.94', 'h': '303.19', 'w': '10.91'}, {'page': '29', 'x': '393.24', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '399.84', 'y': '258.94', 'h': '28.64', 'w': '10.91'}, {'page': '29', 'x': '428.88', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '431.64', 'y': '258.94', 'h': '3.02', 'w': '10.91'}], [{'page': '29', 'x': '440.40', 'y': '258.94', 'h': '81.41', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '271.90', 'h': '279.74', 'w': '10.91'}], [{'page': '29', 'x': '374.76', 'y': '271.90', 'h': '147.05', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '284.86', 'h': '109.82', 'w': '10.91'}], [{'page': '29', 'x': '206.04', 'y': '284.86', 'h': '316.15', 'w': '10.91'}, {'page': '29', 'x': '87.24', 'y': '297.82', 'h': '326.96', 'w': '10.91'}, {'page': '29', 'x': '414.24', 'y': '302.07', 'h': '17.14', 'w': '8.19'}, {'page': '29', 'x': '435.24', 'y': '297.82', 'h': '86.57', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '310.78', 'h': '19.82', 'w': '10.91'}], [{'page': '29', 'x': '118.20', 'y': '310.78', 'h': '404.03', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '323.74', 'h': '342.02', 'w': '10.91'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2301.13329.pdf\", \"section_number\": \"6.\", \"para\": \"6\", \"_id\": \"5a24fce9-eab2-4c1d-8e51-1df08c9f7536\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"metadata\": {\"text\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"paper_title\": \"Quantitative entanglement witnesses of Isotropic-and Werner-class via local measurements\", \"section_title\": \"III. LOCAL DECOMPOSITION OF QEW\", \"bboxes\": \"[[{'page': '4', 'x': '448.61', 'y': '52.70', 'h': '6.28', 'w': '6.31'}, {'page': '4', 'x': '317.01', 'y': '245.45', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '256.91', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '268.37', 'h': '245.08', 'w': '9.65'}, {'page': '4', 'x': '320.89', 'y': '279.82', 'h': '88.77', 'w': '9.65'}, {'page': '4', 'x': '409.66', 'y': '278.25', 'h': '3.97', 'w': '6.12'}, {'page': '4', 'x': '418.36', 'y': '279.82', 'h': '143.73', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '291.28', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '302.74', 'h': '11.79', 'w': '8.74'}], [{'page': '4', 'x': '335.32', 'y': '302.74', 'h': '226.77', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '314.19', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '325.65', 'h': '18.27', 'w': '8.74'}], [{'page': '4', 'x': '342.72', 'y': '325.65', 'h': '219.37', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '337.11', 'h': '150.19', 'w': '8.74'}], [{'page': '4', 'x': '475.20', 'y': '337.11', 'h': '86.89', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '348.57', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '360.02', 'h': '245.08', 'w': '8.74'}], [{'page': '4', 'x': '317.01', 'y': '371.48', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '382.94', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '394.39', 'h': '191.97', 'w': '8.74'}], [{'page': '4', 'x': '513.70', 'y': '394.39', 'h': '48.40', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '405.85', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '417.31', 'h': '245.08', 'w': '9.65'}], [{'page': '4', 'x': '317.01', 'y': '428.76', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '320.89', 'y': '440.22', 'h': '83.00', 'w': '9.65'}], [{'page': '4', 'x': '414.21', 'y': '440.22', 'h': '56.27', 'w': '9.65'}], [{'page': '4', 'x': '476.92', 'y': '440.22', 'h': '85.17', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '451.68', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '463.14', 'h': '17.72', 'w': '8.74'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1102.3821.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ee5e208b-f1e5-4fef-b482-f1a1da230bb6\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"metadata\": {\"text\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"paper_title\": \"HAMILTONIAN EVOLUTIONARY GAMES\", \"section_title\": \"Polymatrix games\", \"bboxes\": \"[[{'page': '7', 'x': '83.39', 'y': '317.51', 'h': '293.72', 'w': '10.48'}], [{'page': '7', 'x': '385.27', 'y': '317.51', 'h': '102.86', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '331.46', 'h': '315.84', 'w': '10.48'}], [{'page': '7', 'x': '392.93', 'y': '331.46', 'h': '95.20', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '345.40', 'h': '232.98', 'w': '10.48'}], [{'page': '7', 'x': '313.82', 'y': '345.40', 'h': '174.31', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '359.35', 'h': '30.81', 'w': '10.48'}], [{'page': '7', 'x': '104.25', 'y': '359.35', 'h': '83.35', 'w': '10.48'}, {'page': '7', 'x': '187.60', 'y': '363.87', 'h': '5.44', 'w': '6.99'}, {'page': '7', 'x': '198.26', 'y': '359.35', 'h': '289.87', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '373.30', 'h': '131.49', 'w': '10.48'}], [{'page': '7', 'x': '212.09', 'y': '373.30', 'h': '276.03', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '387.25', 'h': '11.54', 'w': '10.48'}, {'page': '7', 'x': '82.97', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '87.71', 'y': '387.25', 'h': '18.98', 'w': '10.48'}], [{'page': '7', 'x': '108.69', 'y': '387.25', 'h': '12.22', 'w': '10.48'}, {'page': '7', 'x': '120.91', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '125.68', 'y': '387.25', 'h': '7.80', 'w': '10.48'}], [{'page': '7', 'x': '140.41', 'y': '387.25', 'h': '256.73', 'w': '10.48'}, {'page': '7', 'x': '397.14', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '404.93', 'y': '387.25', 'h': '25.89', 'w': '10.48'}], [{'page': '7', 'x': '433.87', 'y': '387.25', 'h': '19.14', 'w': '10.48'}, {'page': '7', 'x': '453.01', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '457.77', 'y': '387.25', 'h': '3.25', 'w': '10.48'}], [{'page': '7', 'x': '467.96', 'y': '387.25', 'h': '20.16', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '401.19', 'h': '324.04', 'w': '10.48'}, {'page': '7', 'x': '395.48', 'y': '405.71', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '402.74', 'y': '401.19', 'h': '3.25', 'w': '10.48'}, {'page': '7', 'x': '406.00', 'y': '407.76', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '417.84', 'y': '401.19', 'h': '70.28', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '415.88', 'h': '23.63', 'w': '10.48'}, {'page': '7', 'x': '95.07', 'y': '420.40', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '107.14', 'y': '415.88', 'h': '380.98', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '429.83', 'h': '255.42', 'w': '10.48'}], [{'page': '7', 'x': '336.60', 'y': '429.83', 'h': '151.53', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '443.78', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '457.73', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '471.67', 'h': '32.52', 'w': '10.48'}], [{'page': '7', 'x': '110.92', 'y': '471.67', 'h': '377.21', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '485.62', 'h': '259.89', 'w': '10.48'}], [{'page': '7', 'x': '336.31', 'y': '485.62', 'h': '151.81', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '499.57', 'h': '128.06', 'w': '10.48'}], [{'page': '7', 'x': '205.50', 'y': '499.57', 'h': '186.74', 'w': '10.48'}, {'page': '7', 'x': '392.24', 'y': '497.95', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '410.67', 'y': '506.97', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '424.12', 'y': '499.57', 'h': '64.00', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '520.55', 'h': '26.99', 'w': '10.48'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1404.5900.pdf\", \"section_number\": \"3.\", \"para\": \"13\", \"_id\": \"80e7e135-2677-4f4d-ab27-2c32c5e25390\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Automata representation of successful strategies for social dilemmas\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '56.33', 'y': '542.41', 'h': '499.14', 'w': '8.85'}, {'page': '2', 'x': '56.69', 'y': '554.04', 'h': '83.39', 'w': '9.17'}], [{'page': '2', 'x': '143.17', 'y': '554.36', 'h': '276.03', 'w': '8.64'}], [{'page': '2', 'x': '422.28', 'y': '554.36', 'h': '133.03', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '566.00', 'h': '482.10', 'w': '9.73'}], [{'page': '2', 'x': '541.88', 'y': '566.32', 'h': '13.59', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '578.27', 'h': '498.96', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '590.23', 'h': '22.26', 'w': '8.64'}, {'page': '2', 'x': '78.95', 'y': '588.30', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '89.32', 'y': '590.23', 'h': '282.95', 'w': '8.64'}], [{'page': '2', 'x': '375.37', 'y': '590.23', 'h': '179.94', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '602.18', 'h': '498.62', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '614.14', 'h': '111.81', 'w': '8.64'}, {'page': '2', 'x': '168.50', 'y': '612.21', 'h': '43.20', 'w': '10.57'}], [{'page': '2', 'x': '71.64', 'y': '626.09', 'h': '393.33', 'w': '8.64'}], [{'page': '2', 'x': '468.05', 'y': '626.09', 'h': '88.50', 'w': '8.64'}, {'page': '2', 'x': '56.33', 'y': '638.05', 'h': '499.32', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '649.82', 'h': '111.52', 'w': '8.82'}], [{'page': '2', 'x': '171.31', 'y': '650.00', 'h': '227.28', 'w': '8.64'}, {'page': '2', 'x': '398.59', 'y': '648.07', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '408.96', 'y': '650.00', 'h': '146.35', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '661.96', 'h': '103.02', 'w': '8.64'}], [{'page': '2', 'x': '162.80', 'y': '661.96', 'h': '392.50', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '673.91', 'h': '109.93', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '673.91', 'h': '385.49', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '685.87', 'h': '238.77', 'w': '8.64'}]]\", \"text\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1910.02634.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"25eeb897-22f5-4e0a-953d-3e0221b31c04\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"metadata\": {\"text\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"B.7 Proof of Lemma 6\", \"bboxes\": \"[[{'page': '29', 'x': '149.76', 'y': '462.69', 'h': '253.56', 'w': '10.66'}], [{'page': '29', 'x': '406.32', 'y': '462.69', 'h': '74.26', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '474.69', 'h': '202.54', 'w': '9.96'}, {'page': '29', 'x': '337.32', 'y': '473.21', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '337.32', 'y': '479.33', 'h': '3.97', 'w': '6.97'}, {'page': '29', 'x': '346.20', 'y': '474.69', 'h': '32.19', 'w': '10.66'}, {'page': '29', 'x': '379.32', 'y': '474.69', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '386.52', 'y': '474.69', 'h': '94.05', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '486.57', 'h': '234.14', 'w': '9.96'}], [{'page': '29', 'x': '373.44', 'y': '486.57', 'h': '106.23', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '498.57', 'h': '344.97', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '510.23', 'h': '190.61', 'w': '10.87'}, {'page': '29', 'x': '325.68', 'y': '509.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '325.32', 'y': '515.33', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '329.52', 'y': '510.45', 'h': '151.46', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '522.23', 'h': '132.24', 'w': '10.18'}], [{'page': '29', 'x': '271.44', 'y': '522.45', 'h': '209.10', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '534.45', 'h': '89.93', 'w': '9.96'}, {'page': '29', 'x': '225.00', 'y': '533.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '224.64', 'y': '539.21', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '228.84', 'y': '534.45', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '235.68', 'y': '534.45', 'h': '244.90', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '546.33', 'h': '304.71', 'w': '10.66'}, {'page': '29', 'x': '444.48', 'y': '546.33', 'h': '36.10', 'w': '9.96'}, {'page': '29', 'x': '134.52', 'y': '558.33', 'h': '86.64', 'w': '9.96'}], [{'page': '29', 'x': '224.64', 'y': '558.33', 'h': '256.38', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '570.33', 'h': '172.32', 'w': '9.96'}], [{'page': '29', 'x': '311.52', 'y': '570.33', 'h': '169.02', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '582.21', 'h': '346.02', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '594.21', 'h': '58.59', 'w': '10.65'}, {'page': '29', 'x': '194.28', 'y': '594.21', 'h': '147.72', 'w': '9.96'}], [{'page': '29', 'x': '346.20', 'y': '594.21', 'h': '133.47', 'w': '10.65'}, {'page': '29', 'x': '134.40', 'y': '606.09', 'h': '113.61', 'w': '9.96'}, {'page': '29', 'x': '248.88', 'y': '604.73', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '248.04', 'y': '611.21', 'h': '3.88', 'w': '6.97'}, {'page': '29', 'x': '257.28', 'y': '606.09', 'h': '223.77', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '617.87', 'h': '345.82', 'w': '10.87'}, {'page': '29', 'x': '134.76', 'y': '629.87', 'h': '248.52', 'w': '10.18'}], [{'page': '29', 'x': '387.72', 'y': '630.09', 'h': '93.01', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '640.87', 'h': '269.22', 'w': '11.06'}, {'page': '29', 'x': '404.04', 'y': '641.75', 'h': '76.50', 'w': '10.87'}, {'page': '29', 'x': '134.28', 'y': '652.61', 'h': '83.04', 'w': '11.32'}, {'page': '29', 'x': '217.80', 'y': '653.75', 'h': '86.64', 'w': '10.87'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"6d58eaad-c55e-4a91-818c-2e06e60d32c0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0xcbb4d6fc7954ff5f", "trace_id": "0x486dcfb360429241b62450df369905dd"}, "parent_id": "0xf167308fc1d68292", "start_time": 1724841660341470791, "end_time": 1724841660367475310, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"96d857d296ba47a3bd891e44f5d6affc\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\"", "mlflow.spanOutputs": "[{\"page_content\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"III. EXPERIMENTAL SETUP\", \"bboxes\": \"[[{'page': '4', 'x': '321.94', 'y': '619.92', 'h': '241.09', 'w': '8.82'}, {'page': '4', 'x': '311.98', 'y': '632.06', 'h': '98.21', 'w': '8.64'}], [{'page': '4', 'x': '414.24', 'y': '632.06', 'h': '148.79', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '644.01', 'h': '34.60', 'w': '8.64'}], [{'page': '4', 'x': '350.29', 'y': '643.69', 'h': '152.20', 'w': '8.96'}, {'page': '4', 'x': '502.50', 'y': '642.12', 'h': '7.94', 'w': '6.12'}, {'page': '4', 'x': '514.66', 'y': '644.01', 'h': '48.38', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '655.65', 'h': '194.38', 'w': '8.96'}, {'page': '4', 'x': '506.36', 'y': '654.07', 'h': '6.77', 'w': '6.12'}, {'page': '4', 'x': '516.47', 'y': '655.97', 'h': '46.56', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '667.92', 'h': '80.20', 'w': '8.64'}]]\", \"text\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"8c21f839-c634-4e86-9ea7-b4e44fbf28bc\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games\", \"section_title\": \"The Approximation Guarantee\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '410.49', 'h': '190.29', 'w': '9.96'}], [{'page': '7', 'x': '301.08', 'y': '410.49', 'h': '73.15', 'w': '9.96'}, {'page': '7', 'x': '377.04', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '386.40', 'y': '410.49', 'h': '62.26', 'w': '9.96'}, {'page': '7', 'x': '451.44', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '460.92', 'y': '410.95', 'h': '43.14', 'w': '9.10'}]]\", \"text\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.01795.pdf\", \"section_number\": \"5.1\", \"para\": \"1\", \"_id\": \"0548ecd0-5330-4a0b-a7ad-d48853603e96\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"metadata\": {\"pages\": \"('22', '22')\", \"paper_title\": \"From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains\", \"section_title\": \"Algorithm 2 Logistic Bernoulli Factory\", \"bboxes\": \"[[{'page': '22', 'x': '145.68', 'y': '482.37', 'h': '170.46', 'w': '9.96'}, {'page': '22', 'x': '334.32', 'y': '480.65', 'h': '3.97', 'w': '6.97'}, {'page': '22', 'x': '330.36', 'y': '487.16', 'h': '6.12', 'w': '4.42'}, {'page': '22', 'x': '330.36', 'y': '488.35', 'h': '19.81', 'w': '8.15'}, {'page': '22', 'x': '351.84', 'y': '482.37', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '382.92', 'y': '482.37', 'h': '94.67', 'w': '10.33'}, {'page': '22', 'x': '133.80', 'y': '496.53', 'h': '273.60', 'w': '9.96'}], [{'page': '22', 'x': '411.00', 'y': '496.53', 'h': '66.61', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '508.41', 'h': '190.98', 'w': '10.33'}, {'page': '22', 'x': '336.84', 'y': '505.99', 'h': '7.06', 'w': '6.40'}, {'page': '22', 'x': '336.84', 'y': '513.91', 'h': '12.85', 'w': '6.40'}, {'page': '22', 'x': '351.96', 'y': '508.41', 'h': '107.88', 'w': '10.33'}], [{'page': '22', 'x': '463.68', 'y': '508.41', 'h': '13.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '520.41', 'h': '343.62', 'w': '9.96'}, {'page': '22', 'x': '168.60', 'y': '532.41', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '199.68', 'y': '532.41', 'h': '277.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '547.77', 'h': '17.28', 'w': '9.96'}], [{'page': '22', 'x': '154.92', 'y': '547.77', 'h': '322.54', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '559.77', 'h': '326.49', 'w': '9.96'}]]\", \"text\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1912.09229.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"c49ea3dc-7fc6-44c3-8787-5a4d3197ab9c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"metadata\": {\"text\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"paper_title\": \"MULTI-STRUCTURAL GAMES AND BEYOND\", \"section_title\": \"The Syntactic Game\", \"bboxes\": \"[[{'page': '29', 'x': '90.00', 'y': '219.82', 'h': '121.87', 'w': '10.91'}, {'page': '29', 'x': '211.92', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '214.80', 'y': '219.82', 'h': '11.96', 'w': '10.91'}, {'page': '29', 'x': '227.16', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '229.92', 'y': '219.82', 'h': '16.44', 'w': '10.91'}, {'page': '29', 'x': '246.72', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '257.16', 'y': '219.82', 'h': '202.46', 'w': '10.91'}], [{'page': '29', 'x': '464.52', 'y': '219.82', 'h': '57.55', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '233.02', 'h': '4.92', 'w': '10.91'}, {'page': '29', 'x': '95.28', 'y': '231.26', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '101.04', 'y': '233.02', 'h': '420.90', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '245.98', 'h': '26.54', 'w': '10.91'}], [{'page': '29', 'x': '122.64', 'y': '245.98', 'h': '399.67', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '258.94', 'h': '303.19', 'w': '10.91'}, {'page': '29', 'x': '393.24', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '399.84', 'y': '258.94', 'h': '28.64', 'w': '10.91'}, {'page': '29', 'x': '428.88', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '431.64', 'y': '258.94', 'h': '3.02', 'w': '10.91'}], [{'page': '29', 'x': '440.40', 'y': '258.94', 'h': '81.41', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '271.90', 'h': '279.74', 'w': '10.91'}], [{'page': '29', 'x': '374.76', 'y': '271.90', 'h': '147.05', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '284.86', 'h': '109.82', 'w': '10.91'}], [{'page': '29', 'x': '206.04', 'y': '284.86', 'h': '316.15', 'w': '10.91'}, {'page': '29', 'x': '87.24', 'y': '297.82', 'h': '326.96', 'w': '10.91'}, {'page': '29', 'x': '414.24', 'y': '302.07', 'h': '17.14', 'w': '8.19'}, {'page': '29', 'x': '435.24', 'y': '297.82', 'h': '86.57', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '310.78', 'h': '19.82', 'w': '10.91'}], [{'page': '29', 'x': '118.20', 'y': '310.78', 'h': '404.03', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '323.74', 'h': '342.02', 'w': '10.91'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2301.13329.pdf\", \"section_number\": \"6.\", \"para\": \"6\", \"_id\": \"5a24fce9-eab2-4c1d-8e51-1df08c9f7536\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"metadata\": {\"text\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"paper_title\": \"Quantitative entanglement witnesses of Isotropic-and Werner-class via local measurements\", \"section_title\": \"III. LOCAL DECOMPOSITION OF QEW\", \"bboxes\": \"[[{'page': '4', 'x': '448.61', 'y': '52.70', 'h': '6.28', 'w': '6.31'}, {'page': '4', 'x': '317.01', 'y': '245.45', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '256.91', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '268.37', 'h': '245.08', 'w': '9.65'}, {'page': '4', 'x': '320.89', 'y': '279.82', 'h': '88.77', 'w': '9.65'}, {'page': '4', 'x': '409.66', 'y': '278.25', 'h': '3.97', 'w': '6.12'}, {'page': '4', 'x': '418.36', 'y': '279.82', 'h': '143.73', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '291.28', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '302.74', 'h': '11.79', 'w': '8.74'}], [{'page': '4', 'x': '335.32', 'y': '302.74', 'h': '226.77', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '314.19', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '325.65', 'h': '18.27', 'w': '8.74'}], [{'page': '4', 'x': '342.72', 'y': '325.65', 'h': '219.37', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '337.11', 'h': '150.19', 'w': '8.74'}], [{'page': '4', 'x': '475.20', 'y': '337.11', 'h': '86.89', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '348.57', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '360.02', 'h': '245.08', 'w': '8.74'}], [{'page': '4', 'x': '317.01', 'y': '371.48', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '382.94', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '394.39', 'h': '191.97', 'w': '8.74'}], [{'page': '4', 'x': '513.70', 'y': '394.39', 'h': '48.40', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '405.85', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '417.31', 'h': '245.08', 'w': '9.65'}], [{'page': '4', 'x': '317.01', 'y': '428.76', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '320.89', 'y': '440.22', 'h': '83.00', 'w': '9.65'}], [{'page': '4', 'x': '414.21', 'y': '440.22', 'h': '56.27', 'w': '9.65'}], [{'page': '4', 'x': '476.92', 'y': '440.22', 'h': '85.17', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '451.68', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '463.14', 'h': '17.72', 'w': '8.74'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1102.3821.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ee5e208b-f1e5-4fef-b482-f1a1da230bb6\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"metadata\": {\"text\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"paper_title\": \"HAMILTONIAN EVOLUTIONARY GAMES\", \"section_title\": \"Polymatrix games\", \"bboxes\": \"[[{'page': '7', 'x': '83.39', 'y': '317.51', 'h': '293.72', 'w': '10.48'}], [{'page': '7', 'x': '385.27', 'y': '317.51', 'h': '102.86', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '331.46', 'h': '315.84', 'w': '10.48'}], [{'page': '7', 'x': '392.93', 'y': '331.46', 'h': '95.20', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '345.40', 'h': '232.98', 'w': '10.48'}], [{'page': '7', 'x': '313.82', 'y': '345.40', 'h': '174.31', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '359.35', 'h': '30.81', 'w': '10.48'}], [{'page': '7', 'x': '104.25', 'y': '359.35', 'h': '83.35', 'w': '10.48'}, {'page': '7', 'x': '187.60', 'y': '363.87', 'h': '5.44', 'w': '6.99'}, {'page': '7', 'x': '198.26', 'y': '359.35', 'h': '289.87', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '373.30', 'h': '131.49', 'w': '10.48'}], [{'page': '7', 'x': '212.09', 'y': '373.30', 'h': '276.03', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '387.25', 'h': '11.54', 'w': '10.48'}, {'page': '7', 'x': '82.97', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '87.71', 'y': '387.25', 'h': '18.98', 'w': '10.48'}], [{'page': '7', 'x': '108.69', 'y': '387.25', 'h': '12.22', 'w': '10.48'}, {'page': '7', 'x': '120.91', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '125.68', 'y': '387.25', 'h': '7.80', 'w': '10.48'}], [{'page': '7', 'x': '140.41', 'y': '387.25', 'h': '256.73', 'w': '10.48'}, {'page': '7', 'x': '397.14', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '404.93', 'y': '387.25', 'h': '25.89', 'w': '10.48'}], [{'page': '7', 'x': '433.87', 'y': '387.25', 'h': '19.14', 'w': '10.48'}, {'page': '7', 'x': '453.01', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '457.77', 'y': '387.25', 'h': '3.25', 'w': '10.48'}], [{'page': '7', 'x': '467.96', 'y': '387.25', 'h': '20.16', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '401.19', 'h': '324.04', 'w': '10.48'}, {'page': '7', 'x': '395.48', 'y': '405.71', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '402.74', 'y': '401.19', 'h': '3.25', 'w': '10.48'}, {'page': '7', 'x': '406.00', 'y': '407.76', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '417.84', 'y': '401.19', 'h': '70.28', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '415.88', 'h': '23.63', 'w': '10.48'}, {'page': '7', 'x': '95.07', 'y': '420.40', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '107.14', 'y': '415.88', 'h': '380.98', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '429.83', 'h': '255.42', 'w': '10.48'}], [{'page': '7', 'x': '336.60', 'y': '429.83', 'h': '151.53', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '443.78', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '457.73', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '471.67', 'h': '32.52', 'w': '10.48'}], [{'page': '7', 'x': '110.92', 'y': '471.67', 'h': '377.21', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '485.62', 'h': '259.89', 'w': '10.48'}], [{'page': '7', 'x': '336.31', 'y': '485.62', 'h': '151.81', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '499.57', 'h': '128.06', 'w': '10.48'}], [{'page': '7', 'x': '205.50', 'y': '499.57', 'h': '186.74', 'w': '10.48'}, {'page': '7', 'x': '392.24', 'y': '497.95', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '410.67', 'y': '506.97', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '424.12', 'y': '499.57', 'h': '64.00', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '520.55', 'h': '26.99', 'w': '10.48'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1404.5900.pdf\", \"section_number\": \"3.\", \"para\": \"13\", \"_id\": \"80e7e135-2677-4f4d-ab27-2c32c5e25390\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Automata representation of successful strategies for social dilemmas\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '56.33', 'y': '542.41', 'h': '499.14', 'w': '8.85'}, {'page': '2', 'x': '56.69', 'y': '554.04', 'h': '83.39', 'w': '9.17'}], [{'page': '2', 'x': '143.17', 'y': '554.36', 'h': '276.03', 'w': '8.64'}], [{'page': '2', 'x': '422.28', 'y': '554.36', 'h': '133.03', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '566.00', 'h': '482.10', 'w': '9.73'}], [{'page': '2', 'x': '541.88', 'y': '566.32', 'h': '13.59', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '578.27', 'h': '498.96', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '590.23', 'h': '22.26', 'w': '8.64'}, {'page': '2', 'x': '78.95', 'y': '588.30', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '89.32', 'y': '590.23', 'h': '282.95', 'w': '8.64'}], [{'page': '2', 'x': '375.37', 'y': '590.23', 'h': '179.94', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '602.18', 'h': '498.62', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '614.14', 'h': '111.81', 'w': '8.64'}, {'page': '2', 'x': '168.50', 'y': '612.21', 'h': '43.20', 'w': '10.57'}], [{'page': '2', 'x': '71.64', 'y': '626.09', 'h': '393.33', 'w': '8.64'}], [{'page': '2', 'x': '468.05', 'y': '626.09', 'h': '88.50', 'w': '8.64'}, {'page': '2', 'x': '56.33', 'y': '638.05', 'h': '499.32', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '649.82', 'h': '111.52', 'w': '8.82'}], [{'page': '2', 'x': '171.31', 'y': '650.00', 'h': '227.28', 'w': '8.64'}, {'page': '2', 'x': '398.59', 'y': '648.07', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '408.96', 'y': '650.00', 'h': '146.35', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '661.96', 'h': '103.02', 'w': '8.64'}], [{'page': '2', 'x': '162.80', 'y': '661.96', 'h': '392.50', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '673.91', 'h': '109.93', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '673.91', 'h': '385.49', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '685.87', 'h': '238.77', 'w': '8.64'}]]\", \"text\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1910.02634.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"25eeb897-22f5-4e0a-953d-3e0221b31c04\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"metadata\": {\"text\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"B.7 Proof of Lemma 6\", \"bboxes\": \"[[{'page': '29', 'x': '149.76', 'y': '462.69', 'h': '253.56', 'w': '10.66'}], [{'page': '29', 'x': '406.32', 'y': '462.69', 'h': '74.26', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '474.69', 'h': '202.54', 'w': '9.96'}, {'page': '29', 'x': '337.32', 'y': '473.21', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '337.32', 'y': '479.33', 'h': '3.97', 'w': '6.97'}, {'page': '29', 'x': '346.20', 'y': '474.69', 'h': '32.19', 'w': '10.66'}, {'page': '29', 'x': '379.32', 'y': '474.69', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '386.52', 'y': '474.69', 'h': '94.05', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '486.57', 'h': '234.14', 'w': '9.96'}], [{'page': '29', 'x': '373.44', 'y': '486.57', 'h': '106.23', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '498.57', 'h': '344.97', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '510.23', 'h': '190.61', 'w': '10.87'}, {'page': '29', 'x': '325.68', 'y': '509.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '325.32', 'y': '515.33', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '329.52', 'y': '510.45', 'h': '151.46', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '522.23', 'h': '132.24', 'w': '10.18'}], [{'page': '29', 'x': '271.44', 'y': '522.45', 'h': '209.10', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '534.45', 'h': '89.93', 'w': '9.96'}, {'page': '29', 'x': '225.00', 'y': '533.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '224.64', 'y': '539.21', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '228.84', 'y': '534.45', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '235.68', 'y': '534.45', 'h': '244.90', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '546.33', 'h': '304.71', 'w': '10.66'}, {'page': '29', 'x': '444.48', 'y': '546.33', 'h': '36.10', 'w': '9.96'}, {'page': '29', 'x': '134.52', 'y': '558.33', 'h': '86.64', 'w': '9.96'}], [{'page': '29', 'x': '224.64', 'y': '558.33', 'h': '256.38', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '570.33', 'h': '172.32', 'w': '9.96'}], [{'page': '29', 'x': '311.52', 'y': '570.33', 'h': '169.02', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '582.21', 'h': '346.02', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '594.21', 'h': '58.59', 'w': '10.65'}, {'page': '29', 'x': '194.28', 'y': '594.21', 'h': '147.72', 'w': '9.96'}], [{'page': '29', 'x': '346.20', 'y': '594.21', 'h': '133.47', 'w': '10.65'}, {'page': '29', 'x': '134.40', 'y': '606.09', 'h': '113.61', 'w': '9.96'}, {'page': '29', 'x': '248.88', 'y': '604.73', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '248.04', 'y': '611.21', 'h': '3.88', 'w': '6.97'}, {'page': '29', 'x': '257.28', 'y': '606.09', 'h': '223.77', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '617.87', 'h': '345.82', 'w': '10.87'}, {'page': '29', 'x': '134.76', 'y': '629.87', 'h': '248.52', 'w': '10.18'}], [{'page': '29', 'x': '387.72', 'y': '630.09', 'h': '93.01', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '640.87', 'h': '269.22', 'w': '11.06'}, {'page': '29', 'x': '404.04', 'y': '641.75', 'h': '76.50', 'w': '10.87'}, {'page': '29', 'x': '134.28', 'y': '652.61', 'h': '83.04', 'w': '11.32'}, {'page': '29', 'x': '217.80', 'y': '653.75', 'h': '86.64', 'w': '10.87'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"6d58eaad-c55e-4a91-818c-2e06e60d32c0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0xa87d5aeb2f43e162", "trace_id": "0x486dcfb360429241b62450df369905dd"}, "parent_id": "0xf167308fc1d68292", "start_time": 1724841660368342785, "end_time": 1724841661370107777, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"96d857d296ba47a3bd891e44f5d6affc\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"III. EXPERIMENTAL SETUP\", \"bboxes\": \"[[{'page': '4', 'x': '321.94', 'y': '619.92', 'h': '241.09', 'w': '8.82'}, {'page': '4', 'x': '311.98', 'y': '632.06', 'h': '98.21', 'w': '8.64'}], [{'page': '4', 'x': '414.24', 'y': '632.06', 'h': '148.79', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '644.01', 'h': '34.60', 'w': '8.64'}], [{'page': '4', 'x': '350.29', 'y': '643.69', 'h': '152.20', 'w': '8.96'}, {'page': '4', 'x': '502.50', 'y': '642.12', 'h': '7.94', 'w': '6.12'}, {'page': '4', 'x': '514.66', 'y': '644.01', 'h': '48.38', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '655.65', 'h': '194.38', 'w': '8.96'}, {'page': '4', 'x': '506.36', 'y': '654.07', 'h': '6.77', 'w': '6.12'}, {'page': '4', 'x': '516.47', 'y': '655.97', 'h': '46.56', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '667.92', 'h': '80.20', 'w': '8.64'}]]\", \"text\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"8c21f839-c634-4e86-9ea7-b4e44fbf28bc\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games\", \"section_title\": \"The Approximation Guarantee\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '410.49', 'h': '190.29', 'w': '9.96'}], [{'page': '7', 'x': '301.08', 'y': '410.49', 'h': '73.15', 'w': '9.96'}, {'page': '7', 'x': '377.04', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '386.40', 'y': '410.49', 'h': '62.26', 'w': '9.96'}, {'page': '7', 'x': '451.44', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '460.92', 'y': '410.95', 'h': '43.14', 'w': '9.10'}]]\", \"text\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.01795.pdf\", \"section_number\": \"5.1\", \"para\": \"1\", \"_id\": \"0548ecd0-5330-4a0b-a7ad-d48853603e96\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"metadata\": {\"pages\": \"('22', '22')\", \"paper_title\": \"From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains\", \"section_title\": \"Algorithm 2 Logistic Bernoulli Factory\", \"bboxes\": \"[[{'page': '22', 'x': '145.68', 'y': '482.37', 'h': '170.46', 'w': '9.96'}, {'page': '22', 'x': '334.32', 'y': '480.65', 'h': '3.97', 'w': '6.97'}, {'page': '22', 'x': '330.36', 'y': '487.16', 'h': '6.12', 'w': '4.42'}, {'page': '22', 'x': '330.36', 'y': '488.35', 'h': '19.81', 'w': '8.15'}, {'page': '22', 'x': '351.84', 'y': '482.37', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '382.92', 'y': '482.37', 'h': '94.67', 'w': '10.33'}, {'page': '22', 'x': '133.80', 'y': '496.53', 'h': '273.60', 'w': '9.96'}], [{'page': '22', 'x': '411.00', 'y': '496.53', 'h': '66.61', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '508.41', 'h': '190.98', 'w': '10.33'}, {'page': '22', 'x': '336.84', 'y': '505.99', 'h': '7.06', 'w': '6.40'}, {'page': '22', 'x': '336.84', 'y': '513.91', 'h': '12.85', 'w': '6.40'}, {'page': '22', 'x': '351.96', 'y': '508.41', 'h': '107.88', 'w': '10.33'}], [{'page': '22', 'x': '463.68', 'y': '508.41', 'h': '13.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '520.41', 'h': '343.62', 'w': '9.96'}, {'page': '22', 'x': '168.60', 'y': '532.41', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '199.68', 'y': '532.41', 'h': '277.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '547.77', 'h': '17.28', 'w': '9.96'}], [{'page': '22', 'x': '154.92', 'y': '547.77', 'h': '322.54', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '559.77', 'h': '326.49', 'w': '9.96'}]]\", \"text\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1912.09229.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"c49ea3dc-7fc6-44c3-8787-5a4d3197ab9c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"metadata\": {\"text\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"paper_title\": \"MULTI-STRUCTURAL GAMES AND BEYOND\", \"section_title\": \"The Syntactic Game\", \"bboxes\": \"[[{'page': '29', 'x': '90.00', 'y': '219.82', 'h': '121.87', 'w': '10.91'}, {'page': '29', 'x': '211.92', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '214.80', 'y': '219.82', 'h': '11.96', 'w': '10.91'}, {'page': '29', 'x': '227.16', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '229.92', 'y': '219.82', 'h': '16.44', 'w': '10.91'}, {'page': '29', 'x': '246.72', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '257.16', 'y': '219.82', 'h': '202.46', 'w': '10.91'}], [{'page': '29', 'x': '464.52', 'y': '219.82', 'h': '57.55', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '233.02', 'h': '4.92', 'w': '10.91'}, {'page': '29', 'x': '95.28', 'y': '231.26', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '101.04', 'y': '233.02', 'h': '420.90', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '245.98', 'h': '26.54', 'w': '10.91'}], [{'page': '29', 'x': '122.64', 'y': '245.98', 'h': '399.67', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '258.94', 'h': '303.19', 'w': '10.91'}, {'page': '29', 'x': '393.24', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '399.84', 'y': '258.94', 'h': '28.64', 'w': '10.91'}, {'page': '29', 'x': '428.88', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '431.64', 'y': '258.94', 'h': '3.02', 'w': '10.91'}], [{'page': '29', 'x': '440.40', 'y': '258.94', 'h': '81.41', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '271.90', 'h': '279.74', 'w': '10.91'}], [{'page': '29', 'x': '374.76', 'y': '271.90', 'h': '147.05', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '284.86', 'h': '109.82', 'w': '10.91'}], [{'page': '29', 'x': '206.04', 'y': '284.86', 'h': '316.15', 'w': '10.91'}, {'page': '29', 'x': '87.24', 'y': '297.82', 'h': '326.96', 'w': '10.91'}, {'page': '29', 'x': '414.24', 'y': '302.07', 'h': '17.14', 'w': '8.19'}, {'page': '29', 'x': '435.24', 'y': '297.82', 'h': '86.57', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '310.78', 'h': '19.82', 'w': '10.91'}], [{'page': '29', 'x': '118.20', 'y': '310.78', 'h': '404.03', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '323.74', 'h': '342.02', 'w': '10.91'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2301.13329.pdf\", \"section_number\": \"6.\", \"para\": \"6\", \"_id\": \"5a24fce9-eab2-4c1d-8e51-1df08c9f7536\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"metadata\": {\"text\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"paper_title\": \"Quantitative entanglement witnesses of Isotropic-and Werner-class via local measurements\", \"section_title\": \"III. LOCAL DECOMPOSITION OF QEW\", \"bboxes\": \"[[{'page': '4', 'x': '448.61', 'y': '52.70', 'h': '6.28', 'w': '6.31'}, {'page': '4', 'x': '317.01', 'y': '245.45', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '256.91', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '268.37', 'h': '245.08', 'w': '9.65'}, {'page': '4', 'x': '320.89', 'y': '279.82', 'h': '88.77', 'w': '9.65'}, {'page': '4', 'x': '409.66', 'y': '278.25', 'h': '3.97', 'w': '6.12'}, {'page': '4', 'x': '418.36', 'y': '279.82', 'h': '143.73', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '291.28', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '302.74', 'h': '11.79', 'w': '8.74'}], [{'page': '4', 'x': '335.32', 'y': '302.74', 'h': '226.77', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '314.19', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '325.65', 'h': '18.27', 'w': '8.74'}], [{'page': '4', 'x': '342.72', 'y': '325.65', 'h': '219.37', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '337.11', 'h': '150.19', 'w': '8.74'}], [{'page': '4', 'x': '475.20', 'y': '337.11', 'h': '86.89', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '348.57', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '360.02', 'h': '245.08', 'w': '8.74'}], [{'page': '4', 'x': '317.01', 'y': '371.48', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '382.94', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '394.39', 'h': '191.97', 'w': '8.74'}], [{'page': '4', 'x': '513.70', 'y': '394.39', 'h': '48.40', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '405.85', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '417.31', 'h': '245.08', 'w': '9.65'}], [{'page': '4', 'x': '317.01', 'y': '428.76', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '320.89', 'y': '440.22', 'h': '83.00', 'w': '9.65'}], [{'page': '4', 'x': '414.21', 'y': '440.22', 'h': '56.27', 'w': '9.65'}], [{'page': '4', 'x': '476.92', 'y': '440.22', 'h': '85.17', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '451.68', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '463.14', 'h': '17.72', 'w': '8.74'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1102.3821.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ee5e208b-f1e5-4fef-b482-f1a1da230bb6\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"metadata\": {\"text\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"paper_title\": \"HAMILTONIAN EVOLUTIONARY GAMES\", \"section_title\": \"Polymatrix games\", \"bboxes\": \"[[{'page': '7', 'x': '83.39', 'y': '317.51', 'h': '293.72', 'w': '10.48'}], [{'page': '7', 'x': '385.27', 'y': '317.51', 'h': '102.86', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '331.46', 'h': '315.84', 'w': '10.48'}], [{'page': '7', 'x': '392.93', 'y': '331.46', 'h': '95.20', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '345.40', 'h': '232.98', 'w': '10.48'}], [{'page': '7', 'x': '313.82', 'y': '345.40', 'h': '174.31', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '359.35', 'h': '30.81', 'w': '10.48'}], [{'page': '7', 'x': '104.25', 'y': '359.35', 'h': '83.35', 'w': '10.48'}, {'page': '7', 'x': '187.60', 'y': '363.87', 'h': '5.44', 'w': '6.99'}, {'page': '7', 'x': '198.26', 'y': '359.35', 'h': '289.87', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '373.30', 'h': '131.49', 'w': '10.48'}], [{'page': '7', 'x': '212.09', 'y': '373.30', 'h': '276.03', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '387.25', 'h': '11.54', 'w': '10.48'}, {'page': '7', 'x': '82.97', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '87.71', 'y': '387.25', 'h': '18.98', 'w': '10.48'}], [{'page': '7', 'x': '108.69', 'y': '387.25', 'h': '12.22', 'w': '10.48'}, {'page': '7', 'x': '120.91', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '125.68', 'y': '387.25', 'h': '7.80', 'w': '10.48'}], [{'page': '7', 'x': '140.41', 'y': '387.25', 'h': '256.73', 'w': '10.48'}, {'page': '7', 'x': '397.14', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '404.93', 'y': '387.25', 'h': '25.89', 'w': '10.48'}], [{'page': '7', 'x': '433.87', 'y': '387.25', 'h': '19.14', 'w': '10.48'}, {'page': '7', 'x': '453.01', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '457.77', 'y': '387.25', 'h': '3.25', 'w': '10.48'}], [{'page': '7', 'x': '467.96', 'y': '387.25', 'h': '20.16', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '401.19', 'h': '324.04', 'w': '10.48'}, {'page': '7', 'x': '395.48', 'y': '405.71', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '402.74', 'y': '401.19', 'h': '3.25', 'w': '10.48'}, {'page': '7', 'x': '406.00', 'y': '407.76', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '417.84', 'y': '401.19', 'h': '70.28', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '415.88', 'h': '23.63', 'w': '10.48'}, {'page': '7', 'x': '95.07', 'y': '420.40', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '107.14', 'y': '415.88', 'h': '380.98', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '429.83', 'h': '255.42', 'w': '10.48'}], [{'page': '7', 'x': '336.60', 'y': '429.83', 'h': '151.53', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '443.78', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '457.73', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '471.67', 'h': '32.52', 'w': '10.48'}], [{'page': '7', 'x': '110.92', 'y': '471.67', 'h': '377.21', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '485.62', 'h': '259.89', 'w': '10.48'}], [{'page': '7', 'x': '336.31', 'y': '485.62', 'h': '151.81', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '499.57', 'h': '128.06', 'w': '10.48'}], [{'page': '7', 'x': '205.50', 'y': '499.57', 'h': '186.74', 'w': '10.48'}, {'page': '7', 'x': '392.24', 'y': '497.95', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '410.67', 'y': '506.97', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '424.12', 'y': '499.57', 'h': '64.00', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '520.55', 'h': '26.99', 'w': '10.48'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1404.5900.pdf\", \"section_number\": \"3.\", \"para\": \"13\", \"_id\": \"80e7e135-2677-4f4d-ab27-2c32c5e25390\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Automata representation of successful strategies for social dilemmas\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '56.33', 'y': '542.41', 'h': '499.14', 'w': '8.85'}, {'page': '2', 'x': '56.69', 'y': '554.04', 'h': '83.39', 'w': '9.17'}], [{'page': '2', 'x': '143.17', 'y': '554.36', 'h': '276.03', 'w': '8.64'}], [{'page': '2', 'x': '422.28', 'y': '554.36', 'h': '133.03', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '566.00', 'h': '482.10', 'w': '9.73'}], [{'page': '2', 'x': '541.88', 'y': '566.32', 'h': '13.59', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '578.27', 'h': '498.96', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '590.23', 'h': '22.26', 'w': '8.64'}, {'page': '2', 'x': '78.95', 'y': '588.30', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '89.32', 'y': '590.23', 'h': '282.95', 'w': '8.64'}], [{'page': '2', 'x': '375.37', 'y': '590.23', 'h': '179.94', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '602.18', 'h': '498.62', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '614.14', 'h': '111.81', 'w': '8.64'}, {'page': '2', 'x': '168.50', 'y': '612.21', 'h': '43.20', 'w': '10.57'}], [{'page': '2', 'x': '71.64', 'y': '626.09', 'h': '393.33', 'w': '8.64'}], [{'page': '2', 'x': '468.05', 'y': '626.09', 'h': '88.50', 'w': '8.64'}, {'page': '2', 'x': '56.33', 'y': '638.05', 'h': '499.32', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '649.82', 'h': '111.52', 'w': '8.82'}], [{'page': '2', 'x': '171.31', 'y': '650.00', 'h': '227.28', 'w': '8.64'}, {'page': '2', 'x': '398.59', 'y': '648.07', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '408.96', 'y': '650.00', 'h': '146.35', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '661.96', 'h': '103.02', 'w': '8.64'}], [{'page': '2', 'x': '162.80', 'y': '661.96', 'h': '392.50', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '673.91', 'h': '109.93', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '673.91', 'h': '385.49', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '685.87', 'h': '238.77', 'w': '8.64'}]]\", \"text\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1910.02634.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"25eeb897-22f5-4e0a-953d-3e0221b31c04\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"metadata\": {\"text\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"B.7 Proof of Lemma 6\", \"bboxes\": \"[[{'page': '29', 'x': '149.76', 'y': '462.69', 'h': '253.56', 'w': '10.66'}], [{'page': '29', 'x': '406.32', 'y': '462.69', 'h': '74.26', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '474.69', 'h': '202.54', 'w': '9.96'}, {'page': '29', 'x': '337.32', 'y': '473.21', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '337.32', 'y': '479.33', 'h': '3.97', 'w': '6.97'}, {'page': '29', 'x': '346.20', 'y': '474.69', 'h': '32.19', 'w': '10.66'}, {'page': '29', 'x': '379.32', 'y': '474.69', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '386.52', 'y': '474.69', 'h': '94.05', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '486.57', 'h': '234.14', 'w': '9.96'}], [{'page': '29', 'x': '373.44', 'y': '486.57', 'h': '106.23', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '498.57', 'h': '344.97', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '510.23', 'h': '190.61', 'w': '10.87'}, {'page': '29', 'x': '325.68', 'y': '509.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '325.32', 'y': '515.33', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '329.52', 'y': '510.45', 'h': '151.46', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '522.23', 'h': '132.24', 'w': '10.18'}], [{'page': '29', 'x': '271.44', 'y': '522.45', 'h': '209.10', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '534.45', 'h': '89.93', 'w': '9.96'}, {'page': '29', 'x': '225.00', 'y': '533.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '224.64', 'y': '539.21', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '228.84', 'y': '534.45', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '235.68', 'y': '534.45', 'h': '244.90', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '546.33', 'h': '304.71', 'w': '10.66'}, {'page': '29', 'x': '444.48', 'y': '546.33', 'h': '36.10', 'w': '9.96'}, {'page': '29', 'x': '134.52', 'y': '558.33', 'h': '86.64', 'w': '9.96'}], [{'page': '29', 'x': '224.64', 'y': '558.33', 'h': '256.38', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '570.33', 'h': '172.32', 'w': '9.96'}], [{'page': '29', 'x': '311.52', 'y': '570.33', 'h': '169.02', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '582.21', 'h': '346.02', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '594.21', 'h': '58.59', 'w': '10.65'}, {'page': '29', 'x': '194.28', 'y': '594.21', 'h': '147.72', 'w': '9.96'}], [{'page': '29', 'x': '346.20', 'y': '594.21', 'h': '133.47', 'w': '10.65'}, {'page': '29', 'x': '134.40', 'y': '606.09', 'h': '113.61', 'w': '9.96'}, {'page': '29', 'x': '248.88', 'y': '604.73', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '248.04', 'y': '611.21', 'h': '3.88', 'w': '6.97'}, {'page': '29', 'x': '257.28', 'y': '606.09', 'h': '223.77', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '617.87', 'h': '345.82', 'w': '10.87'}, {'page': '29', 'x': '134.76', 'y': '629.87', 'h': '248.52', 'w': '10.18'}], [{'page': '29', 'x': '387.72', 'y': '630.09', 'h': '93.01', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '640.87', 'h': '269.22', 'w': '11.06'}, {'page': '29', 'x': '404.04', 'y': '641.75', 'h': '76.50', 'w': '10.87'}, {'page': '29', 'x': '134.28', 'y': '652.61', 'h': '83.04', 'w': '11.32'}, {'page': '29', 'x': '217.80', 'y': '653.75', 'h': '86.64', 'w': '10.87'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"6d58eaad-c55e-4a91-818c-2e06e60d32c0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\"}", "mlflow.spanOutputs": "{\"output_text\": \" The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \\u2264 7 when using alpha-beta search and the Minimax algorithm.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x4da5fbdfd7c60a81", "trace_id": "0x486dcfb360429241b62450df369905dd"}, "parent_id": "0xa87d5aeb2f43e162", "start_time": 1724841660369231592, "end_time": 1724841661369912368, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"96d857d296ba47a3bd891e44f5d6affc\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\", \"context\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\\n\\nLemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\\n\\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\\n\\nWe now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\\n\\nConsider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\\n\\n2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\\n\\nFinally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\\n\\nwhere the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\\n\\nConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\\n\\nPlaying from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\"}", "mlflow.spanOutputs": "{\"text\": \" The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \\u2264 7 when using alpha-beta search and the Minimax algorithm.\"}"}, "events": [{"name": "text", "timestamp": 1724841660369327, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\n\nLemma 5. Let \u01eb > 0 and \u03c0 be a strategy profile.If for each player i \u2208 [n], each state s \u2208 S and each\n\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\n\nWe now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\n\nConsider a node X = (A \u2032 , B \u2032 ) r \u2032 in the game tree T for a given QVT game.The counter r \u2032 \u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \"distinguish\" between A \u2032 and B \u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \"reasonable\" complexity measure of infinitary formulas in the logic L \u03c9 1 ,\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\n\n2) A \u2208 \u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \u03b1 \u2297 P \u03b1 , X \u03b1\u03b2 \u2297 X \u03b1\u03b2 and X \u03b1\u03b2 \u2297 X \u03b1\u03b2 from d 2 to order d (here we use \u2022 \u2022 \u2022 to represent the expectation value with respect to the state \u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \u03b1-th vertex green we are actually acquiring P \u03b1 \u2297 P \u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \u03b1 and \u03b2 we evaluate X \u03b1\u03b2 \u2297 X \u03b1\u03b2 (resp.Y \u03b1\u03b2 \u2297 Y \u03b1\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\n\nFinally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \u03b1 and \u03b2 to denote these groups.Assume that for each group \u03b1 \u2208 {1, . . ., p}, there are n \u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \u03b1,\u03b2 \u03b1,\u03b2 , where each block\n\nwhere the number of defectors among the two co-players is written at the top of each column , and \u03c1 is a multiplication factor satisfying 1 < \u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\n\nConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\n\nPlaying from c 0 in M S , the strategy \u03c3 \u01eb behaves as follows.First it plays like strategy \u03c3 in the corresponding game from c \u2032 0 on M S \u2032 .(Function \u03bb connects the corresponding configurations in the two games.)When the game in M S \u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \u2265 s i (d(\u01eb)) then \u03c3 \u01eb henceforth plays like \u03c3 \u01eb i , which ensures to reach the target X with probability \u2265 1 -2\u01eb.Otherwise, the configuration in M S is still too small to switch to \u03c3 \u01eb i .In this case, \u03c3 \u01eb continues to play like \u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \u2032 , i.e., component k(i) is pumped.Since we know that \u03c3 on M S \u2032 will almost surely visit X \u2032 f or R, we obtain that \u03c3 \u01eb on M S will almost surely eventually visit X or some configuration \u2265 s i (d(\u01eb)) for i \u2208 J (and from there achieve to reach the target with probability \u2265 1 -2\u01eb).Since every weighted average of probabilities \u2265 1-2\u01eb is still \u2265 1-2\u01eb, we obtain P(M S , c 0 , \u03c3 \u01eb , \u2666X ) \u2265 1 -2\u01eb and thus P + (M S , c 0 , \u2666X ) = 1.\n\nQuestion: What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x40fda7983a32dcd0", "trace_id": "0x486dcfb360429241b62450df369905dd"}, "parent_id": "0x4da5fbdfd7c60a81", "start_time": 1724841660369637507, "end_time": 1724841661369652380, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"96d857d296ba47a3bd891e44f5d6affc\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\\n\\nLemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\\n\\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\\n\\nWe now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\\n\\nConsider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\\n\\n2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\\n\\nFinally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\\n\\nwhere the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\\n\\nConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\\n\\nPlaying from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\\n\\nQuestion: What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \\u2264 7 when using alpha-beta search and the Minimax algorithm.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 2123, \"completion_tokens\": 34, \"total_tokens\": 2157}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?\"}", "response": "{\"result\": \" The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \\u2264 7 when using alpha-beta search and the Minimax algorithm.\", \"source_documents\": [{\"page_content\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"III. EXPERIMENTAL SETUP\", \"bboxes\": \"[[{'page': '4', 'x': '321.94', 'y': '619.92', 'h': '241.09', 'w': '8.82'}, {'page': '4', 'x': '311.98', 'y': '632.06', 'h': '98.21', 'w': '8.64'}], [{'page': '4', 'x': '414.24', 'y': '632.06', 'h': '148.79', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '644.01', 'h': '34.60', 'w': '8.64'}], [{'page': '4', 'x': '350.29', 'y': '643.69', 'h': '152.20', 'w': '8.96'}, {'page': '4', 'x': '502.50', 'y': '642.12', 'h': '7.94', 'w': '6.12'}, {'page': '4', 'x': '514.66', 'y': '644.01', 'h': '48.38', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '655.65', 'h': '194.38', 'w': '8.96'}, {'page': '4', 'x': '506.36', 'y': '654.07', 'h': '6.77', 'w': '6.12'}, {'page': '4', 'x': '516.47', 'y': '655.97', 'h': '46.56', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '667.92', 'h': '80.20', 'w': '8.64'}]]\", \"text\": \"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \\u2264 7. It is a solved game: The 1 st player wins if playing perfectly.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"8c21f839-c634-4e86-9ea7-b4e44fbf28bc\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games\", \"section_title\": \"The Approximation Guarantee\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '410.49', 'h': '190.29', 'w': '9.96'}], [{'page': '7', 'x': '301.08', 'y': '410.49', 'h': '73.15', 'w': '9.96'}, {'page': '7', 'x': '377.04', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '386.40', 'y': '410.49', 'h': '62.26', 'w': '9.96'}, {'page': '7', 'x': '451.44', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '460.92', 'y': '410.95', 'h': '43.14', 'w': '9.10'}]]\", \"text\": \"Lemma 5. Let \\u01eb > 0 and \\u03c0 be a strategy profile.If for each player i \\u2208 [n], each state s \\u2208 S and each\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.01795.pdf\", \"section_number\": \"5.1\", \"para\": \"1\", \"_id\": \"0548ecd0-5330-4a0b-a7ad-d48853603e96\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"metadata\": {\"pages\": \"('22', '22')\", \"paper_title\": \"From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains\", \"section_title\": \"Algorithm 2 Logistic Bernoulli Factory\", \"bboxes\": \"[[{'page': '22', 'x': '145.68', 'y': '482.37', 'h': '170.46', 'w': '9.96'}, {'page': '22', 'x': '334.32', 'y': '480.65', 'h': '3.97', 'w': '6.97'}, {'page': '22', 'x': '330.36', 'y': '487.16', 'h': '6.12', 'w': '4.42'}, {'page': '22', 'x': '330.36', 'y': '488.35', 'h': '19.81', 'w': '8.15'}, {'page': '22', 'x': '351.84', 'y': '482.37', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '382.92', 'y': '482.37', 'h': '94.67', 'w': '10.33'}, {'page': '22', 'x': '133.80', 'y': '496.53', 'h': '273.60', 'w': '9.96'}], [{'page': '22', 'x': '411.00', 'y': '496.53', 'h': '66.61', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '508.41', 'h': '190.98', 'w': '10.33'}, {'page': '22', 'x': '336.84', 'y': '505.99', 'h': '7.06', 'w': '6.40'}, {'page': '22', 'x': '336.84', 'y': '513.91', 'h': '12.85', 'w': '6.40'}, {'page': '22', 'x': '351.96', 'y': '508.41', 'h': '107.88', 'w': '10.33'}], [{'page': '22', 'x': '463.68', 'y': '508.41', 'h': '13.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '520.41', 'h': '343.62', 'w': '9.96'}, {'page': '22', 'x': '168.60', 'y': '532.41', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '199.68', 'y': '532.41', 'h': '277.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '547.77', 'h': '17.28', 'w': '9.96'}], [{'page': '22', 'x': '154.92', 'y': '547.77', 'h': '322.54', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '559.77', 'h': '326.49', 'w': '9.96'}]]\", \"text\": \"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1912.09229.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"c49ea3dc-7fc6-44c3-8787-5a4d3197ab9c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"metadata\": {\"text\": \"Consider a node X = (A \\u2032 , B \\u2032 ) r \\u2032 in the game tree T for a given QVT game.The counter r \\u2032 \\u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \\\"distinguish\\\" between A \\u2032 and B \\u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \\\"reasonable\\\" complexity measure of infinitary formulas in the logic L \\u03c9 1 ,\\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.\", \"paper_title\": \"MULTI-STRUCTURAL GAMES AND BEYOND\", \"section_title\": \"The Syntactic Game\", \"bboxes\": \"[[{'page': '29', 'x': '90.00', 'y': '219.82', 'h': '121.87', 'w': '10.91'}, {'page': '29', 'x': '211.92', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '214.80', 'y': '219.82', 'h': '11.96', 'w': '10.91'}, {'page': '29', 'x': '227.16', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '229.92', 'y': '219.82', 'h': '16.44', 'w': '10.91'}, {'page': '29', 'x': '246.72', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '257.16', 'y': '219.82', 'h': '202.46', 'w': '10.91'}], [{'page': '29', 'x': '464.52', 'y': '219.82', 'h': '57.55', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '233.02', 'h': '4.92', 'w': '10.91'}, {'page': '29', 'x': '95.28', 'y': '231.26', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '101.04', 'y': '233.02', 'h': '420.90', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '245.98', 'h': '26.54', 'w': '10.91'}], [{'page': '29', 'x': '122.64', 'y': '245.98', 'h': '399.67', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '258.94', 'h': '303.19', 'w': '10.91'}, {'page': '29', 'x': '393.24', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '399.84', 'y': '258.94', 'h': '28.64', 'w': '10.91'}, {'page': '29', 'x': '428.88', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '431.64', 'y': '258.94', 'h': '3.02', 'w': '10.91'}], [{'page': '29', 'x': '440.40', 'y': '258.94', 'h': '81.41', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '271.90', 'h': '279.74', 'w': '10.91'}], [{'page': '29', 'x': '374.76', 'y': '271.90', 'h': '147.05', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '284.86', 'h': '109.82', 'w': '10.91'}], [{'page': '29', 'x': '206.04', 'y': '284.86', 'h': '316.15', 'w': '10.91'}, {'page': '29', 'x': '87.24', 'y': '297.82', 'h': '326.96', 'w': '10.91'}, {'page': '29', 'x': '414.24', 'y': '302.07', 'h': '17.14', 'w': '8.19'}, {'page': '29', 'x': '435.24', 'y': '297.82', 'h': '86.57', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '310.78', 'h': '19.82', 'w': '10.91'}], [{'page': '29', 'x': '118.20', 'y': '310.78', 'h': '404.03', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '323.74', 'h': '342.02', 'w': '10.91'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2301.13329.pdf\", \"section_number\": \"6.\", \"para\": \"6\", \"_id\": \"5a24fce9-eab2-4c1d-8e51-1df08c9f7536\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"metadata\": {\"text\": \"2) A \\u2208 \\u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \\u03b1 \\u2297 P \\u03b1 , X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 and X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 from d 2 to order d (here we use \\u2022 \\u2022 \\u2022 to represent the expectation value with respect to the state \\u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \\u03b1-th vertex green we are actually acquiring P \\u03b1 \\u2297 P \\u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \\u03b1 and \\u03b2 we evaluate X \\u03b1\\u03b2 \\u2297 X \\u03b1\\u03b2 (resp.Y \\u03b1\\u03b2 \\u2297 Y \\u03b1\\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:\", \"paper_title\": \"Quantitative entanglement witnesses of Isotropic-and Werner-class via local measurements\", \"section_title\": \"III. LOCAL DECOMPOSITION OF QEW\", \"bboxes\": \"[[{'page': '4', 'x': '448.61', 'y': '52.70', 'h': '6.28', 'w': '6.31'}, {'page': '4', 'x': '317.01', 'y': '245.45', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '256.91', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '268.37', 'h': '245.08', 'w': '9.65'}, {'page': '4', 'x': '320.89', 'y': '279.82', 'h': '88.77', 'w': '9.65'}, {'page': '4', 'x': '409.66', 'y': '278.25', 'h': '3.97', 'w': '6.12'}, {'page': '4', 'x': '418.36', 'y': '279.82', 'h': '143.73', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '291.28', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '302.74', 'h': '11.79', 'w': '8.74'}], [{'page': '4', 'x': '335.32', 'y': '302.74', 'h': '226.77', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '314.19', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '325.65', 'h': '18.27', 'w': '8.74'}], [{'page': '4', 'x': '342.72', 'y': '325.65', 'h': '219.37', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '337.11', 'h': '150.19', 'w': '8.74'}], [{'page': '4', 'x': '475.20', 'y': '337.11', 'h': '86.89', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '348.57', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '360.02', 'h': '245.08', 'w': '8.74'}], [{'page': '4', 'x': '317.01', 'y': '371.48', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '382.94', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '394.39', 'h': '191.97', 'w': '8.74'}], [{'page': '4', 'x': '513.70', 'y': '394.39', 'h': '48.40', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '405.85', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '417.31', 'h': '245.08', 'w': '9.65'}], [{'page': '4', 'x': '317.01', 'y': '428.76', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '320.89', 'y': '440.22', 'h': '83.00', 'w': '9.65'}], [{'page': '4', 'x': '414.21', 'y': '440.22', 'h': '56.27', 'w': '9.65'}], [{'page': '4', 'x': '476.92', 'y': '440.22', 'h': '85.17', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '451.68', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '463.14', 'h': '17.72', 'w': '8.74'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1102.3821.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ee5e208b-f1e5-4fef-b482-f1a1da230bb6\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"metadata\": {\"text\": \"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \\u03b1 and \\u03b2 to denote these groups.Assume that for each group \\u03b1 \\u2208 {1, . . ., p}, there are n \\u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \\u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \\u03b1,\\u03b2 \\u03b1,\\u03b2 , where each block\", \"paper_title\": \"HAMILTONIAN EVOLUTIONARY GAMES\", \"section_title\": \"Polymatrix games\", \"bboxes\": \"[[{'page': '7', 'x': '83.39', 'y': '317.51', 'h': '293.72', 'w': '10.48'}], [{'page': '7', 'x': '385.27', 'y': '317.51', 'h': '102.86', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '331.46', 'h': '315.84', 'w': '10.48'}], [{'page': '7', 'x': '392.93', 'y': '331.46', 'h': '95.20', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '345.40', 'h': '232.98', 'w': '10.48'}], [{'page': '7', 'x': '313.82', 'y': '345.40', 'h': '174.31', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '359.35', 'h': '30.81', 'w': '10.48'}], [{'page': '7', 'x': '104.25', 'y': '359.35', 'h': '83.35', 'w': '10.48'}, {'page': '7', 'x': '187.60', 'y': '363.87', 'h': '5.44', 'w': '6.99'}, {'page': '7', 'x': '198.26', 'y': '359.35', 'h': '289.87', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '373.30', 'h': '131.49', 'w': '10.48'}], [{'page': '7', 'x': '212.09', 'y': '373.30', 'h': '276.03', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '387.25', 'h': '11.54', 'w': '10.48'}, {'page': '7', 'x': '82.97', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '87.71', 'y': '387.25', 'h': '18.98', 'w': '10.48'}], [{'page': '7', 'x': '108.69', 'y': '387.25', 'h': '12.22', 'w': '10.48'}, {'page': '7', 'x': '120.91', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '125.68', 'y': '387.25', 'h': '7.80', 'w': '10.48'}], [{'page': '7', 'x': '140.41', 'y': '387.25', 'h': '256.73', 'w': '10.48'}, {'page': '7', 'x': '397.14', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '404.93', 'y': '387.25', 'h': '25.89', 'w': '10.48'}], [{'page': '7', 'x': '433.87', 'y': '387.25', 'h': '19.14', 'w': '10.48'}, {'page': '7', 'x': '453.01', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '457.77', 'y': '387.25', 'h': '3.25', 'w': '10.48'}], [{'page': '7', 'x': '467.96', 'y': '387.25', 'h': '20.16', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '401.19', 'h': '324.04', 'w': '10.48'}, {'page': '7', 'x': '395.48', 'y': '405.71', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '402.74', 'y': '401.19', 'h': '3.25', 'w': '10.48'}, {'page': '7', 'x': '406.00', 'y': '407.76', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '417.84', 'y': '401.19', 'h': '70.28', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '415.88', 'h': '23.63', 'w': '10.48'}, {'page': '7', 'x': '95.07', 'y': '420.40', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '107.14', 'y': '415.88', 'h': '380.98', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '429.83', 'h': '255.42', 'w': '10.48'}], [{'page': '7', 'x': '336.60', 'y': '429.83', 'h': '151.53', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '443.78', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '457.73', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '471.67', 'h': '32.52', 'w': '10.48'}], [{'page': '7', 'x': '110.92', 'y': '471.67', 'h': '377.21', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '485.62', 'h': '259.89', 'w': '10.48'}], [{'page': '7', 'x': '336.31', 'y': '485.62', 'h': '151.81', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '499.57', 'h': '128.06', 'w': '10.48'}], [{'page': '7', 'x': '205.50', 'y': '499.57', 'h': '186.74', 'w': '10.48'}, {'page': '7', 'x': '392.24', 'y': '497.95', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '410.67', 'y': '506.97', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '424.12', 'y': '499.57', 'h': '64.00', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '520.55', 'h': '26.99', 'w': '10.48'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1404.5900.pdf\", \"section_number\": \"3.\", \"para\": \"13\", \"_id\": \"80e7e135-2677-4f4d-ab27-2c32c5e25390\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Automata representation of successful strategies for social dilemmas\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '56.33', 'y': '542.41', 'h': '499.14', 'w': '8.85'}, {'page': '2', 'x': '56.69', 'y': '554.04', 'h': '83.39', 'w': '9.17'}], [{'page': '2', 'x': '143.17', 'y': '554.36', 'h': '276.03', 'w': '8.64'}], [{'page': '2', 'x': '422.28', 'y': '554.36', 'h': '133.03', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '566.00', 'h': '482.10', 'w': '9.73'}], [{'page': '2', 'x': '541.88', 'y': '566.32', 'h': '13.59', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '578.27', 'h': '498.96', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '590.23', 'h': '22.26', 'w': '8.64'}, {'page': '2', 'x': '78.95', 'y': '588.30', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '89.32', 'y': '590.23', 'h': '282.95', 'w': '8.64'}], [{'page': '2', 'x': '375.37', 'y': '590.23', 'h': '179.94', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '602.18', 'h': '498.62', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '614.14', 'h': '111.81', 'w': '8.64'}, {'page': '2', 'x': '168.50', 'y': '612.21', 'h': '43.20', 'w': '10.57'}], [{'page': '2', 'x': '71.64', 'y': '626.09', 'h': '393.33', 'w': '8.64'}], [{'page': '2', 'x': '468.05', 'y': '626.09', 'h': '88.50', 'w': '8.64'}, {'page': '2', 'x': '56.33', 'y': '638.05', 'h': '499.32', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '649.82', 'h': '111.52', 'w': '8.82'}], [{'page': '2', 'x': '171.31', 'y': '650.00', 'h': '227.28', 'w': '8.64'}, {'page': '2', 'x': '398.59', 'y': '648.07', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '408.96', 'y': '650.00', 'h': '146.35', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '661.96', 'h': '103.02', 'w': '8.64'}], [{'page': '2', 'x': '162.80', 'y': '661.96', 'h': '392.50', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '673.91', 'h': '109.93', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '673.91', 'h': '385.49', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '685.87', 'h': '238.77', 'w': '8.64'}]]\", \"text\": \"where the number of defectors among the two co-players is written at the top of each column , and \\u03c1 is a multiplication factor satisfying 1 < \\u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1910.02634.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"25eeb897-22f5-4e0a-953d-3e0221b31c04\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"metadata\": {\"text\": \"Playing from c 0 in M S , the strategy \\u03c3 \\u01eb behaves as follows.First it plays like strategy \\u03c3 in the corresponding game from c \\u2032 0 on M S \\u2032 .(Function \\u03bb connects the corresponding configurations in the two games.)When the game in M S \\u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \\u2265 s i (d(\\u01eb)) then \\u03c3 \\u01eb henceforth plays like \\u03c3 \\u01eb i , which ensures to reach the target X with probability \\u2265 1 -2\\u01eb.Otherwise, the configuration in M S is still too small to switch to \\u03c3 \\u01eb i .In this case, \\u03c3 \\u01eb continues to play like \\u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \\u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \\u2032 , i.e., component k(i) is pumped.Since we know that \\u03c3 on M S \\u2032 will almost surely visit X \\u2032 f or R, we obtain that \\u03c3 \\u01eb on M S will almost surely eventually visit X or some configuration \\u2265 s i (d(\\u01eb)) for i \\u2208 J (and from there achieve to reach the target with probability \\u2265 1 -2\\u01eb).Since every weighted average of probabilities \\u2265 1-2\\u01eb is still \\u2265 1-2\\u01eb, we obtain P(M S , c 0 , \\u03c3 \\u01eb , \\u2666X ) \\u2265 1 -2\\u01eb and thus P + (M S , c 0 , \\u2666X ) = 1.\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"B.7 Proof of Lemma 6\", \"bboxes\": \"[[{'page': '29', 'x': '149.76', 'y': '462.69', 'h': '253.56', 'w': '10.66'}], [{'page': '29', 'x': '406.32', 'y': '462.69', 'h': '74.26', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '474.69', 'h': '202.54', 'w': '9.96'}, {'page': '29', 'x': '337.32', 'y': '473.21', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '337.32', 'y': '479.33', 'h': '3.97', 'w': '6.97'}, {'page': '29', 'x': '346.20', 'y': '474.69', 'h': '32.19', 'w': '10.66'}, {'page': '29', 'x': '379.32', 'y': '474.69', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '386.52', 'y': '474.69', 'h': '94.05', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '486.57', 'h': '234.14', 'w': '9.96'}], [{'page': '29', 'x': '373.44', 'y': '486.57', 'h': '106.23', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '498.57', 'h': '344.97', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '510.23', 'h': '190.61', 'w': '10.87'}, {'page': '29', 'x': '325.68', 'y': '509.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '325.32', 'y': '515.33', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '329.52', 'y': '510.45', 'h': '151.46', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '522.23', 'h': '132.24', 'w': '10.18'}], [{'page': '29', 'x': '271.44', 'y': '522.45', 'h': '209.10', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '534.45', 'h': '89.93', 'w': '9.96'}, {'page': '29', 'x': '225.00', 'y': '533.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '224.64', 'y': '539.21', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '228.84', 'y': '534.45', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '235.68', 'y': '534.45', 'h': '244.90', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '546.33', 'h': '304.71', 'w': '10.66'}, {'page': '29', 'x': '444.48', 'y': '546.33', 'h': '36.10', 'w': '9.96'}, {'page': '29', 'x': '134.52', 'y': '558.33', 'h': '86.64', 'w': '9.96'}], [{'page': '29', 'x': '224.64', 'y': '558.33', 'h': '256.38', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '570.33', 'h': '172.32', 'w': '9.96'}], [{'page': '29', 'x': '311.52', 'y': '570.33', 'h': '169.02', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '582.21', 'h': '346.02', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '594.21', 'h': '58.59', 'w': '10.65'}, {'page': '29', 'x': '194.28', 'y': '594.21', 'h': '147.72', 'w': '9.96'}], [{'page': '29', 'x': '346.20', 'y': '594.21', 'h': '133.47', 'w': '10.65'}, {'page': '29', 'x': '134.40', 'y': '606.09', 'h': '113.61', 'w': '9.96'}, {'page': '29', 'x': '248.88', 'y': '604.73', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '248.04', 'y': '611.21', 'h': '3.88', 'w': '6.97'}, {'page': '29', 'x': '257.28', 'y': '606.09', 'h': '223.77', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '617.87', 'h': '345.82', 'w': '10.87'}, {'page': '29', 'x': '134.76', 'y': '629.87', 'h': '248.52', 'w': '10.18'}], [{'page': '29', 'x': '387.72', 'y': '630.09', 'h': '93.01', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '640.87', 'h': '269.22', 'w': '11.06'}, {'page': '29', 'x': '404.04', 'y': '641.75', 'h': '76.50', 'w': '10.87'}, {'page': '29', 'x': '134.28', 'y': '652.61', 'h': '83.04', 'w': '11.32'}, {'page': '29', 'x': '217.80', 'y': '653.75', 'h': '86.64', 'w': '10.87'}]]\", \"pages\": \"('29', '29')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"None\", \"para\": \"9\", \"_id\": \"6d58eaad-c55e-4a91-818c-2e06e60d32c0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}