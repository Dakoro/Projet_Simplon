{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x2eba2a24df2bacde", "trace_id": "0x3950b3beaf948786f6d2e2865d0bb13c"}, "parent_id": null, "start_time": 1724841662528466175, "end_time": 1724841663067780279, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"f9d76789403b44edbb8d0fe3a0fca45f\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\"}", "mlflow.spanOutputs": "{\"result\": \" AB and AB-DL\", \"source_documents\": [{\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"metadata\": {\"text\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"paper_title\": \"Present-Biased Optimization *\", \"section_title\": \"Our Results\", \"bboxes\": \"[[{'page': '4', 'x': '72.00', 'y': '449.82', 'h': '183.75', 'w': '10.91'}], [{'page': '4', 'x': '266.66', 'y': '449.82', 'h': '257.25', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '463.37', 'h': '451.92', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '476.92', 'h': '257.38', 'w': '10.91'}, {'page': '4', 'x': '333.67', 'y': '476.64', 'h': '11.69', 'w': '18.93'}, {'page': '4', 'x': '345.36', 'y': '474.93', 'h': '4.23', 'w': '13.82'}, {'page': '4', 'x': '345.36', 'y': '482.04', 'h': '4.23', 'w': '7.97'}, {'page': '4', 'x': '350.09', 'y': '476.92', 'h': '17.58', 'w': '10.91'}], [{'page': '4', 'x': '369.49', 'y': '474.93', 'h': '15.31', 'w': '13.82'}, {'page': '4', 'x': '380.57', 'y': '482.44', 'h': '4.41', 'w': '7.97'}, {'page': '4', 'x': '385.69', 'y': '476.64', 'h': '8.48', 'w': '18.93'}, {'page': '4', 'x': '398.62', 'y': '476.92', 'h': '36.54', 'w': '10.91'}, {'page': '4', 'x': '449.20', 'y': '476.64', 'h': '8.49', 'w': '18.93'}, {'page': '4', 'x': '461.80', 'y': '476.92', 'h': '9.05', 'w': '10.91'}], [{'page': '4', 'x': '477.65', 'y': '476.92', 'h': '46.26', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '490.47', 'h': '451.57', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '504.02', 'h': '295.18', 'w': '10.91'}], [{'page': '4', 'x': '374.45', 'y': '504.02', 'h': '149.47', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '517.57', 'h': '451.91', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '531.11', 'h': '359.73', 'w': '10.91'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.14736.pdf\", \"section_number\": \"1.1\", \"para\": \"4\", \"_id\": \"c21ccac4-4cd2-470f-8c53-1633b726ac8f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"metadata\": {\"text\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"paper_title\": \"Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach\", \"section_title\": \"Deriving tailored policies using past graph instances:\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '443.01', 'h': '452.49', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '454.96', 'h': '468.11', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '466.92', 'h': '51.77', 'w': '9.96'}], [{'page': '2', 'x': '128.20', 'y': '466.92', 'h': '408.15', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '478.87', 'h': '103.59', 'w': '9.96'}], [{'page': '2', 'x': '180.03', 'y': '478.87', 'h': '358.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '490.83', 'h': '288.82', 'w': '9.96'}], [{'page': '2', 'x': '365.25', 'y': '490.83', 'h': '172.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '502.78', 'h': '305.77', 'w': '9.96'}], [{'page': '2', 'x': '382.20', 'y': '502.78', 'h': '146.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '514.74', 'h': '466.95', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.10380.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"6eea14ef-5fe6-41e3-8890-a98f462fa226\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"Stable Dinner Party Seating Arrangements\", \"section_title\": \"Polynomial Solvability for k-Class Preferences\", \"bboxes\": \"[[{'page': '9', 'x': '149.71', 'y': '357.18', 'h': '146.38', 'w': '8.80'}], [{'page': '9', 'x': '299.99', 'y': '357.18', 'h': '180.60', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '369.13', 'h': '243.34', 'w': '9.35'}], [{'page': '9', 'x': '381.42', 'y': '369.13', 'h': '99.17', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '381.09', 'h': '191.55', 'w': '8.80'}], [{'page': '9', 'x': '327.98', 'y': '381.09', 'h': '99.35', 'w': '9.71'}], [{'page': '9', 'x': '428.99', 'y': '381.09', 'h': '51.61', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '393.04', 'h': '302.37', 'w': '9.71'}], [{'page': '9', 'x': '438.54', 'y': '393.10', 'h': '42.06', 'w': '9.29'}], [{'page': '9', 'x': '134.77', 'y': '405.00', 'h': '345.83', 'w': '10.08'}, {'page': '9', 'x': '134.77', 'y': '416.95', 'h': '345.83', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '428.91', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '440.86', 'h': '272.33', 'w': '8.80'}], [{'page': '9', 'x': '412.30', 'y': '440.86', 'h': '68.29', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '452.82', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '464.78', 'h': '328.70', 'w': '8.80'}], [{'page': '9', 'x': '466.76', 'y': '464.78', 'h': '13.84', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '476.73', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '488.69', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '500.64', 'h': '272.90', 'w': '8.80'}], [{'page': '9', 'x': '412.25', 'y': '500.64', 'h': '68.33', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '512.60', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '524.55', 'h': '667.36', 'w': '9.71'}, {'page': '9', 'x': '144.06', 'y': '536.51', 'h': '336.54', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '548.46', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '560.42', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '572.37', 'h': '345.83', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '584.33', 'h': '72.82', 'w': '8.80'}], [{'page': '9', 'x': '210.65', 'y': '584.33', 'h': '269.94', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '596.28', 'h': '36.71', 'w': '8.80'}], [{'page': '9', 'x': '175.54', 'y': '596.28', 'h': '305.06', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '608.24', 'h': '25.48', 'w': '8.80'}], [{'page': '9', 'x': '163.75', 'y': '608.24', 'h': '316.84', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '620.19', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '632.15', 'h': '176.93', 'w': '9.35'}], [{'page': '9', 'x': '315.80', 'y': '632.15', 'h': '164.78', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '644.10', 'h': '24.43', 'w': '8.80'}], [{'page': '9', 'x': '162.44', 'y': '644.10', 'h': '162.04', 'w': '8.80'}], [{'page': '9', 'x': '327.73', 'y': '644.10', 'h': '152.86', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '656.06', 'h': '217.85', 'w': '8.80'}], [{'page': '10', 'x': '134.77', 'y': '118.92', 'h': '345.83', 'w': '8.81'}, {'page': '10', 'x': '134.77', 'y': '130.89', 'h': '72.29', 'w': '8.80'}]]\", \"text\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.09549.pdf\", \"section_number\": \"5\", \"para\": \"18\", \"_id\": \"31289307-4c78-4fe6-bcd0-a2e5a491816b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Reward Shaping\", \"bboxes\": \"[[{'page': '3', 'x': '149.71', 'y': '353.63', 'h': '330.88', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '365.58', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '377.54', 'h': '59.87', 'w': '8.74'}], [{'page': '3', 'x': '197.02', 'y': '377.54', 'h': '283.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '389.49', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '401.45', 'h': '89.98', 'w': '8.74'}], [{'page': '3', 'x': '229.09', 'y': '401.45', 'h': '251.49', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '413.40', 'h': '294.09', 'w': '8.74'}], [{'page': '3', 'x': '431.31', 'y': '413.40', 'h': '49.28', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '425.36', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '437.31', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '449.27', 'h': '60.59', 'w': '8.74'}], [{'page': '3', 'x': '198.21', 'y': '449.27', 'h': '282.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '461.22', 'h': '201.36', 'w': '8.74'}]]\", \"text\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.1\", \"para\": \"4\", \"_id\": \"b7f445e2-861a-4d8a-8f44-abc181529169\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"metadata\": {\"text\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '604.16', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '616.11', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '628.07', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '640.02', 'h': '119.16', 'w': '9.33'}], [{'page': '9', 'x': '174.57', 'y': '640.02', 'h': '125.45', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '651.98', 'h': '225.34', 'w': '8.64'}], [{'page': '9', 'x': '278.17', 'y': '651.98', 'h': '21.85', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '663.93', 'h': '218.16', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"5eb50813-b8dd-45d7-9053-156a4c823d95\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning\", \"section_title\": \"Experimental setup\", \"bboxes\": \"[[{'page': '4', 'x': '329.46', 'y': '156.11', 'h': '228.53', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '167.07', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '178.03', 'h': '172.99', 'w': '8.64'}], [{'page': '4', 'x': '494.99', 'y': '178.03', 'h': '63.01', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '188.98', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '199.94', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '210.90', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '221.86', 'h': '20.75', 'w': '8.64'}], [{'page': '4', 'x': '343.84', 'y': '221.86', 'h': '214.17', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '232.82', 'h': '238.50', 'w': '8.64'}], [{'page': '4', 'x': '319.50', 'y': '243.78', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '254.74', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '265.70', 'h': '167.01', 'w': '8.64'}]]\", \"text\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.12062.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"782393e7-a352-4535-b9f3-c8cb687c936d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"metadata\": {\"pages\": \"('15', '15')\", \"paper_title\": \"A T E X template Contextually Aware Intelligent Control Agents for Heterogeneous Swarms\", \"section_title\": \"Shepherding Agent Behaviours\", \"bboxes\": \"[[{'page': '15', 'x': '51.02', 'y': '440.17', 'h': '36.14', 'w': '8.74'}, {'page': '15', 'x': '87.17', 'y': '438.60', 'h': '3.01', 'w': '6.12'}, {'page': '15', 'x': '87.17', 'y': '445.03', 'h': '9.53', 'w': '6.12'}, {'page': '15', 'x': '100.91', 'y': '440.17', 'h': '157.75', 'w': '8.74'}], [{'page': '15', 'x': '262.12', 'y': '440.17', 'h': '126.23', 'w': '8.74'}, {'page': '15', 'x': '51.02', 'y': '452.17', 'h': '186.30', 'w': '8.74'}]]\", \"text\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2211.12560.pdf\", \"section_number\": \"4.1\", \"para\": \"1\", \"_id\": \"61fec9c2-66a5-46c6-b26d-237b26f59520\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '88.94', 'y': '75.86', 'h': '451.07', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '89.41', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '102.77', 'h': '346.67', 'w': '9.64'}], [{'page': '2', 'x': '422.40', 'y': '102.96', 'h': '117.60', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '116.50', 'h': '221.60', 'w': '9.46'}], [{'page': '2', 'x': '296.72', 'y': '116.50', 'h': '243.28', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '130.05', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '143.60', 'h': '78.64', 'w': '9.46'}], [{'page': '2', 'x': '154.29', 'y': '143.60', 'h': '385.71', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '157.15', 'h': '121.14', 'w': '9.46'}], [{'page': '2', 'x': '198.45', 'y': '156.97', 'h': '341.56', 'w': '9.64'}, {'page': '2', 'x': '72.00', 'y': '170.52', 'h': '237.24', 'w': '9.39'}]]\", \"text\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"3f9ce1b2-9d4d-4b76-97aa-36a2cc06074f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0xf6348563c6e63bfc", "trace_id": "0x3950b3beaf948786f6d2e2865d0bb13c"}, "parent_id": "0x2eba2a24df2bacde", "start_time": 1724841662532355882, "end_time": 1724841662559611446, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"f9d76789403b44edbb8d0fe3a0fca45f\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\"", "mlflow.spanOutputs": "[{\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"metadata\": {\"text\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"paper_title\": \"Present-Biased Optimization *\", \"section_title\": \"Our Results\", \"bboxes\": \"[[{'page': '4', 'x': '72.00', 'y': '449.82', 'h': '183.75', 'w': '10.91'}], [{'page': '4', 'x': '266.66', 'y': '449.82', 'h': '257.25', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '463.37', 'h': '451.92', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '476.92', 'h': '257.38', 'w': '10.91'}, {'page': '4', 'x': '333.67', 'y': '476.64', 'h': '11.69', 'w': '18.93'}, {'page': '4', 'x': '345.36', 'y': '474.93', 'h': '4.23', 'w': '13.82'}, {'page': '4', 'x': '345.36', 'y': '482.04', 'h': '4.23', 'w': '7.97'}, {'page': '4', 'x': '350.09', 'y': '476.92', 'h': '17.58', 'w': '10.91'}], [{'page': '4', 'x': '369.49', 'y': '474.93', 'h': '15.31', 'w': '13.82'}, {'page': '4', 'x': '380.57', 'y': '482.44', 'h': '4.41', 'w': '7.97'}, {'page': '4', 'x': '385.69', 'y': '476.64', 'h': '8.48', 'w': '18.93'}, {'page': '4', 'x': '398.62', 'y': '476.92', 'h': '36.54', 'w': '10.91'}, {'page': '4', 'x': '449.20', 'y': '476.64', 'h': '8.49', 'w': '18.93'}, {'page': '4', 'x': '461.80', 'y': '476.92', 'h': '9.05', 'w': '10.91'}], [{'page': '4', 'x': '477.65', 'y': '476.92', 'h': '46.26', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '490.47', 'h': '451.57', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '504.02', 'h': '295.18', 'w': '10.91'}], [{'page': '4', 'x': '374.45', 'y': '504.02', 'h': '149.47', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '517.57', 'h': '451.91', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '531.11', 'h': '359.73', 'w': '10.91'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.14736.pdf\", \"section_number\": \"1.1\", \"para\": \"4\", \"_id\": \"c21ccac4-4cd2-470f-8c53-1633b726ac8f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"metadata\": {\"text\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"paper_title\": \"Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach\", \"section_title\": \"Deriving tailored policies using past graph instances:\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '443.01', 'h': '452.49', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '454.96', 'h': '468.11', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '466.92', 'h': '51.77', 'w': '9.96'}], [{'page': '2', 'x': '128.20', 'y': '466.92', 'h': '408.15', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '478.87', 'h': '103.59', 'w': '9.96'}], [{'page': '2', 'x': '180.03', 'y': '478.87', 'h': '358.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '490.83', 'h': '288.82', 'w': '9.96'}], [{'page': '2', 'x': '365.25', 'y': '490.83', 'h': '172.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '502.78', 'h': '305.77', 'w': '9.96'}], [{'page': '2', 'x': '382.20', 'y': '502.78', 'h': '146.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '514.74', 'h': '466.95', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.10380.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"6eea14ef-5fe6-41e3-8890-a98f462fa226\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"Stable Dinner Party Seating Arrangements\", \"section_title\": \"Polynomial Solvability for k-Class Preferences\", \"bboxes\": \"[[{'page': '9', 'x': '149.71', 'y': '357.18', 'h': '146.38', 'w': '8.80'}], [{'page': '9', 'x': '299.99', 'y': '357.18', 'h': '180.60', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '369.13', 'h': '243.34', 'w': '9.35'}], [{'page': '9', 'x': '381.42', 'y': '369.13', 'h': '99.17', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '381.09', 'h': '191.55', 'w': '8.80'}], [{'page': '9', 'x': '327.98', 'y': '381.09', 'h': '99.35', 'w': '9.71'}], [{'page': '9', 'x': '428.99', 'y': '381.09', 'h': '51.61', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '393.04', 'h': '302.37', 'w': '9.71'}], [{'page': '9', 'x': '438.54', 'y': '393.10', 'h': '42.06', 'w': '9.29'}], [{'page': '9', 'x': '134.77', 'y': '405.00', 'h': '345.83', 'w': '10.08'}, {'page': '9', 'x': '134.77', 'y': '416.95', 'h': '345.83', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '428.91', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '440.86', 'h': '272.33', 'w': '8.80'}], [{'page': '9', 'x': '412.30', 'y': '440.86', 'h': '68.29', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '452.82', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '464.78', 'h': '328.70', 'w': '8.80'}], [{'page': '9', 'x': '466.76', 'y': '464.78', 'h': '13.84', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '476.73', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '488.69', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '500.64', 'h': '272.90', 'w': '8.80'}], [{'page': '9', 'x': '412.25', 'y': '500.64', 'h': '68.33', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '512.60', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '524.55', 'h': '667.36', 'w': '9.71'}, {'page': '9', 'x': '144.06', 'y': '536.51', 'h': '336.54', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '548.46', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '560.42', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '572.37', 'h': '345.83', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '584.33', 'h': '72.82', 'w': '8.80'}], [{'page': '9', 'x': '210.65', 'y': '584.33', 'h': '269.94', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '596.28', 'h': '36.71', 'w': '8.80'}], [{'page': '9', 'x': '175.54', 'y': '596.28', 'h': '305.06', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '608.24', 'h': '25.48', 'w': '8.80'}], [{'page': '9', 'x': '163.75', 'y': '608.24', 'h': '316.84', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '620.19', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '632.15', 'h': '176.93', 'w': '9.35'}], [{'page': '9', 'x': '315.80', 'y': '632.15', 'h': '164.78', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '644.10', 'h': '24.43', 'w': '8.80'}], [{'page': '9', 'x': '162.44', 'y': '644.10', 'h': '162.04', 'w': '8.80'}], [{'page': '9', 'x': '327.73', 'y': '644.10', 'h': '152.86', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '656.06', 'h': '217.85', 'w': '8.80'}], [{'page': '10', 'x': '134.77', 'y': '118.92', 'h': '345.83', 'w': '8.81'}, {'page': '10', 'x': '134.77', 'y': '130.89', 'h': '72.29', 'w': '8.80'}]]\", \"text\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.09549.pdf\", \"section_number\": \"5\", \"para\": \"18\", \"_id\": \"31289307-4c78-4fe6-bcd0-a2e5a491816b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Reward Shaping\", \"bboxes\": \"[[{'page': '3', 'x': '149.71', 'y': '353.63', 'h': '330.88', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '365.58', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '377.54', 'h': '59.87', 'w': '8.74'}], [{'page': '3', 'x': '197.02', 'y': '377.54', 'h': '283.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '389.49', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '401.45', 'h': '89.98', 'w': '8.74'}], [{'page': '3', 'x': '229.09', 'y': '401.45', 'h': '251.49', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '413.40', 'h': '294.09', 'w': '8.74'}], [{'page': '3', 'x': '431.31', 'y': '413.40', 'h': '49.28', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '425.36', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '437.31', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '449.27', 'h': '60.59', 'w': '8.74'}], [{'page': '3', 'x': '198.21', 'y': '449.27', 'h': '282.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '461.22', 'h': '201.36', 'w': '8.74'}]]\", \"text\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.1\", \"para\": \"4\", \"_id\": \"b7f445e2-861a-4d8a-8f44-abc181529169\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"metadata\": {\"text\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '604.16', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '616.11', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '628.07', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '640.02', 'h': '119.16', 'w': '9.33'}], [{'page': '9', 'x': '174.57', 'y': '640.02', 'h': '125.45', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '651.98', 'h': '225.34', 'w': '8.64'}], [{'page': '9', 'x': '278.17', 'y': '651.98', 'h': '21.85', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '663.93', 'h': '218.16', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"5eb50813-b8dd-45d7-9053-156a4c823d95\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning\", \"section_title\": \"Experimental setup\", \"bboxes\": \"[[{'page': '4', 'x': '329.46', 'y': '156.11', 'h': '228.53', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '167.07', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '178.03', 'h': '172.99', 'w': '8.64'}], [{'page': '4', 'x': '494.99', 'y': '178.03', 'h': '63.01', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '188.98', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '199.94', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '210.90', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '221.86', 'h': '20.75', 'w': '8.64'}], [{'page': '4', 'x': '343.84', 'y': '221.86', 'h': '214.17', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '232.82', 'h': '238.50', 'w': '8.64'}], [{'page': '4', 'x': '319.50', 'y': '243.78', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '254.74', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '265.70', 'h': '167.01', 'w': '8.64'}]]\", \"text\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.12062.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"782393e7-a352-4535-b9f3-c8cb687c936d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"metadata\": {\"pages\": \"('15', '15')\", \"paper_title\": \"A T E X template Contextually Aware Intelligent Control Agents for Heterogeneous Swarms\", \"section_title\": \"Shepherding Agent Behaviours\", \"bboxes\": \"[[{'page': '15', 'x': '51.02', 'y': '440.17', 'h': '36.14', 'w': '8.74'}, {'page': '15', 'x': '87.17', 'y': '438.60', 'h': '3.01', 'w': '6.12'}, {'page': '15', 'x': '87.17', 'y': '445.03', 'h': '9.53', 'w': '6.12'}, {'page': '15', 'x': '100.91', 'y': '440.17', 'h': '157.75', 'w': '8.74'}], [{'page': '15', 'x': '262.12', 'y': '440.17', 'h': '126.23', 'w': '8.74'}, {'page': '15', 'x': '51.02', 'y': '452.17', 'h': '186.30', 'w': '8.74'}]]\", \"text\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2211.12560.pdf\", \"section_number\": \"4.1\", \"para\": \"1\", \"_id\": \"61fec9c2-66a5-46c6-b26d-237b26f59520\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '88.94', 'y': '75.86', 'h': '451.07', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '89.41', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '102.77', 'h': '346.67', 'w': '9.64'}], [{'page': '2', 'x': '422.40', 'y': '102.96', 'h': '117.60', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '116.50', 'h': '221.60', 'w': '9.46'}], [{'page': '2', 'x': '296.72', 'y': '116.50', 'h': '243.28', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '130.05', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '143.60', 'h': '78.64', 'w': '9.46'}], [{'page': '2', 'x': '154.29', 'y': '143.60', 'h': '385.71', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '157.15', 'h': '121.14', 'w': '9.46'}], [{'page': '2', 'x': '198.45', 'y': '156.97', 'h': '341.56', 'w': '9.64'}, {'page': '2', 'x': '72.00', 'y': '170.52', 'h': '237.24', 'w': '9.39'}]]\", \"text\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"3f9ce1b2-9d4d-4b76-97aa-36a2cc06074f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0xdfc4cabc25e10e69", "trace_id": "0x3950b3beaf948786f6d2e2865d0bb13c"}, "parent_id": "0x2eba2a24df2bacde", "start_time": 1724841662560408171, "end_time": 1724841663066748095, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"f9d76789403b44edbb8d0fe3a0fca45f\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"metadata\": {\"text\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"paper_title\": \"Present-Biased Optimization *\", \"section_title\": \"Our Results\", \"bboxes\": \"[[{'page': '4', 'x': '72.00', 'y': '449.82', 'h': '183.75', 'w': '10.91'}], [{'page': '4', 'x': '266.66', 'y': '449.82', 'h': '257.25', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '463.37', 'h': '451.92', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '476.92', 'h': '257.38', 'w': '10.91'}, {'page': '4', 'x': '333.67', 'y': '476.64', 'h': '11.69', 'w': '18.93'}, {'page': '4', 'x': '345.36', 'y': '474.93', 'h': '4.23', 'w': '13.82'}, {'page': '4', 'x': '345.36', 'y': '482.04', 'h': '4.23', 'w': '7.97'}, {'page': '4', 'x': '350.09', 'y': '476.92', 'h': '17.58', 'w': '10.91'}], [{'page': '4', 'x': '369.49', 'y': '474.93', 'h': '15.31', 'w': '13.82'}, {'page': '4', 'x': '380.57', 'y': '482.44', 'h': '4.41', 'w': '7.97'}, {'page': '4', 'x': '385.69', 'y': '476.64', 'h': '8.48', 'w': '18.93'}, {'page': '4', 'x': '398.62', 'y': '476.92', 'h': '36.54', 'w': '10.91'}, {'page': '4', 'x': '449.20', 'y': '476.64', 'h': '8.49', 'w': '18.93'}, {'page': '4', 'x': '461.80', 'y': '476.92', 'h': '9.05', 'w': '10.91'}], [{'page': '4', 'x': '477.65', 'y': '476.92', 'h': '46.26', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '490.47', 'h': '451.57', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '504.02', 'h': '295.18', 'w': '10.91'}], [{'page': '4', 'x': '374.45', 'y': '504.02', 'h': '149.47', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '517.57', 'h': '451.91', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '531.11', 'h': '359.73', 'w': '10.91'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.14736.pdf\", \"section_number\": \"1.1\", \"para\": \"4\", \"_id\": \"c21ccac4-4cd2-470f-8c53-1633b726ac8f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"metadata\": {\"text\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"paper_title\": \"Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach\", \"section_title\": \"Deriving tailored policies using past graph instances:\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '443.01', 'h': '452.49', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '454.96', 'h': '468.11', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '466.92', 'h': '51.77', 'w': '9.96'}], [{'page': '2', 'x': '128.20', 'y': '466.92', 'h': '408.15', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '478.87', 'h': '103.59', 'w': '9.96'}], [{'page': '2', 'x': '180.03', 'y': '478.87', 'h': '358.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '490.83', 'h': '288.82', 'w': '9.96'}], [{'page': '2', 'x': '365.25', 'y': '490.83', 'h': '172.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '502.78', 'h': '305.77', 'w': '9.96'}], [{'page': '2', 'x': '382.20', 'y': '502.78', 'h': '146.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '514.74', 'h': '466.95', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.10380.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"6eea14ef-5fe6-41e3-8890-a98f462fa226\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"Stable Dinner Party Seating Arrangements\", \"section_title\": \"Polynomial Solvability for k-Class Preferences\", \"bboxes\": \"[[{'page': '9', 'x': '149.71', 'y': '357.18', 'h': '146.38', 'w': '8.80'}], [{'page': '9', 'x': '299.99', 'y': '357.18', 'h': '180.60', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '369.13', 'h': '243.34', 'w': '9.35'}], [{'page': '9', 'x': '381.42', 'y': '369.13', 'h': '99.17', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '381.09', 'h': '191.55', 'w': '8.80'}], [{'page': '9', 'x': '327.98', 'y': '381.09', 'h': '99.35', 'w': '9.71'}], [{'page': '9', 'x': '428.99', 'y': '381.09', 'h': '51.61', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '393.04', 'h': '302.37', 'w': '9.71'}], [{'page': '9', 'x': '438.54', 'y': '393.10', 'h': '42.06', 'w': '9.29'}], [{'page': '9', 'x': '134.77', 'y': '405.00', 'h': '345.83', 'w': '10.08'}, {'page': '9', 'x': '134.77', 'y': '416.95', 'h': '345.83', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '428.91', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '440.86', 'h': '272.33', 'w': '8.80'}], [{'page': '9', 'x': '412.30', 'y': '440.86', 'h': '68.29', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '452.82', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '464.78', 'h': '328.70', 'w': '8.80'}], [{'page': '9', 'x': '466.76', 'y': '464.78', 'h': '13.84', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '476.73', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '488.69', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '500.64', 'h': '272.90', 'w': '8.80'}], [{'page': '9', 'x': '412.25', 'y': '500.64', 'h': '68.33', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '512.60', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '524.55', 'h': '667.36', 'w': '9.71'}, {'page': '9', 'x': '144.06', 'y': '536.51', 'h': '336.54', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '548.46', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '560.42', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '572.37', 'h': '345.83', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '584.33', 'h': '72.82', 'w': '8.80'}], [{'page': '9', 'x': '210.65', 'y': '584.33', 'h': '269.94', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '596.28', 'h': '36.71', 'w': '8.80'}], [{'page': '9', 'x': '175.54', 'y': '596.28', 'h': '305.06', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '608.24', 'h': '25.48', 'w': '8.80'}], [{'page': '9', 'x': '163.75', 'y': '608.24', 'h': '316.84', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '620.19', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '632.15', 'h': '176.93', 'w': '9.35'}], [{'page': '9', 'x': '315.80', 'y': '632.15', 'h': '164.78', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '644.10', 'h': '24.43', 'w': '8.80'}], [{'page': '9', 'x': '162.44', 'y': '644.10', 'h': '162.04', 'w': '8.80'}], [{'page': '9', 'x': '327.73', 'y': '644.10', 'h': '152.86', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '656.06', 'h': '217.85', 'w': '8.80'}], [{'page': '10', 'x': '134.77', 'y': '118.92', 'h': '345.83', 'w': '8.81'}, {'page': '10', 'x': '134.77', 'y': '130.89', 'h': '72.29', 'w': '8.80'}]]\", \"text\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.09549.pdf\", \"section_number\": \"5\", \"para\": \"18\", \"_id\": \"31289307-4c78-4fe6-bcd0-a2e5a491816b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Reward Shaping\", \"bboxes\": \"[[{'page': '3', 'x': '149.71', 'y': '353.63', 'h': '330.88', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '365.58', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '377.54', 'h': '59.87', 'w': '8.74'}], [{'page': '3', 'x': '197.02', 'y': '377.54', 'h': '283.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '389.49', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '401.45', 'h': '89.98', 'w': '8.74'}], [{'page': '3', 'x': '229.09', 'y': '401.45', 'h': '251.49', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '413.40', 'h': '294.09', 'w': '8.74'}], [{'page': '3', 'x': '431.31', 'y': '413.40', 'h': '49.28', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '425.36', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '437.31', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '449.27', 'h': '60.59', 'w': '8.74'}], [{'page': '3', 'x': '198.21', 'y': '449.27', 'h': '282.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '461.22', 'h': '201.36', 'w': '8.74'}]]\", \"text\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.1\", \"para\": \"4\", \"_id\": \"b7f445e2-861a-4d8a-8f44-abc181529169\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"metadata\": {\"text\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '604.16', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '616.11', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '628.07', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '640.02', 'h': '119.16', 'w': '9.33'}], [{'page': '9', 'x': '174.57', 'y': '640.02', 'h': '125.45', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '651.98', 'h': '225.34', 'w': '8.64'}], [{'page': '9', 'x': '278.17', 'y': '651.98', 'h': '21.85', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '663.93', 'h': '218.16', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"5eb50813-b8dd-45d7-9053-156a4c823d95\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning\", \"section_title\": \"Experimental setup\", \"bboxes\": \"[[{'page': '4', 'x': '329.46', 'y': '156.11', 'h': '228.53', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '167.07', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '178.03', 'h': '172.99', 'w': '8.64'}], [{'page': '4', 'x': '494.99', 'y': '178.03', 'h': '63.01', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '188.98', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '199.94', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '210.90', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '221.86', 'h': '20.75', 'w': '8.64'}], [{'page': '4', 'x': '343.84', 'y': '221.86', 'h': '214.17', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '232.82', 'h': '238.50', 'w': '8.64'}], [{'page': '4', 'x': '319.50', 'y': '243.78', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '254.74', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '265.70', 'h': '167.01', 'w': '8.64'}]]\", \"text\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.12062.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"782393e7-a352-4535-b9f3-c8cb687c936d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"metadata\": {\"pages\": \"('15', '15')\", \"paper_title\": \"A T E X template Contextually Aware Intelligent Control Agents for Heterogeneous Swarms\", \"section_title\": \"Shepherding Agent Behaviours\", \"bboxes\": \"[[{'page': '15', 'x': '51.02', 'y': '440.17', 'h': '36.14', 'w': '8.74'}, {'page': '15', 'x': '87.17', 'y': '438.60', 'h': '3.01', 'w': '6.12'}, {'page': '15', 'x': '87.17', 'y': '445.03', 'h': '9.53', 'w': '6.12'}, {'page': '15', 'x': '100.91', 'y': '440.17', 'h': '157.75', 'w': '8.74'}], [{'page': '15', 'x': '262.12', 'y': '440.17', 'h': '126.23', 'w': '8.74'}, {'page': '15', 'x': '51.02', 'y': '452.17', 'h': '186.30', 'w': '8.74'}]]\", \"text\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2211.12560.pdf\", \"section_number\": \"4.1\", \"para\": \"1\", \"_id\": \"61fec9c2-66a5-46c6-b26d-237b26f59520\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '88.94', 'y': '75.86', 'h': '451.07', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '89.41', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '102.77', 'h': '346.67', 'w': '9.64'}], [{'page': '2', 'x': '422.40', 'y': '102.96', 'h': '117.60', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '116.50', 'h': '221.60', 'w': '9.46'}], [{'page': '2', 'x': '296.72', 'y': '116.50', 'h': '243.28', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '130.05', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '143.60', 'h': '78.64', 'w': '9.46'}], [{'page': '2', 'x': '154.29', 'y': '143.60', 'h': '385.71', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '157.15', 'h': '121.14', 'w': '9.46'}], [{'page': '2', 'x': '198.45', 'y': '156.97', 'h': '341.56', 'w': '9.64'}, {'page': '2', 'x': '72.00', 'y': '170.52', 'h': '237.24', 'w': '9.39'}]]\", \"text\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"3f9ce1b2-9d4d-4b76-97aa-36a2cc06074f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\"}", "mlflow.spanOutputs": "{\"output_text\": \" AB and AB-DL\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x9d6110eec111fdc2", "trace_id": "0x3950b3beaf948786f6d2e2865d0bb13c"}, "parent_id": "0xdfc4cabc25e10e69", "start_time": 1724841662561295304, "end_time": 1724841663066444109, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"f9d76789403b44edbb8d0fe3a0fca45f\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\", \"context\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\\n\\nMinimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\\n\\nLeveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\\n\\nWe begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\\n\\nBrys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\\n\\nDawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\\n\\nTo keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\\n\\nwhere \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\\n\\nA promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\\n\\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\"}", "mlflow.spanOutputs": "{\"text\": \" AB and AB-DL\"}"}, "events": [{"name": "text", "timestamp": 1724841662561388, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\n\nMinimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\n\nLeveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\n\nWe begin with the case of paths.For simplicity, we assume that n \u011b 3, as for n \u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \" n.For ease of writing, we will see arrangements as sequences s \" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \"class a likes class b.\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \" s n`1 \" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \u011b p b pdq `pb pf q and p e pdq pe pf q \u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \u011b p b pdq and p c pdq \u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \"or\" instead of \"and.\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \u010f i \u0103 j \u010f n the triplets ps i\u00b41 , s i , s i`1 q and ps j\u00b41 , s j , s j`1 q are long-range compatible when j \u00b4i \u0105 1 and short-range compatible when j \u00b4i \" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\n\nBrys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\n\nDawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\n\nTo keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\n\nwhere \u2126 t \u03b2\u03c0 is the set of \u03c0 agents \u03b2 operates on.We summarise the algorithm by Str\u00f6mbom et al. (2014) in Algorithm 2.\n\nA promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\n\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\n\nQuestion: What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x71e87ad588555e97", "trace_id": "0x3950b3beaf948786f6d2e2865d0bb13c"}, "parent_id": "0x9d6110eec111fdc2", "start_time": 1724841662561707064, "end_time": 1724841663066121781, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"f9d76789403b44edbb8d0fe3a0fca45f\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\\n\\nMinimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\\n\\nLeveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\\n\\nWe begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\\n\\nBrys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\\n\\nDawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\\n\\nTo keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\\n\\nwhere \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\\n\\nA promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\\n\\nHowever, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\\n\\nQuestion: What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" AB and AB-DL\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 1865, \"completion_tokens\": 5, \"total_tokens\": 1870}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?\"}", "response": "{\"result\": \" AB and AB-DL\", \"source_documents\": [{\"page_content\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. ConnectFour\", \"bboxes\": \"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]\", \"text\": \"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"metadata\": {\"text\": \"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \\u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \\u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.\", \"paper_title\": \"Present-Biased Optimization *\", \"section_title\": \"Our Results\", \"bboxes\": \"[[{'page': '4', 'x': '72.00', 'y': '449.82', 'h': '183.75', 'w': '10.91'}], [{'page': '4', 'x': '266.66', 'y': '449.82', 'h': '257.25', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '463.37', 'h': '451.92', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '476.92', 'h': '257.38', 'w': '10.91'}, {'page': '4', 'x': '333.67', 'y': '476.64', 'h': '11.69', 'w': '18.93'}, {'page': '4', 'x': '345.36', 'y': '474.93', 'h': '4.23', 'w': '13.82'}, {'page': '4', 'x': '345.36', 'y': '482.04', 'h': '4.23', 'w': '7.97'}, {'page': '4', 'x': '350.09', 'y': '476.92', 'h': '17.58', 'w': '10.91'}], [{'page': '4', 'x': '369.49', 'y': '474.93', 'h': '15.31', 'w': '13.82'}, {'page': '4', 'x': '380.57', 'y': '482.44', 'h': '4.41', 'w': '7.97'}, {'page': '4', 'x': '385.69', 'y': '476.64', 'h': '8.48', 'w': '18.93'}, {'page': '4', 'x': '398.62', 'y': '476.92', 'h': '36.54', 'w': '10.91'}, {'page': '4', 'x': '449.20', 'y': '476.64', 'h': '8.49', 'w': '18.93'}, {'page': '4', 'x': '461.80', 'y': '476.92', 'h': '9.05', 'w': '10.91'}], [{'page': '4', 'x': '477.65', 'y': '476.92', 'h': '46.26', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '490.47', 'h': '451.57', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '504.02', 'h': '295.18', 'w': '10.91'}], [{'page': '4', 'x': '374.45', 'y': '504.02', 'h': '149.47', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '517.57', 'h': '451.91', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '531.11', 'h': '359.73', 'w': '10.91'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.14736.pdf\", \"section_number\": \"1.1\", \"para\": \"4\", \"_id\": \"c21ccac4-4cd2-470f-8c53-1633b726ac8f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"metadata\": {\"text\": \"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.\", \"paper_title\": \"Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach\", \"section_title\": \"Deriving tailored policies using past graph instances:\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '443.01', 'h': '452.49', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '454.96', 'h': '468.11', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '466.92', 'h': '51.77', 'w': '9.96'}], [{'page': '2', 'x': '128.20', 'y': '466.92', 'h': '408.15', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '478.87', 'h': '103.59', 'w': '9.96'}], [{'page': '2', 'x': '180.03', 'y': '478.87', 'h': '358.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '490.83', 'h': '288.82', 'w': '9.96'}], [{'page': '2', 'x': '365.25', 'y': '490.83', 'h': '172.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '502.78', 'h': '305.77', 'w': '9.96'}], [{'page': '2', 'x': '382.20', 'y': '502.78', 'h': '146.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '514.74', 'h': '466.95', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.10380.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"6eea14ef-5fe6-41e3-8890-a98f462fa226\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"Stable Dinner Party Seating Arrangements\", \"section_title\": \"Polynomial Solvability for k-Class Preferences\", \"bboxes\": \"[[{'page': '9', 'x': '149.71', 'y': '357.18', 'h': '146.38', 'w': '8.80'}], [{'page': '9', 'x': '299.99', 'y': '357.18', 'h': '180.60', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '369.13', 'h': '243.34', 'w': '9.35'}], [{'page': '9', 'x': '381.42', 'y': '369.13', 'h': '99.17', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '381.09', 'h': '191.55', 'w': '8.80'}], [{'page': '9', 'x': '327.98', 'y': '381.09', 'h': '99.35', 'w': '9.71'}], [{'page': '9', 'x': '428.99', 'y': '381.09', 'h': '51.61', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '393.04', 'h': '302.37', 'w': '9.71'}], [{'page': '9', 'x': '438.54', 'y': '393.10', 'h': '42.06', 'w': '9.29'}], [{'page': '9', 'x': '134.77', 'y': '405.00', 'h': '345.83', 'w': '10.08'}, {'page': '9', 'x': '134.77', 'y': '416.95', 'h': '345.83', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '428.91', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '440.86', 'h': '272.33', 'w': '8.80'}], [{'page': '9', 'x': '412.30', 'y': '440.86', 'h': '68.29', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '452.82', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '464.78', 'h': '328.70', 'w': '8.80'}], [{'page': '9', 'x': '466.76', 'y': '464.78', 'h': '13.84', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '476.73', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '488.69', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '500.64', 'h': '272.90', 'w': '8.80'}], [{'page': '9', 'x': '412.25', 'y': '500.64', 'h': '68.33', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '512.60', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '524.55', 'h': '667.36', 'w': '9.71'}, {'page': '9', 'x': '144.06', 'y': '536.51', 'h': '336.54', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '548.46', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '560.42', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '572.37', 'h': '345.83', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '584.33', 'h': '72.82', 'w': '8.80'}], [{'page': '9', 'x': '210.65', 'y': '584.33', 'h': '269.94', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '596.28', 'h': '36.71', 'w': '8.80'}], [{'page': '9', 'x': '175.54', 'y': '596.28', 'h': '305.06', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '608.24', 'h': '25.48', 'w': '8.80'}], [{'page': '9', 'x': '163.75', 'y': '608.24', 'h': '316.84', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '620.19', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '632.15', 'h': '176.93', 'w': '9.35'}], [{'page': '9', 'x': '315.80', 'y': '632.15', 'h': '164.78', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '644.10', 'h': '24.43', 'w': '8.80'}], [{'page': '9', 'x': '162.44', 'y': '644.10', 'h': '162.04', 'w': '8.80'}], [{'page': '9', 'x': '327.73', 'y': '644.10', 'h': '152.86', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '656.06', 'h': '217.85', 'w': '8.80'}], [{'page': '10', 'x': '134.77', 'y': '118.92', 'h': '345.83', 'w': '8.81'}, {'page': '10', 'x': '134.77', 'y': '130.89', 'h': '72.29', 'w': '8.80'}]]\", \"text\": \"We begin with the case of paths.For simplicity, we assume that n \\u011b 3, as for n \\u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \\\" n.For ease of writing, we will see arrangements as sequences s \\\" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \\\"class a likes class b.\\\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \\\" s n`1 \\\" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \\u011b p b pdq `pb pf q and p e pdq pe pf q \\u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \\u011b p b pdq and p c pdq \\u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \\\"or\\\" instead of \\\"and.\\\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \\u010f i \\u0103 j \\u010f n the triplets ps i\\u00b41 , s i , s i`1 q and ps j\\u00b41 , s j , s j`1 q are long-range compatible when j \\u00b4i \\u0105 1 and short-range compatible when j \\u00b4i \\\" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.09549.pdf\", \"section_number\": \"5\", \"para\": \"18\", \"_id\": \"31289307-4c78-4fe6-bcd0-a2e5a491816b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Reward Shaping\", \"bboxes\": \"[[{'page': '3', 'x': '149.71', 'y': '353.63', 'h': '330.88', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '365.58', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '377.54', 'h': '59.87', 'w': '8.74'}], [{'page': '3', 'x': '197.02', 'y': '377.54', 'h': '283.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '389.49', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '401.45', 'h': '89.98', 'w': '8.74'}], [{'page': '3', 'x': '229.09', 'y': '401.45', 'h': '251.49', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '413.40', 'h': '294.09', 'w': '8.74'}], [{'page': '3', 'x': '431.31', 'y': '413.40', 'h': '49.28', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '425.36', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '437.31', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '449.27', 'h': '60.59', 'w': '8.74'}], [{'page': '3', 'x': '198.21', 'y': '449.27', 'h': '282.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '461.22', 'h': '201.36', 'w': '8.74'}]]\", \"text\": \"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.1\", \"para\": \"4\", \"_id\": \"b7f445e2-861a-4d8a-8f44-abc181529169\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"metadata\": {\"text\": \"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '604.16', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '616.11', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '628.07', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '640.02', 'h': '119.16', 'w': '9.33'}], [{'page': '9', 'x': '174.57', 'y': '640.02', 'h': '125.45', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '651.98', 'h': '225.34', 'w': '8.64'}], [{'page': '9', 'x': '278.17', 'y': '651.98', 'h': '21.85', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '663.93', 'h': '218.16', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"5eb50813-b8dd-45d7-9053-156a4c823d95\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"metadata\": {\"pages\": \"('4', '4')\", \"paper_title\": \"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning\", \"section_title\": \"Experimental setup\", \"bboxes\": \"[[{'page': '4', 'x': '329.46', 'y': '156.11', 'h': '228.53', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '167.07', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '178.03', 'h': '172.99', 'w': '8.64'}], [{'page': '4', 'x': '494.99', 'y': '178.03', 'h': '63.01', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '188.98', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '199.94', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '210.90', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '221.86', 'h': '20.75', 'w': '8.64'}], [{'page': '4', 'x': '343.84', 'y': '221.86', 'h': '214.17', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '232.82', 'h': '238.50', 'w': '8.64'}], [{'page': '4', 'x': '319.50', 'y': '243.78', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '254.74', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '265.70', 'h': '167.01', 'w': '8.64'}]]\", \"text\": \"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.12062.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"782393e7-a352-4535-b9f3-c8cb687c936d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"metadata\": {\"pages\": \"('15', '15')\", \"paper_title\": \"A T E X template Contextually Aware Intelligent Control Agents for Heterogeneous Swarms\", \"section_title\": \"Shepherding Agent Behaviours\", \"bboxes\": \"[[{'page': '15', 'x': '51.02', 'y': '440.17', 'h': '36.14', 'w': '8.74'}, {'page': '15', 'x': '87.17', 'y': '438.60', 'h': '3.01', 'w': '6.12'}, {'page': '15', 'x': '87.17', 'y': '445.03', 'h': '9.53', 'w': '6.12'}, {'page': '15', 'x': '100.91', 'y': '440.17', 'h': '157.75', 'w': '8.74'}], [{'page': '15', 'x': '262.12', 'y': '440.17', 'h': '126.23', 'w': '8.74'}, {'page': '15', 'x': '51.02', 'y': '452.17', 'h': '186.30', 'w': '8.74'}]]\", \"text\": \"where \\u2126 t \\u03b2\\u03c0 is the set of \\u03c0 agents \\u03b2 operates on.We summarise the algorithm by Str\\u00f6mbom et al. (2014) in Algorithm 2.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2211.12560.pdf\", \"section_number\": \"4.1\", \"para\": \"1\", \"_id\": \"61fec9c2-66a5-46c6-b26d-237b26f59520\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '88.94', 'y': '75.86', 'h': '451.07', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '89.41', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '102.77', 'h': '346.67', 'w': '9.64'}], [{'page': '2', 'x': '422.40', 'y': '102.96', 'h': '117.60', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '116.50', 'h': '221.60', 'w': '9.46'}], [{'page': '2', 'x': '296.72', 'y': '116.50', 'h': '243.28', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '130.05', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '143.60', 'h': '78.64', 'w': '9.46'}], [{'page': '2', 'x': '154.29', 'y': '143.60', 'h': '385.71', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '157.15', 'h': '121.14', 'w': '9.46'}], [{'page': '2', 'x': '198.45', 'y': '156.97', 'h': '341.56', 'w': '9.64'}, {'page': '2', 'x': '72.00', 'y': '170.52', 'h': '237.24', 'w': '9.39'}]]\", \"text\": \"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1\", \"para\": \"4\", \"_id\": \"3f9ce1b2-9d4d-4b76-97aa-36a2cc06074f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"metadata\": {\"text\": \"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"412b28e4-f9e3-4960-bdfe-c029c812cab4\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}