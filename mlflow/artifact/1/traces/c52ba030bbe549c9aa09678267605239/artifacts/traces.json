{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0xaa2d48bb15fe8aa2", "trace_id": "0xc8c8f94a22df7ed1e768f3811a3d1b27"}, "parent_id": null, "start_time": 1724841658397552807, "end_time": 1724841660225571322, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c52ba030bbe549c9aa09678267605239\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\"}", "mlflow.spanOutputs": "{\"result\": \" The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.\", \"source_documents\": [{\"page_content\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '256.28', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '268.23', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '280.19', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '292.14', 'h': '38.91', 'w': '8.64'}], [{'page': '10', 'x': '353.40', 'y': '292.14', 'h': '209.63', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '303.92', 'h': '251.06', 'w': '8.82'}, {'page': '10', 'x': '311.98', 'y': '316.05', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '328.01', 'h': '40.11', 'w': '8.64'}]]\", \"text\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"69bf60a6-d4ec-4964-8f63-b3ad6b7b089b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"metadata\": {\"text\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '339.55', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '351.51', 'h': '141.76', 'w': '8.64'}], [{'page': '10', 'x': '456.15', 'y': '351.51', 'h': '106.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '363.46', 'h': '183.86', 'w': '8.64'}], [{'page': '10', 'x': '501.15', 'y': '363.46', 'h': '61.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '375.42', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '387.37', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '399.33', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '411.28', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '423.24', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '435.20', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '447.15', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '459.11', 'h': '29.06', 'w': '8.64'}], [{'page': '10', 'x': '343.61', 'y': '459.11', 'h': '219.43', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '471.06', 'h': '124.56', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"414b691d-581f-4c25-beeb-d65b9c0035cb\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"metadata\": {\"pages\": \"('17', '17')\", \"paper_title\": \"On the optimal pivot path of simplex method for linear programming based on reinforcement learning\", \"section_title\": \"Extracting Multiple Shortest Pivot Paths\", \"bboxes\": \"[[{'page': '17', 'x': '148.71', 'y': '148.88', 'h': '328.77', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '166.81', 'h': '94.83', 'w': '8.74'}], [{'page': '17', 'x': '233.59', 'y': '166.81', 'h': '243.89', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '184.75', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '202.68', 'h': '81.04', 'w': '8.74'}], [{'page': '17', 'x': '222.69', 'y': '202.68', 'h': '254.79', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '220.61', 'h': '189.98', 'w': '8.74'}], [{'page': '17', 'x': '328.71', 'y': '220.61', 'h': '148.76', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '238.55', 'h': '77.42', 'w': '8.74'}], [{'page': '17', 'x': '216.01', 'y': '238.55', 'h': '261.47', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '256.48', 'h': '120.24', 'w': '8.74'}], [{'page': '17', 'x': '258.08', 'y': '256.48', 'h': '219.40', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '274.41', 'h': '279.13', 'w': '8.74'}], [{'page': '17', 'x': '419.27', 'y': '274.41', 'h': '58.21', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '292.34', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '310.28', 'h': '256.24', 'w': '8.74'}], [{'page': '17', 'x': '394.26', 'y': '310.28', 'h': '83.22', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '328.21', 'h': '230.93', 'w': '8.74'}], [{'page': '17', 'x': '369.70', 'y': '328.21', 'h': '107.78', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '346.14', 'h': '257.76', 'w': '8.74'}]]\", \"text\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2210.02945.pdf\", \"section_number\": \"4.3.\", \"para\": \"8\", \"_id\": \"e6cd2bf4-d04c-40b3-a4f6-7a2b40dddc73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"metadata\": {\"text\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"paper_title\": \"PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS\", \"section_title\": \"BENCHMARKS\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '123.83', 'h': '396.22', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '134.75', 'h': '396.06', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '145.79', 'h': '67.17', 'w': '9.03'}], [{'page': '7', 'x': '178.32', 'y': '145.79', 'h': '325.69', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '156.71', 'h': '396.30', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '167.63', 'h': '30.69', 'w': '9.03'}], [{'page': '7', 'x': '141.48', 'y': '167.63', 'h': '362.67', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '178.05', 'h': '396.06', 'w': '9.96'}, {'page': '7', 'x': '108.00', 'y': '189.59', 'h': '341.25', 'w': '9.03'}], [{'page': '7', 'x': '455.16', 'y': '189.59', 'h': '48.92', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '200.51', 'h': '396.08', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '210.93', 'h': '178.41', 'w': '9.96'}], [{'page': '7', 'x': '292.44', 'y': '211.55', 'h': '211.50', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '221.85', 'h': '329.49', 'w': '9.96'}], [{'page': '7', 'x': '442.44', 'y': '222.47', 'h': '61.43', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '233.39', 'h': '395.86', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '243.81', 'h': '368.49', 'w': '9.96'}], [{'page': '7', 'x': '482.40', 'y': '244.43', 'h': '21.70', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '255.35', 'h': '144.21', 'w': '9.03'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.14659.pdf\", \"section_number\": \"4.1\", \"para\": \"6\", \"_id\": \"6e059989-cd59-448a-a4fe-db4f1e1d5593\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"metadata\": {\"pages\": \"('15', '16')\", \"paper_title\": \"Reset-free Trial-and-Error Learning for Robot Damage Recovery\", \"section_title\": \"Discussion and Conclusion\", \"bboxes\": \"[[{'page': '15', 'x': '316.57', 'y': '694.09', 'h': '241.10', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '706.05', 'h': '251.05', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '718.00', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '729.96', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '741.91', 'h': '32.49', 'w': '8.74'}], [{'page': '15', 'x': '348.39', 'y': '741.91', 'h': '209.27', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '753.87', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '654.67', 'h': '100.48', 'w': '8.74'}], [{'page': '16', 'x': '144.47', 'y': '654.67', 'h': '144.20', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '666.63', 'h': '251.05', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '678.58', 'h': '166.72', 'w': '8.74'}], [{'page': '16', 'x': '208.70', 'y': '678.58', 'h': '79.98', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '690.54', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '702.49', 'h': '39.38', 'w': '8.74'}]]\", \"text\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1610.04213.pdf\", \"section_number\": \"8.\", \"para\": \"3\", \"_id\": \"701347e2-703c-49b3-8c29-c483d9f99719\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"metadata\": {\"text\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Experiments and Results\", \"bboxes\": \"[[{'page': '5', 'x': '107.69', 'y': '137.78', 'h': '396.56', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '149.01', 'h': '24.56', 'w': '8.64'}], [{'page': '5', 'x': '135.96', 'y': '148.69', 'h': '301.51', 'w': '8.96'}], [{'page': '5', 'x': '440.86', 'y': '149.01', 'h': '63.31', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '159.91', 'h': '396.00', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '170.50', 'h': '396.00', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '181.41', 'h': '106.12', 'w': '9.65'}], [{'page': '5', 'x': '217.55', 'y': '181.41', 'h': '228.49', 'w': '8.96'}], [{'page': '5', 'x': '449.47', 'y': '181.73', 'h': '54.53', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '192.64', 'h': '397.75', 'w': '8.64'}], [{'page': '5', 'x': '107.69', 'y': '203.23', 'h': '396.65', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '214.14', 'h': '132.59', 'w': '8.96'}], [{'page': '5', 'x': '243.68', 'y': '214.14', 'h': '260.31', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '225.37', 'h': '284.58', 'w': '8.64'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"5\", \"para\": \"6\", \"_id\": \"21ca484e-c984-4a38-b134-67906841bcc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"metadata\": {\"text\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '352.78', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '364.73', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '376.69', 'h': '117.21', 'w': '8.64'}], [{'page': '9', 'x': '169.88', 'y': '376.69', 'h': '130.14', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '388.64', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '400.60', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '412.55', 'h': '21.46', 'w': '8.64'}], [{'page': '9', 'x': '74.98', 'y': '412.55', 'h': '225.04', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '424.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '436.46', 'h': '122.07', 'w': '8.64'}], [{'page': '9', 'x': '176.36', 'y': '436.46', 'h': '123.67', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '448.42', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '460.37', 'h': '188.81', 'w': '8.64'}], [{'page': '9', 'x': '241.50', 'y': '460.37', 'h': '58.52', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '472.33', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '484.28', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '496.24', 'h': '80.87', 'w': '8.64'}], [{'page': '9', 'x': '133.60', 'y': '496.24', 'h': '166.43', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '508.19', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '520.15', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '532.10', 'h': '79.44', 'w': '8.64'}], [{'page': '9', 'x': '131.76', 'y': '532.10', 'h': '168.26', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '544.06', 'h': '155.22', 'w': '8.64'}], [{'page': '9', 'x': '207.87', 'y': '544.06', 'h': '92.15', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '556.01', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '567.97', 'h': '51.38', 'w': '8.64'}], [{'page': '9', 'x': '104.40', 'y': '567.97', 'h': '195.63', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '579.93', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '591.88', 'h': '183.23', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ece653ae-4f5a-4359-95df-81ca272e7706\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"metadata\": {\"text\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Algorithm Overview\", \"bboxes\": \"[[{'page': '2', 'x': '58.93', 'y': '300.75', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '312.71', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '324.66', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '336.62', 'h': '226.56', 'w': '8.64'}], [{'page': '2', 'x': '279.70', 'y': '336.62', 'h': '20.32', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '348.57', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '360.53', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '372.48', 'h': '206.38', 'w': '8.64'}], [{'page': '2', 'x': '58.93', 'y': '538.50', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '550.46', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '562.41', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '574.37', 'h': '29.61', 'w': '8.64'}], [{'page': '2', 'x': '81.47', 'y': '574.37', 'h': '218.55', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '586.32', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '598.28', 'h': '115.47', 'w': '8.64'}], [{'page': '2', 'x': '168.38', 'y': '598.28', 'h': '131.65', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '610.23', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '622.19', 'h': '212.91', 'w': '8.64'}], [{'page': '2', 'x': '264.48', 'y': '622.19', 'h': '35.54', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '634.14', 'h': '47.98', 'w': '8.64'}], [{'page': '2', 'x': '100.42', 'y': '634.14', 'h': '199.60', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '646.10', 'h': '37.53', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"ad14b04c-df4f-4056-a6ac-95a2e08aabc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0x6b80b476fa078dfb", "trace_id": "0xc8c8f94a22df7ed1e768f3811a3d1b27"}, "parent_id": "0xaa2d48bb15fe8aa2", "start_time": 1724841658401552827, "end_time": 1724841658429486955, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c52ba030bbe549c9aa09678267605239\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\"", "mlflow.spanOutputs": "[{\"page_content\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '256.28', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '268.23', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '280.19', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '292.14', 'h': '38.91', 'w': '8.64'}], [{'page': '10', 'x': '353.40', 'y': '292.14', 'h': '209.63', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '303.92', 'h': '251.06', 'w': '8.82'}, {'page': '10', 'x': '311.98', 'y': '316.05', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '328.01', 'h': '40.11', 'w': '8.64'}]]\", \"text\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"69bf60a6-d4ec-4964-8f63-b3ad6b7b089b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"metadata\": {\"text\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '339.55', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '351.51', 'h': '141.76', 'w': '8.64'}], [{'page': '10', 'x': '456.15', 'y': '351.51', 'h': '106.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '363.46', 'h': '183.86', 'w': '8.64'}], [{'page': '10', 'x': '501.15', 'y': '363.46', 'h': '61.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '375.42', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '387.37', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '399.33', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '411.28', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '423.24', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '435.20', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '447.15', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '459.11', 'h': '29.06', 'w': '8.64'}], [{'page': '10', 'x': '343.61', 'y': '459.11', 'h': '219.43', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '471.06', 'h': '124.56', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"414b691d-581f-4c25-beeb-d65b9c0035cb\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"metadata\": {\"pages\": \"('17', '17')\", \"paper_title\": \"On the optimal pivot path of simplex method for linear programming based on reinforcement learning\", \"section_title\": \"Extracting Multiple Shortest Pivot Paths\", \"bboxes\": \"[[{'page': '17', 'x': '148.71', 'y': '148.88', 'h': '328.77', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '166.81', 'h': '94.83', 'w': '8.74'}], [{'page': '17', 'x': '233.59', 'y': '166.81', 'h': '243.89', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '184.75', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '202.68', 'h': '81.04', 'w': '8.74'}], [{'page': '17', 'x': '222.69', 'y': '202.68', 'h': '254.79', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '220.61', 'h': '189.98', 'w': '8.74'}], [{'page': '17', 'x': '328.71', 'y': '220.61', 'h': '148.76', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '238.55', 'h': '77.42', 'w': '8.74'}], [{'page': '17', 'x': '216.01', 'y': '238.55', 'h': '261.47', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '256.48', 'h': '120.24', 'w': '8.74'}], [{'page': '17', 'x': '258.08', 'y': '256.48', 'h': '219.40', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '274.41', 'h': '279.13', 'w': '8.74'}], [{'page': '17', 'x': '419.27', 'y': '274.41', 'h': '58.21', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '292.34', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '310.28', 'h': '256.24', 'w': '8.74'}], [{'page': '17', 'x': '394.26', 'y': '310.28', 'h': '83.22', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '328.21', 'h': '230.93', 'w': '8.74'}], [{'page': '17', 'x': '369.70', 'y': '328.21', 'h': '107.78', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '346.14', 'h': '257.76', 'w': '8.74'}]]\", \"text\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2210.02945.pdf\", \"section_number\": \"4.3.\", \"para\": \"8\", \"_id\": \"e6cd2bf4-d04c-40b3-a4f6-7a2b40dddc73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"metadata\": {\"text\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"paper_title\": \"PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS\", \"section_title\": \"BENCHMARKS\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '123.83', 'h': '396.22', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '134.75', 'h': '396.06', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '145.79', 'h': '67.17', 'w': '9.03'}], [{'page': '7', 'x': '178.32', 'y': '145.79', 'h': '325.69', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '156.71', 'h': '396.30', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '167.63', 'h': '30.69', 'w': '9.03'}], [{'page': '7', 'x': '141.48', 'y': '167.63', 'h': '362.67', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '178.05', 'h': '396.06', 'w': '9.96'}, {'page': '7', 'x': '108.00', 'y': '189.59', 'h': '341.25', 'w': '9.03'}], [{'page': '7', 'x': '455.16', 'y': '189.59', 'h': '48.92', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '200.51', 'h': '396.08', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '210.93', 'h': '178.41', 'w': '9.96'}], [{'page': '7', 'x': '292.44', 'y': '211.55', 'h': '211.50', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '221.85', 'h': '329.49', 'w': '9.96'}], [{'page': '7', 'x': '442.44', 'y': '222.47', 'h': '61.43', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '233.39', 'h': '395.86', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '243.81', 'h': '368.49', 'w': '9.96'}], [{'page': '7', 'x': '482.40', 'y': '244.43', 'h': '21.70', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '255.35', 'h': '144.21', 'w': '9.03'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.14659.pdf\", \"section_number\": \"4.1\", \"para\": \"6\", \"_id\": \"6e059989-cd59-448a-a4fe-db4f1e1d5593\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"metadata\": {\"pages\": \"('15', '16')\", \"paper_title\": \"Reset-free Trial-and-Error Learning for Robot Damage Recovery\", \"section_title\": \"Discussion and Conclusion\", \"bboxes\": \"[[{'page': '15', 'x': '316.57', 'y': '694.09', 'h': '241.10', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '706.05', 'h': '251.05', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '718.00', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '729.96', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '741.91', 'h': '32.49', 'w': '8.74'}], [{'page': '15', 'x': '348.39', 'y': '741.91', 'h': '209.27', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '753.87', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '654.67', 'h': '100.48', 'w': '8.74'}], [{'page': '16', 'x': '144.47', 'y': '654.67', 'h': '144.20', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '666.63', 'h': '251.05', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '678.58', 'h': '166.72', 'w': '8.74'}], [{'page': '16', 'x': '208.70', 'y': '678.58', 'h': '79.98', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '690.54', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '702.49', 'h': '39.38', 'w': '8.74'}]]\", \"text\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1610.04213.pdf\", \"section_number\": \"8.\", \"para\": \"3\", \"_id\": \"701347e2-703c-49b3-8c29-c483d9f99719\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"metadata\": {\"text\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Experiments and Results\", \"bboxes\": \"[[{'page': '5', 'x': '107.69', 'y': '137.78', 'h': '396.56', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '149.01', 'h': '24.56', 'w': '8.64'}], [{'page': '5', 'x': '135.96', 'y': '148.69', 'h': '301.51', 'w': '8.96'}], [{'page': '5', 'x': '440.86', 'y': '149.01', 'h': '63.31', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '159.91', 'h': '396.00', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '170.50', 'h': '396.00', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '181.41', 'h': '106.12', 'w': '9.65'}], [{'page': '5', 'x': '217.55', 'y': '181.41', 'h': '228.49', 'w': '8.96'}], [{'page': '5', 'x': '449.47', 'y': '181.73', 'h': '54.53', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '192.64', 'h': '397.75', 'w': '8.64'}], [{'page': '5', 'x': '107.69', 'y': '203.23', 'h': '396.65', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '214.14', 'h': '132.59', 'w': '8.96'}], [{'page': '5', 'x': '243.68', 'y': '214.14', 'h': '260.31', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '225.37', 'h': '284.58', 'w': '8.64'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"5\", \"para\": \"6\", \"_id\": \"21ca484e-c984-4a38-b134-67906841bcc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"metadata\": {\"text\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '352.78', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '364.73', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '376.69', 'h': '117.21', 'w': '8.64'}], [{'page': '9', 'x': '169.88', 'y': '376.69', 'h': '130.14', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '388.64', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '400.60', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '412.55', 'h': '21.46', 'w': '8.64'}], [{'page': '9', 'x': '74.98', 'y': '412.55', 'h': '225.04', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '424.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '436.46', 'h': '122.07', 'w': '8.64'}], [{'page': '9', 'x': '176.36', 'y': '436.46', 'h': '123.67', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '448.42', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '460.37', 'h': '188.81', 'w': '8.64'}], [{'page': '9', 'x': '241.50', 'y': '460.37', 'h': '58.52', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '472.33', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '484.28', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '496.24', 'h': '80.87', 'w': '8.64'}], [{'page': '9', 'x': '133.60', 'y': '496.24', 'h': '166.43', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '508.19', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '520.15', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '532.10', 'h': '79.44', 'w': '8.64'}], [{'page': '9', 'x': '131.76', 'y': '532.10', 'h': '168.26', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '544.06', 'h': '155.22', 'w': '8.64'}], [{'page': '9', 'x': '207.87', 'y': '544.06', 'h': '92.15', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '556.01', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '567.97', 'h': '51.38', 'w': '8.64'}], [{'page': '9', 'x': '104.40', 'y': '567.97', 'h': '195.63', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '579.93', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '591.88', 'h': '183.23', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ece653ae-4f5a-4359-95df-81ca272e7706\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"metadata\": {\"text\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Algorithm Overview\", \"bboxes\": \"[[{'page': '2', 'x': '58.93', 'y': '300.75', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '312.71', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '324.66', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '336.62', 'h': '226.56', 'w': '8.64'}], [{'page': '2', 'x': '279.70', 'y': '336.62', 'h': '20.32', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '348.57', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '360.53', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '372.48', 'h': '206.38', 'w': '8.64'}], [{'page': '2', 'x': '58.93', 'y': '538.50', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '550.46', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '562.41', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '574.37', 'h': '29.61', 'w': '8.64'}], [{'page': '2', 'x': '81.47', 'y': '574.37', 'h': '218.55', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '586.32', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '598.28', 'h': '115.47', 'w': '8.64'}], [{'page': '2', 'x': '168.38', 'y': '598.28', 'h': '131.65', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '610.23', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '622.19', 'h': '212.91', 'w': '8.64'}], [{'page': '2', 'x': '264.48', 'y': '622.19', 'h': '35.54', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '634.14', 'h': '47.98', 'w': '8.64'}], [{'page': '2', 'x': '100.42', 'y': '634.14', 'h': '199.60', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '646.10', 'h': '37.53', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"ad14b04c-df4f-4056-a6ac-95a2e08aabc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0x4c4c80c751f6c570", "trace_id": "0xc8c8f94a22df7ed1e768f3811a3d1b27"}, "parent_id": "0xaa2d48bb15fe8aa2", "start_time": 1724841658430390468, "end_time": 1724841660224822953, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c52ba030bbe549c9aa09678267605239\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '256.28', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '268.23', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '280.19', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '292.14', 'h': '38.91', 'w': '8.64'}], [{'page': '10', 'x': '353.40', 'y': '292.14', 'h': '209.63', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '303.92', 'h': '251.06', 'w': '8.82'}, {'page': '10', 'x': '311.98', 'y': '316.05', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '328.01', 'h': '40.11', 'w': '8.64'}]]\", \"text\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"69bf60a6-d4ec-4964-8f63-b3ad6b7b089b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"metadata\": {\"text\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '339.55', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '351.51', 'h': '141.76', 'w': '8.64'}], [{'page': '10', 'x': '456.15', 'y': '351.51', 'h': '106.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '363.46', 'h': '183.86', 'w': '8.64'}], [{'page': '10', 'x': '501.15', 'y': '363.46', 'h': '61.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '375.42', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '387.37', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '399.33', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '411.28', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '423.24', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '435.20', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '447.15', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '459.11', 'h': '29.06', 'w': '8.64'}], [{'page': '10', 'x': '343.61', 'y': '459.11', 'h': '219.43', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '471.06', 'h': '124.56', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"414b691d-581f-4c25-beeb-d65b9c0035cb\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"metadata\": {\"pages\": \"('17', '17')\", \"paper_title\": \"On the optimal pivot path of simplex method for linear programming based on reinforcement learning\", \"section_title\": \"Extracting Multiple Shortest Pivot Paths\", \"bboxes\": \"[[{'page': '17', 'x': '148.71', 'y': '148.88', 'h': '328.77', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '166.81', 'h': '94.83', 'w': '8.74'}], [{'page': '17', 'x': '233.59', 'y': '166.81', 'h': '243.89', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '184.75', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '202.68', 'h': '81.04', 'w': '8.74'}], [{'page': '17', 'x': '222.69', 'y': '202.68', 'h': '254.79', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '220.61', 'h': '189.98', 'w': '8.74'}], [{'page': '17', 'x': '328.71', 'y': '220.61', 'h': '148.76', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '238.55', 'h': '77.42', 'w': '8.74'}], [{'page': '17', 'x': '216.01', 'y': '238.55', 'h': '261.47', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '256.48', 'h': '120.24', 'w': '8.74'}], [{'page': '17', 'x': '258.08', 'y': '256.48', 'h': '219.40', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '274.41', 'h': '279.13', 'w': '8.74'}], [{'page': '17', 'x': '419.27', 'y': '274.41', 'h': '58.21', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '292.34', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '310.28', 'h': '256.24', 'w': '8.74'}], [{'page': '17', 'x': '394.26', 'y': '310.28', 'h': '83.22', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '328.21', 'h': '230.93', 'w': '8.74'}], [{'page': '17', 'x': '369.70', 'y': '328.21', 'h': '107.78', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '346.14', 'h': '257.76', 'w': '8.74'}]]\", \"text\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2210.02945.pdf\", \"section_number\": \"4.3.\", \"para\": \"8\", \"_id\": \"e6cd2bf4-d04c-40b3-a4f6-7a2b40dddc73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"metadata\": {\"text\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"paper_title\": \"PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS\", \"section_title\": \"BENCHMARKS\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '123.83', 'h': '396.22', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '134.75', 'h': '396.06', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '145.79', 'h': '67.17', 'w': '9.03'}], [{'page': '7', 'x': '178.32', 'y': '145.79', 'h': '325.69', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '156.71', 'h': '396.30', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '167.63', 'h': '30.69', 'w': '9.03'}], [{'page': '7', 'x': '141.48', 'y': '167.63', 'h': '362.67', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '178.05', 'h': '396.06', 'w': '9.96'}, {'page': '7', 'x': '108.00', 'y': '189.59', 'h': '341.25', 'w': '9.03'}], [{'page': '7', 'x': '455.16', 'y': '189.59', 'h': '48.92', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '200.51', 'h': '396.08', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '210.93', 'h': '178.41', 'w': '9.96'}], [{'page': '7', 'x': '292.44', 'y': '211.55', 'h': '211.50', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '221.85', 'h': '329.49', 'w': '9.96'}], [{'page': '7', 'x': '442.44', 'y': '222.47', 'h': '61.43', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '233.39', 'h': '395.86', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '243.81', 'h': '368.49', 'w': '9.96'}], [{'page': '7', 'x': '482.40', 'y': '244.43', 'h': '21.70', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '255.35', 'h': '144.21', 'w': '9.03'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.14659.pdf\", \"section_number\": \"4.1\", \"para\": \"6\", \"_id\": \"6e059989-cd59-448a-a4fe-db4f1e1d5593\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"metadata\": {\"pages\": \"('15', '16')\", \"paper_title\": \"Reset-free Trial-and-Error Learning for Robot Damage Recovery\", \"section_title\": \"Discussion and Conclusion\", \"bboxes\": \"[[{'page': '15', 'x': '316.57', 'y': '694.09', 'h': '241.10', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '706.05', 'h': '251.05', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '718.00', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '729.96', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '741.91', 'h': '32.49', 'w': '8.74'}], [{'page': '15', 'x': '348.39', 'y': '741.91', 'h': '209.27', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '753.87', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '654.67', 'h': '100.48', 'w': '8.74'}], [{'page': '16', 'x': '144.47', 'y': '654.67', 'h': '144.20', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '666.63', 'h': '251.05', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '678.58', 'h': '166.72', 'w': '8.74'}], [{'page': '16', 'x': '208.70', 'y': '678.58', 'h': '79.98', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '690.54', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '702.49', 'h': '39.38', 'w': '8.74'}]]\", \"text\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1610.04213.pdf\", \"section_number\": \"8.\", \"para\": \"3\", \"_id\": \"701347e2-703c-49b3-8c29-c483d9f99719\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"metadata\": {\"text\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Experiments and Results\", \"bboxes\": \"[[{'page': '5', 'x': '107.69', 'y': '137.78', 'h': '396.56', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '149.01', 'h': '24.56', 'w': '8.64'}], [{'page': '5', 'x': '135.96', 'y': '148.69', 'h': '301.51', 'w': '8.96'}], [{'page': '5', 'x': '440.86', 'y': '149.01', 'h': '63.31', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '159.91', 'h': '396.00', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '170.50', 'h': '396.00', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '181.41', 'h': '106.12', 'w': '9.65'}], [{'page': '5', 'x': '217.55', 'y': '181.41', 'h': '228.49', 'w': '8.96'}], [{'page': '5', 'x': '449.47', 'y': '181.73', 'h': '54.53', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '192.64', 'h': '397.75', 'w': '8.64'}], [{'page': '5', 'x': '107.69', 'y': '203.23', 'h': '396.65', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '214.14', 'h': '132.59', 'w': '8.96'}], [{'page': '5', 'x': '243.68', 'y': '214.14', 'h': '260.31', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '225.37', 'h': '284.58', 'w': '8.64'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"5\", \"para\": \"6\", \"_id\": \"21ca484e-c984-4a38-b134-67906841bcc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"metadata\": {\"text\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '352.78', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '364.73', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '376.69', 'h': '117.21', 'w': '8.64'}], [{'page': '9', 'x': '169.88', 'y': '376.69', 'h': '130.14', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '388.64', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '400.60', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '412.55', 'h': '21.46', 'w': '8.64'}], [{'page': '9', 'x': '74.98', 'y': '412.55', 'h': '225.04', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '424.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '436.46', 'h': '122.07', 'w': '8.64'}], [{'page': '9', 'x': '176.36', 'y': '436.46', 'h': '123.67', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '448.42', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '460.37', 'h': '188.81', 'w': '8.64'}], [{'page': '9', 'x': '241.50', 'y': '460.37', 'h': '58.52', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '472.33', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '484.28', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '496.24', 'h': '80.87', 'w': '8.64'}], [{'page': '9', 'x': '133.60', 'y': '496.24', 'h': '166.43', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '508.19', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '520.15', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '532.10', 'h': '79.44', 'w': '8.64'}], [{'page': '9', 'x': '131.76', 'y': '532.10', 'h': '168.26', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '544.06', 'h': '155.22', 'w': '8.64'}], [{'page': '9', 'x': '207.87', 'y': '544.06', 'h': '92.15', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '556.01', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '567.97', 'h': '51.38', 'w': '8.64'}], [{'page': '9', 'x': '104.40', 'y': '567.97', 'h': '195.63', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '579.93', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '591.88', 'h': '183.23', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ece653ae-4f5a-4359-95df-81ca272e7706\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"metadata\": {\"text\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Algorithm Overview\", \"bboxes\": \"[[{'page': '2', 'x': '58.93', 'y': '300.75', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '312.71', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '324.66', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '336.62', 'h': '226.56', 'w': '8.64'}], [{'page': '2', 'x': '279.70', 'y': '336.62', 'h': '20.32', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '348.57', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '360.53', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '372.48', 'h': '206.38', 'w': '8.64'}], [{'page': '2', 'x': '58.93', 'y': '538.50', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '550.46', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '562.41', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '574.37', 'h': '29.61', 'w': '8.64'}], [{'page': '2', 'x': '81.47', 'y': '574.37', 'h': '218.55', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '586.32', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '598.28', 'h': '115.47', 'w': '8.64'}], [{'page': '2', 'x': '168.38', 'y': '598.28', 'h': '131.65', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '610.23', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '622.19', 'h': '212.91', 'w': '8.64'}], [{'page': '2', 'x': '264.48', 'y': '622.19', 'h': '35.54', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '634.14', 'h': '47.98', 'w': '8.64'}], [{'page': '2', 'x': '100.42', 'y': '634.14', 'h': '199.60', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '646.10', 'h': '37.53', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"ad14b04c-df4f-4056-a6ac-95a2e08aabc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\"}", "mlflow.spanOutputs": "{\"output_text\": \" The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x88699775ebda2e3e", "trace_id": "0xc8c8f94a22df7ed1e768f3811a3d1b27"}, "parent_id": "0x4c4c80c751f6c570", "start_time": 1724841658431548944, "end_time": 1724841660224587829, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c52ba030bbe549c9aa09678267605239\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\", \"context\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\\n\\nThe new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\\n\\nThe optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\\n\\nSince there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\\n\\nOur implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\\n\\nIn this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\\n\\nTo validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\\n\\nSelf-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\\n\\nDespite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\\n\\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\"}", "mlflow.spanOutputs": "{\"text\": \" The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.\"}"}, "events": [{"name": "text", "timestamp": 1724841658431645, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nWe have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\n\nThe new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\n\nThe optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\n\nSince there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\n\nOur implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.\n\nIn this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\n\nTo validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\n\nSelf-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\n\nDespite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\n\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\n\nQuestion: How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x57db07ba4812a137", "trace_id": "0xc8c8f94a22df7ed1e768f3811a3d1b27"}, "parent_id": "0x88699775ebda2e3e", "start_time": 1724841658431968657, "end_time": 1724841660224283640, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c52ba030bbe549c9aa09678267605239\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nWe have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\\n\\nThe new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\\n\\nThe optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\\n\\nSince there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\\n\\nOur implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\\n\\nIn this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\\n\\nTo validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\\n\\nSelf-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\\n\\nDespite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\\n\\nN-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\\n\\nQuestion: How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 1708, \"completion_tokens\": 86, \"total_tokens\": 1794}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?\"}", "response": "{\"result\": \" The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.\", \"source_documents\": [{\"page_content\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '256.28', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '268.23', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '280.19', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '292.14', 'h': '38.91', 'w': '8.64'}], [{'page': '10', 'x': '353.40', 'y': '292.14', 'h': '209.63', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '303.92', 'h': '251.06', 'w': '8.82'}, {'page': '10', 'x': '311.98', 'y': '316.05', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '328.01', 'h': '40.11', 'w': '8.64'}]]\", \"text\": \"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"69bf60a6-d4ec-4964-8f63-b3ad6b7b089b\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"metadata\": {\"text\": \"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '339.55', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '351.51', 'h': '141.76', 'w': '8.64'}], [{'page': '10', 'x': '456.15', 'y': '351.51', 'h': '106.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '363.46', 'h': '183.86', 'w': '8.64'}], [{'page': '10', 'x': '501.15', 'y': '363.46', 'h': '61.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '375.42', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '387.37', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '399.33', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '411.28', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '423.24', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '435.20', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '447.15', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '459.11', 'h': '29.06', 'w': '8.64'}], [{'page': '10', 'x': '343.61', 'y': '459.11', 'h': '219.43', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '471.06', 'h': '124.56', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"414b691d-581f-4c25-beeb-d65b9c0035cb\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"metadata\": {\"pages\": \"('17', '17')\", \"paper_title\": \"On the optimal pivot path of simplex method for linear programming based on reinforcement learning\", \"section_title\": \"Extracting Multiple Shortest Pivot Paths\", \"bboxes\": \"[[{'page': '17', 'x': '148.71', 'y': '148.88', 'h': '328.77', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '166.81', 'h': '94.83', 'w': '8.74'}], [{'page': '17', 'x': '233.59', 'y': '166.81', 'h': '243.89', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '184.75', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '202.68', 'h': '81.04', 'w': '8.74'}], [{'page': '17', 'x': '222.69', 'y': '202.68', 'h': '254.79', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '220.61', 'h': '189.98', 'w': '8.74'}], [{'page': '17', 'x': '328.71', 'y': '220.61', 'h': '148.76', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '238.55', 'h': '77.42', 'w': '8.74'}], [{'page': '17', 'x': '216.01', 'y': '238.55', 'h': '261.47', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '256.48', 'h': '120.24', 'w': '8.74'}], [{'page': '17', 'x': '258.08', 'y': '256.48', 'h': '219.40', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '274.41', 'h': '279.13', 'w': '8.74'}], [{'page': '17', 'x': '419.27', 'y': '274.41', 'h': '58.21', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '292.34', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '310.28', 'h': '256.24', 'w': '8.74'}], [{'page': '17', 'x': '394.26', 'y': '310.28', 'h': '83.22', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '328.21', 'h': '230.93', 'w': '8.74'}], [{'page': '17', 'x': '369.70', 'y': '328.21', 'h': '107.78', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '346.14', 'h': '257.76', 'w': '8.74'}]]\", \"text\": \"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2210.02945.pdf\", \"section_number\": \"4.3.\", \"para\": \"8\", \"_id\": \"e6cd2bf4-d04c-40b3-a4f6-7a2b40dddc73\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"metadata\": {\"text\": \"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.\", \"paper_title\": \"PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS\", \"section_title\": \"BENCHMARKS\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '123.83', 'h': '396.22', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '134.75', 'h': '396.06', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '145.79', 'h': '67.17', 'w': '9.03'}], [{'page': '7', 'x': '178.32', 'y': '145.79', 'h': '325.69', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '156.71', 'h': '396.30', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '167.63', 'h': '30.69', 'w': '9.03'}], [{'page': '7', 'x': '141.48', 'y': '167.63', 'h': '362.67', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '178.05', 'h': '396.06', 'w': '9.96'}, {'page': '7', 'x': '108.00', 'y': '189.59', 'h': '341.25', 'w': '9.03'}], [{'page': '7', 'x': '455.16', 'y': '189.59', 'h': '48.92', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '200.51', 'h': '396.08', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '210.93', 'h': '178.41', 'w': '9.96'}], [{'page': '7', 'x': '292.44', 'y': '211.55', 'h': '211.50', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '221.85', 'h': '329.49', 'w': '9.96'}], [{'page': '7', 'x': '442.44', 'y': '222.47', 'h': '61.43', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '233.39', 'h': '395.86', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '243.81', 'h': '368.49', 'w': '9.96'}], [{'page': '7', 'x': '482.40', 'y': '244.43', 'h': '21.70', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '255.35', 'h': '144.21', 'w': '9.03'}]]\", \"pages\": \"('7', '7')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.14659.pdf\", \"section_number\": \"4.1\", \"para\": \"6\", \"_id\": \"6e059989-cd59-448a-a4fe-db4f1e1d5593\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"metadata\": {\"pages\": \"('15', '16')\", \"paper_title\": \"Reset-free Trial-and-Error Learning for Robot Damage Recovery\", \"section_title\": \"Discussion and Conclusion\", \"bboxes\": \"[[{'page': '15', 'x': '316.57', 'y': '694.09', 'h': '241.10', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '706.05', 'h': '251.05', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '718.00', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '729.96', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '741.91', 'h': '32.49', 'w': '8.74'}], [{'page': '15', 'x': '348.39', 'y': '741.91', 'h': '209.27', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '753.87', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '654.67', 'h': '100.48', 'w': '8.74'}], [{'page': '16', 'x': '144.47', 'y': '654.67', 'h': '144.20', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '666.63', 'h': '251.05', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '678.58', 'h': '166.72', 'w': '8.74'}], [{'page': '16', 'x': '208.70', 'y': '678.58', 'h': '79.98', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '690.54', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '702.49', 'h': '39.38', 'w': '8.74'}]]\", \"text\": \"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1610.04213.pdf\", \"section_number\": \"8.\", \"para\": \"3\", \"_id\": \"701347e2-703c-49b3-8c29-c483d9f99719\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"metadata\": {\"text\": \"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \\u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.\", \"paper_title\": \"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\", \"section_title\": \"Experiments and Results\", \"bboxes\": \"[[{'page': '5', 'x': '107.69', 'y': '137.78', 'h': '396.56', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '149.01', 'h': '24.56', 'w': '8.64'}], [{'page': '5', 'x': '135.96', 'y': '148.69', 'h': '301.51', 'w': '8.96'}], [{'page': '5', 'x': '440.86', 'y': '149.01', 'h': '63.31', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '159.91', 'h': '396.00', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '170.50', 'h': '396.00', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '181.41', 'h': '106.12', 'w': '9.65'}], [{'page': '5', 'x': '217.55', 'y': '181.41', 'h': '228.49', 'w': '8.96'}], [{'page': '5', 'x': '449.47', 'y': '181.73', 'h': '54.53', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '192.64', 'h': '397.75', 'w': '8.64'}], [{'page': '5', 'x': '107.69', 'y': '203.23', 'h': '396.65', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '214.14', 'h': '132.59', 'w': '8.96'}], [{'page': '5', 'x': '243.68', 'y': '214.14', 'h': '260.31', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '225.37', 'h': '284.58', 'w': '8.64'}]]\", \"pages\": \"('5', '5')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1807.01672.pdf\", \"section_number\": \"5\", \"para\": \"6\", \"_id\": \"21ca484e-c984-4a38-b134-67906841bcc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"metadata\": {\"text\": \"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '58.93', 'y': '352.78', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '364.73', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '376.69', 'h': '117.21', 'w': '8.64'}], [{'page': '9', 'x': '169.88', 'y': '376.69', 'h': '130.14', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '388.64', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '400.60', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '412.55', 'h': '21.46', 'w': '8.64'}], [{'page': '9', 'x': '74.98', 'y': '412.55', 'h': '225.04', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '424.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '436.46', 'h': '122.07', 'w': '8.64'}], [{'page': '9', 'x': '176.36', 'y': '436.46', 'h': '123.67', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '448.42', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '460.37', 'h': '188.81', 'w': '8.64'}], [{'page': '9', 'x': '241.50', 'y': '460.37', 'h': '58.52', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '472.33', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '484.28', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '496.24', 'h': '80.87', 'w': '8.64'}], [{'page': '9', 'x': '133.60', 'y': '496.24', 'h': '166.43', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '508.19', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '520.15', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '532.10', 'h': '79.44', 'w': '8.64'}], [{'page': '9', 'x': '131.76', 'y': '532.10', 'h': '168.26', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '544.06', 'h': '155.22', 'w': '8.64'}], [{'page': '9', 'x': '207.87', 'y': '544.06', 'h': '92.15', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '556.01', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '567.97', 'h': '51.38', 'w': '8.64'}], [{'page': '9', 'x': '104.40', 'y': '567.97', 'h': '195.63', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '579.93', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '591.88', 'h': '183.23', 'w': '8.64'}]]\", \"pages\": \"('9', '9')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"8\", \"_id\": \"ece653ae-4f5a-4359-95df-81ca272e7706\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"metadata\": {\"text\": \"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Algorithm Overview\", \"bboxes\": \"[[{'page': '2', 'x': '58.93', 'y': '300.75', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '312.71', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '324.66', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '336.62', 'h': '226.56', 'w': '8.64'}], [{'page': '2', 'x': '279.70', 'y': '336.62', 'h': '20.32', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '348.57', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '360.53', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '372.48', 'h': '206.38', 'w': '8.64'}], [{'page': '2', 'x': '58.93', 'y': '538.50', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '550.46', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '562.41', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '574.37', 'h': '29.61', 'w': '8.64'}], [{'page': '2', 'x': '81.47', 'y': '574.37', 'h': '218.55', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '586.32', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '598.28', 'h': '115.47', 'w': '8.64'}], [{'page': '2', 'x': '168.38', 'y': '598.28', 'h': '131.65', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '610.23', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '622.19', 'h': '212.91', 'w': '8.64'}], [{'page': '2', 'x': '264.48', 'y': '622.19', 'h': '35.54', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '634.14', 'h': '47.98', 'w': '8.64'}], [{'page': '2', 'x': '100.42', 'y': '634.14', 'h': '199.60', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '646.10', 'h': '37.53', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"ad14b04c-df4f-4056-a6ac-95a2e08aabc7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]\", \"text\": \"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"f44fc6ea-68b1-483c-94e5-9150ebad6361\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}