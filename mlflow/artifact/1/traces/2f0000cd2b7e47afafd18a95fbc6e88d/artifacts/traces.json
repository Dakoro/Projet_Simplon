{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x9a5f1f4117f4e7a1", "trace_id": "0x5a36d5209b8e2fc5873ca84ecfd6dcb0"}, "parent_id": null, "start_time": 1724841654197283147, "end_time": 1724841656027808207, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"2f0000cd2b7e47afafd18a95fbc6e88d\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\"}", "mlflow.spanOutputs": "{\"result\": \" The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.\", \"source_documents\": [{\"page_content\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"II. ALGORITHMS AND METHODS\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '641.49', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '653.44', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '665.40', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '677.35', 'h': '13.00', 'w': '8.64'}], [{'page': '1', 'x': '327.61', 'y': '677.35', 'h': '235.43', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '689.31', 'h': '39.68', 'w': '8.64'}], [{'page': '1', 'x': '355.15', 'y': '689.31', 'h': '187.21', 'w': '8.64'}, {'page': '1', 'x': '542.35', 'y': '687.64', 'h': '3.49', 'w': '6.05'}, {'page': '1', 'x': '546.34', 'y': '689.31', 'h': '2.49', 'w': '8.64'}]]\", \"text\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"e19a113d-e501-4f37-b4db-dc80fe890142\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Expert-Driven Genetic Algorithms for Simulating Evaluation Functions \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '146.02', 'h': '330.67', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '157.42', 'h': '94.11', 'w': '8.97'}], [{'page': '2', 'x': '169.44', 'y': '157.42', 'h': '248.26', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '168.94', 'h': '345.51', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '180.34', 'h': '277.72', 'w': '8.97'}], [{'page': '2', 'x': '353.40', 'y': '180.34', 'h': '64.34', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '191.86', 'h': '345.64', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '203.26', 'h': '345.55', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '214.78', 'h': '321.28', 'w': '8.97'}]]\", \"text\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1711.06841.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"c6addab5-dc64-4994-9fd1-00b986893d5d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Integration of cognitive tasks into artificial general intelligence test for large models\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '106.10', 'y': '88.70', 'h': '421.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '105.62', 'h': '185.51', 'w': '9.60'}, {'page': '2', 'x': '270.65', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '279.05', 'y': '105.62', 'h': '55.60', 'w': '9.60'}, {'page': '2', 'x': '334.63', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '348.31', 'y': '105.62', 'h': '34.09', 'w': '9.60'}, {'page': '2', 'x': '382.39', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '390.79', 'y': '105.62', 'h': '34.69', 'w': '9.60'}, {'page': '2', 'x': '425.50', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '439.06', 'y': '105.62', 'h': '47.29', 'w': '9.60'}, {'page': '2', 'x': '486.34', 'y': '104.16', 'h': '10.44', 'w': '6.33'}, {'page': '2', 'x': '501.70', 'y': '105.62', 'h': '25.80', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '121.82', 'h': '276.08', 'w': '9.60'}], [{'page': '2', 'x': '364.17', 'y': '121.82', 'h': '163.01', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '138.02', 'h': '442.30', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '154.82', 'h': '100.58', 'w': '9.60'}, {'page': '2', 'x': '185.69', 'y': '153.36', 'h': '15.72', 'w': '6.33'}], [{'page': '2', 'x': '203.09', 'y': '154.82', 'h': '324.18', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '171.74', 'h': '322.27', 'w': '9.60'}, {'page': '2', 'x': '407.47', 'y': '170.28', 'h': '6.96', 'w': '6.33'}], [{'page': '2', 'x': '418.78', 'y': '171.74', 'h': '108.71', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '187.82', 'h': '414.70', 'w': '9.60'}], [{'page': '2', 'x': '502.65', 'y': '187.82', 'h': '24.63', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '204.02', 'h': '442.26', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '220.94', 'h': '57.59', 'w': '9.60'}, {'page': '2', 'x': '142.70', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '151.58', 'y': '220.94', 'h': '159.32', 'w': '9.60'}, {'page': '2', 'x': '310.99', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '319.75', 'y': '220.94', 'h': '207.62', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '237.02', 'h': '442.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '253.25', 'h': '128.26', 'w': '9.60'}], [{'page': '2', 'x': '217.73', 'y': '253.25', 'h': '309.57', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '269.33', 'h': '240.41', 'w': '9.60'}], [{'page': '2', 'x': '329.07', 'y': '269.33', 'h': '198.16', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '285.53', 'h': '442.25', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '301.73', 'h': '110.33', 'w': '9.60'}]]\", \"text\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2402.02547.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"df806d2c-c726-4362-a3ac-6afa7ec64d08\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '500.05', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '512.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '523.96', 'h': '59.12', 'w': '8.64'}], [{'page': '1', 'x': '113.25', 'y': '523.96', 'h': '186.77', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '535.92', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '547.87', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '559.83', 'h': '67.72', 'w': '8.64'}], [{'page': '1', 'x': '120.49', 'y': '559.83', 'h': '179.54', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '571.78', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '583.74', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '595.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '607.65', 'h': '32.38', 'w': '8.64'}]]\", \"text\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"d2503d91-b1d2-4d61-998d-0a003066fd74\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"metadata\": {\"text\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"paper_title\": \"The Neural MMO Platform for Massively Multiagent Research\", \"section_title\": \"Configuration\", \"bboxes\": \"[[{'page': '2', 'x': '108.00', 'y': '636.76', 'h': '396.00', 'w': '9.03'}, {'page': '2', 'x': '108.00', 'y': '648.06', 'h': '240.75', 'w': '8.64'}], [{'page': '2', 'x': '351.86', 'y': '648.06', 'h': '152.14', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '658.97', 'h': '396.17', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '669.88', 'h': '69.14', 'w': '8.64'}], [{'page': '2', 'x': '180.22', 'y': '669.88', 'h': '323.78', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '680.79', 'h': '234.15', 'w': '8.64'}], [{'page': '2', 'x': '345.26', 'y': '680.79', 'h': '158.73', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '691.70', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '702.61', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '713.51', 'h': '163.51', 'w': '8.64'}], [{'page': '3', 'x': '108.00', 'y': '255.32', 'h': '395.99', 'w': '9.03'}, {'page': '3', 'x': '108.00', 'y': '266.62', 'h': '314.19', 'w': '8.64'}], [{'page': '3', 'x': '425.14', 'y': '266.62', 'h': '78.86', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '277.35', 'h': '396.00', 'w': '8.82'}, {'page': '3', 'x': '108.00', 'y': '288.44', 'h': '267.92', 'w': '8.64'}], [{'page': '3', 'x': '379.03', 'y': '288.44', 'h': '124.97', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '299.35', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '310.26', 'h': '130.97', 'w': '8.64'}], [{'page': '3', 'x': '242.05', 'y': '310.26', 'h': '261.94', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '321.16', 'h': '186.52', 'w': '8.64'}]]\", \"pages\": \"('2', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2110.07594.pdf\", \"section_number\": \"2.1\", \"para\": \"7\", \"_id\": \"91d435c1-be88-4fde-8e9f-263d78c2bb0c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"metadata\": {\"text\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"paper_title\": \"General Board Geometry\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '149.71', 'y': '532.90', 'h': '330.88', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '544.86', 'h': '188.03', 'w': '8.74'}], [{'page': '1', 'x': '326.23', 'y': '544.86', 'h': '154.36', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '556.81', 'h': '282.20', 'w': '8.74'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2111.11329.pdf\", \"section_number\": \"1\", \"para\": \"1\", \"_id\": \"e346bc45-87af-4bb6-a81b-44970631f61c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '408.22', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '420.18', 'h': '43.60', 'w': '8.64'}], [{'page': '9', 'x': '358.49', 'y': '420.18', 'h': '204.54', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '432.13', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '444.09', 'h': '194.64', 'w': '8.64'}], [{'page': '9', 'x': '509.84', 'y': '444.09', 'h': '53.20', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '456.04', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '468.00', 'h': '96.88', 'w': '8.64'}], [{'page': '9', 'x': '413.29', 'y': '468.00', 'h': '149.75', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '479.96', 'h': '238.97', 'w': '8.64'}], [{'page': '9', 'x': '321.94', 'y': '492.69', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '504.65', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '516.60', 'h': '61.95', 'w': '8.64'}], [{'page': '9', 'x': '376.59', 'y': '516.60', 'h': '186.45', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '528.56', 'h': '227.83', 'w': '8.64'}], [{'page': '9', 'x': '543.51', 'y': '528.56', 'h': '19.53', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '540.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '552.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '564.42', 'h': '205.95', 'w': '8.64'}]]\", \"text\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"263f8343-a94d-4f8d-baac-90b85b2b33e7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"metadata\": {\"pages\": \"('8', '8')\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"B. Future Work\", \"bboxes\": \"[[{'page': '8', 'x': '58.93', 'y': '126.29', 'h': '241.09', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '138.24', 'h': '251.06', 'w': '8.64'}], [{'page': '8', 'x': '48.96', 'y': '150.20', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '162.15', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '174.11', 'h': '84.00', 'w': '8.64'}], [{'page': '8', 'x': '136.91', 'y': '174.11', 'h': '163.11', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '186.06', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '198.02', 'h': '201.19', 'w': '8.64'}], [{'page': '8', 'x': '253.30', 'y': '198.02', 'h': '46.73', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '209.97', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '221.93', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '233.88', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '245.84', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '257.79', 'h': '136.47', 'w': '8.64'}], [{'page': '8', 'x': '189.50', 'y': '257.79', 'h': '110.53', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '269.75', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '281.70', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '293.66', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '305.61', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '317.57', 'h': '142.72', 'w': '8.64'}]]\", \"text\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"dea85849-9565-44f1-8f1e-6bc4b7f2b505\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Sokoban\", \"bboxes\": \"[[{'page': '3', 'x': '134.77', 'y': '512.51', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '524.46', 'h': '167.14', 'w': '8.74'}], [{'page': '3', 'x': '305.03', 'y': '524.46', 'h': '175.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '536.42', 'h': '86.55', 'w': '8.74'}], [{'page': '3', 'x': '224.84', 'y': '536.42', 'h': '197.65', 'w': '8.74'}], [{'page': '3', 'x': '426.01', 'y': '536.42', 'h': '54.58', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '548.37', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '560.33', 'h': '70.24', 'w': '8.74'}], [{'page': '3', 'x': '208.21', 'y': '560.33', 'h': '272.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '572.28', 'h': '53.32', 'w': '8.74'}], [{'page': '3', 'x': '192.65', 'y': '572.28', 'h': '287.94', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '584.24', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '596.19', 'h': '139.61', 'w': '8.74'}]]\", \"text\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.2\", \"para\": \"5\", \"_id\": \"bf6d7428-caa3-4aa1-80b5-d330762f3de5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Typed Graph Networks\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '515.50', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '527.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '48.96', 'y': '539.41', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '551.37', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '563.00', 'h': '251.06', 'w': '8.96'}, {'page': '1', 'x': '48.96', 'y': '575.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '587.23', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '599.19', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '611.14', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '623.10', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '635.05', 'h': '30.74', 'w': '8.64'}], [{'page': '1', 'x': '83.17', 'y': '635.05', 'h': '216.85', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '647.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '153.59', 'h': '142.90', 'w': '8.64'}], [{'page': '1', 'x': '458.01', 'y': '153.59', 'h': '105.38', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '165.55', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '177.50', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '189.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '311.51', 'y': '201.41', 'h': '251.53', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '213.37', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.33', 'h': '251.23', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.24', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.19', 'h': '140.93', 'w': '8.64'}], [{'page': '1', 'x': '456.08', 'y': '261.19', 'h': '106.96', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.15', 'h': '251.40', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.10', 'h': '136.56', 'w': '8.64'}]]\", \"text\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1901.07984.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"6aba2813-4b28-48fe-95fb-95e48ddd7549\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0xc3169bd59ab0c47b", "trace_id": "0x5a36d5209b8e2fc5873ca84ecfd6dcb0"}, "parent_id": "0x9a5f1f4117f4e7a1", "start_time": 1724841654202679413, "end_time": 1724841654228686658, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"2f0000cd2b7e47afafd18a95fbc6e88d\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\"", "mlflow.spanOutputs": "[{\"page_content\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"II. ALGORITHMS AND METHODS\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '641.49', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '653.44', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '665.40', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '677.35', 'h': '13.00', 'w': '8.64'}], [{'page': '1', 'x': '327.61', 'y': '677.35', 'h': '235.43', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '689.31', 'h': '39.68', 'w': '8.64'}], [{'page': '1', 'x': '355.15', 'y': '689.31', 'h': '187.21', 'w': '8.64'}, {'page': '1', 'x': '542.35', 'y': '687.64', 'h': '3.49', 'w': '6.05'}, {'page': '1', 'x': '546.34', 'y': '689.31', 'h': '2.49', 'w': '8.64'}]]\", \"text\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"e19a113d-e501-4f37-b4db-dc80fe890142\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Expert-Driven Genetic Algorithms for Simulating Evaluation Functions \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '146.02', 'h': '330.67', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '157.42', 'h': '94.11', 'w': '8.97'}], [{'page': '2', 'x': '169.44', 'y': '157.42', 'h': '248.26', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '168.94', 'h': '345.51', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '180.34', 'h': '277.72', 'w': '8.97'}], [{'page': '2', 'x': '353.40', 'y': '180.34', 'h': '64.34', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '191.86', 'h': '345.64', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '203.26', 'h': '345.55', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '214.78', 'h': '321.28', 'w': '8.97'}]]\", \"text\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1711.06841.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"c6addab5-dc64-4994-9fd1-00b986893d5d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Integration of cognitive tasks into artificial general intelligence test for large models\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '106.10', 'y': '88.70', 'h': '421.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '105.62', 'h': '185.51', 'w': '9.60'}, {'page': '2', 'x': '270.65', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '279.05', 'y': '105.62', 'h': '55.60', 'w': '9.60'}, {'page': '2', 'x': '334.63', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '348.31', 'y': '105.62', 'h': '34.09', 'w': '9.60'}, {'page': '2', 'x': '382.39', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '390.79', 'y': '105.62', 'h': '34.69', 'w': '9.60'}, {'page': '2', 'x': '425.50', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '439.06', 'y': '105.62', 'h': '47.29', 'w': '9.60'}, {'page': '2', 'x': '486.34', 'y': '104.16', 'h': '10.44', 'w': '6.33'}, {'page': '2', 'x': '501.70', 'y': '105.62', 'h': '25.80', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '121.82', 'h': '276.08', 'w': '9.60'}], [{'page': '2', 'x': '364.17', 'y': '121.82', 'h': '163.01', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '138.02', 'h': '442.30', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '154.82', 'h': '100.58', 'w': '9.60'}, {'page': '2', 'x': '185.69', 'y': '153.36', 'h': '15.72', 'w': '6.33'}], [{'page': '2', 'x': '203.09', 'y': '154.82', 'h': '324.18', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '171.74', 'h': '322.27', 'w': '9.60'}, {'page': '2', 'x': '407.47', 'y': '170.28', 'h': '6.96', 'w': '6.33'}], [{'page': '2', 'x': '418.78', 'y': '171.74', 'h': '108.71', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '187.82', 'h': '414.70', 'w': '9.60'}], [{'page': '2', 'x': '502.65', 'y': '187.82', 'h': '24.63', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '204.02', 'h': '442.26', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '220.94', 'h': '57.59', 'w': '9.60'}, {'page': '2', 'x': '142.70', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '151.58', 'y': '220.94', 'h': '159.32', 'w': '9.60'}, {'page': '2', 'x': '310.99', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '319.75', 'y': '220.94', 'h': '207.62', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '237.02', 'h': '442.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '253.25', 'h': '128.26', 'w': '9.60'}], [{'page': '2', 'x': '217.73', 'y': '253.25', 'h': '309.57', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '269.33', 'h': '240.41', 'w': '9.60'}], [{'page': '2', 'x': '329.07', 'y': '269.33', 'h': '198.16', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '285.53', 'h': '442.25', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '301.73', 'h': '110.33', 'w': '9.60'}]]\", \"text\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2402.02547.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"df806d2c-c726-4362-a3ac-6afa7ec64d08\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '500.05', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '512.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '523.96', 'h': '59.12', 'w': '8.64'}], [{'page': '1', 'x': '113.25', 'y': '523.96', 'h': '186.77', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '535.92', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '547.87', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '559.83', 'h': '67.72', 'w': '8.64'}], [{'page': '1', 'x': '120.49', 'y': '559.83', 'h': '179.54', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '571.78', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '583.74', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '595.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '607.65', 'h': '32.38', 'w': '8.64'}]]\", \"text\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"d2503d91-b1d2-4d61-998d-0a003066fd74\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"metadata\": {\"text\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"paper_title\": \"The Neural MMO Platform for Massively Multiagent Research\", \"section_title\": \"Configuration\", \"bboxes\": \"[[{'page': '2', 'x': '108.00', 'y': '636.76', 'h': '396.00', 'w': '9.03'}, {'page': '2', 'x': '108.00', 'y': '648.06', 'h': '240.75', 'w': '8.64'}], [{'page': '2', 'x': '351.86', 'y': '648.06', 'h': '152.14', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '658.97', 'h': '396.17', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '669.88', 'h': '69.14', 'w': '8.64'}], [{'page': '2', 'x': '180.22', 'y': '669.88', 'h': '323.78', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '680.79', 'h': '234.15', 'w': '8.64'}], [{'page': '2', 'x': '345.26', 'y': '680.79', 'h': '158.73', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '691.70', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '702.61', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '713.51', 'h': '163.51', 'w': '8.64'}], [{'page': '3', 'x': '108.00', 'y': '255.32', 'h': '395.99', 'w': '9.03'}, {'page': '3', 'x': '108.00', 'y': '266.62', 'h': '314.19', 'w': '8.64'}], [{'page': '3', 'x': '425.14', 'y': '266.62', 'h': '78.86', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '277.35', 'h': '396.00', 'w': '8.82'}, {'page': '3', 'x': '108.00', 'y': '288.44', 'h': '267.92', 'w': '8.64'}], [{'page': '3', 'x': '379.03', 'y': '288.44', 'h': '124.97', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '299.35', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '310.26', 'h': '130.97', 'w': '8.64'}], [{'page': '3', 'x': '242.05', 'y': '310.26', 'h': '261.94', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '321.16', 'h': '186.52', 'w': '8.64'}]]\", \"pages\": \"('2', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2110.07594.pdf\", \"section_number\": \"2.1\", \"para\": \"7\", \"_id\": \"91d435c1-be88-4fde-8e9f-263d78c2bb0c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"metadata\": {\"text\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"paper_title\": \"General Board Geometry\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '149.71', 'y': '532.90', 'h': '330.88', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '544.86', 'h': '188.03', 'w': '8.74'}], [{'page': '1', 'x': '326.23', 'y': '544.86', 'h': '154.36', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '556.81', 'h': '282.20', 'w': '8.74'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2111.11329.pdf\", \"section_number\": \"1\", \"para\": \"1\", \"_id\": \"e346bc45-87af-4bb6-a81b-44970631f61c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '408.22', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '420.18', 'h': '43.60', 'w': '8.64'}], [{'page': '9', 'x': '358.49', 'y': '420.18', 'h': '204.54', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '432.13', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '444.09', 'h': '194.64', 'w': '8.64'}], [{'page': '9', 'x': '509.84', 'y': '444.09', 'h': '53.20', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '456.04', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '468.00', 'h': '96.88', 'w': '8.64'}], [{'page': '9', 'x': '413.29', 'y': '468.00', 'h': '149.75', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '479.96', 'h': '238.97', 'w': '8.64'}], [{'page': '9', 'x': '321.94', 'y': '492.69', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '504.65', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '516.60', 'h': '61.95', 'w': '8.64'}], [{'page': '9', 'x': '376.59', 'y': '516.60', 'h': '186.45', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '528.56', 'h': '227.83', 'w': '8.64'}], [{'page': '9', 'x': '543.51', 'y': '528.56', 'h': '19.53', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '540.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '552.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '564.42', 'h': '205.95', 'w': '8.64'}]]\", \"text\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"263f8343-a94d-4f8d-baac-90b85b2b33e7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"metadata\": {\"pages\": \"('8', '8')\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"B. Future Work\", \"bboxes\": \"[[{'page': '8', 'x': '58.93', 'y': '126.29', 'h': '241.09', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '138.24', 'h': '251.06', 'w': '8.64'}], [{'page': '8', 'x': '48.96', 'y': '150.20', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '162.15', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '174.11', 'h': '84.00', 'w': '8.64'}], [{'page': '8', 'x': '136.91', 'y': '174.11', 'h': '163.11', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '186.06', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '198.02', 'h': '201.19', 'w': '8.64'}], [{'page': '8', 'x': '253.30', 'y': '198.02', 'h': '46.73', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '209.97', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '221.93', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '233.88', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '245.84', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '257.79', 'h': '136.47', 'w': '8.64'}], [{'page': '8', 'x': '189.50', 'y': '257.79', 'h': '110.53', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '269.75', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '281.70', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '293.66', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '305.61', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '317.57', 'h': '142.72', 'w': '8.64'}]]\", \"text\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"dea85849-9565-44f1-8f1e-6bc4b7f2b505\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Sokoban\", \"bboxes\": \"[[{'page': '3', 'x': '134.77', 'y': '512.51', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '524.46', 'h': '167.14', 'w': '8.74'}], [{'page': '3', 'x': '305.03', 'y': '524.46', 'h': '175.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '536.42', 'h': '86.55', 'w': '8.74'}], [{'page': '3', 'x': '224.84', 'y': '536.42', 'h': '197.65', 'w': '8.74'}], [{'page': '3', 'x': '426.01', 'y': '536.42', 'h': '54.58', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '548.37', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '560.33', 'h': '70.24', 'w': '8.74'}], [{'page': '3', 'x': '208.21', 'y': '560.33', 'h': '272.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '572.28', 'h': '53.32', 'w': '8.74'}], [{'page': '3', 'x': '192.65', 'y': '572.28', 'h': '287.94', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '584.24', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '596.19', 'h': '139.61', 'w': '8.74'}]]\", \"text\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.2\", \"para\": \"5\", \"_id\": \"bf6d7428-caa3-4aa1-80b5-d330762f3de5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Typed Graph Networks\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '515.50', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '527.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '48.96', 'y': '539.41', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '551.37', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '563.00', 'h': '251.06', 'w': '8.96'}, {'page': '1', 'x': '48.96', 'y': '575.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '587.23', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '599.19', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '611.14', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '623.10', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '635.05', 'h': '30.74', 'w': '8.64'}], [{'page': '1', 'x': '83.17', 'y': '635.05', 'h': '216.85', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '647.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '153.59', 'h': '142.90', 'w': '8.64'}], [{'page': '1', 'x': '458.01', 'y': '153.59', 'h': '105.38', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '165.55', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '177.50', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '189.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '311.51', 'y': '201.41', 'h': '251.53', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '213.37', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.33', 'h': '251.23', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.24', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.19', 'h': '140.93', 'w': '8.64'}], [{'page': '1', 'x': '456.08', 'y': '261.19', 'h': '106.96', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.15', 'h': '251.40', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.10', 'h': '136.56', 'w': '8.64'}]]\", \"text\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1901.07984.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"6aba2813-4b28-48fe-95fb-95e48ddd7549\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0x8bb4eca32f9e645b", "trace_id": "0x5a36d5209b8e2fc5873ca84ecfd6dcb0"}, "parent_id": "0x9a5f1f4117f4e7a1", "start_time": 1724841654229579907, "end_time": 1724841656026904140, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"2f0000cd2b7e47afafd18a95fbc6e88d\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"II. ALGORITHMS AND METHODS\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '641.49', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '653.44', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '665.40', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '677.35', 'h': '13.00', 'w': '8.64'}], [{'page': '1', 'x': '327.61', 'y': '677.35', 'h': '235.43', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '689.31', 'h': '39.68', 'w': '8.64'}], [{'page': '1', 'x': '355.15', 'y': '689.31', 'h': '187.21', 'w': '8.64'}, {'page': '1', 'x': '542.35', 'y': '687.64', 'h': '3.49', 'w': '6.05'}, {'page': '1', 'x': '546.34', 'y': '689.31', 'h': '2.49', 'w': '8.64'}]]\", \"text\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"e19a113d-e501-4f37-b4db-dc80fe890142\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Expert-Driven Genetic Algorithms for Simulating Evaluation Functions \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '146.02', 'h': '330.67', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '157.42', 'h': '94.11', 'w': '8.97'}], [{'page': '2', 'x': '169.44', 'y': '157.42', 'h': '248.26', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '168.94', 'h': '345.51', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '180.34', 'h': '277.72', 'w': '8.97'}], [{'page': '2', 'x': '353.40', 'y': '180.34', 'h': '64.34', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '191.86', 'h': '345.64', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '203.26', 'h': '345.55', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '214.78', 'h': '321.28', 'w': '8.97'}]]\", \"text\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1711.06841.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"c6addab5-dc64-4994-9fd1-00b986893d5d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Integration of cognitive tasks into artificial general intelligence test for large models\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '106.10', 'y': '88.70', 'h': '421.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '105.62', 'h': '185.51', 'w': '9.60'}, {'page': '2', 'x': '270.65', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '279.05', 'y': '105.62', 'h': '55.60', 'w': '9.60'}, {'page': '2', 'x': '334.63', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '348.31', 'y': '105.62', 'h': '34.09', 'w': '9.60'}, {'page': '2', 'x': '382.39', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '390.79', 'y': '105.62', 'h': '34.69', 'w': '9.60'}, {'page': '2', 'x': '425.50', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '439.06', 'y': '105.62', 'h': '47.29', 'w': '9.60'}, {'page': '2', 'x': '486.34', 'y': '104.16', 'h': '10.44', 'w': '6.33'}, {'page': '2', 'x': '501.70', 'y': '105.62', 'h': '25.80', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '121.82', 'h': '276.08', 'w': '9.60'}], [{'page': '2', 'x': '364.17', 'y': '121.82', 'h': '163.01', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '138.02', 'h': '442.30', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '154.82', 'h': '100.58', 'w': '9.60'}, {'page': '2', 'x': '185.69', 'y': '153.36', 'h': '15.72', 'w': '6.33'}], [{'page': '2', 'x': '203.09', 'y': '154.82', 'h': '324.18', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '171.74', 'h': '322.27', 'w': '9.60'}, {'page': '2', 'x': '407.47', 'y': '170.28', 'h': '6.96', 'w': '6.33'}], [{'page': '2', 'x': '418.78', 'y': '171.74', 'h': '108.71', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '187.82', 'h': '414.70', 'w': '9.60'}], [{'page': '2', 'x': '502.65', 'y': '187.82', 'h': '24.63', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '204.02', 'h': '442.26', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '220.94', 'h': '57.59', 'w': '9.60'}, {'page': '2', 'x': '142.70', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '151.58', 'y': '220.94', 'h': '159.32', 'w': '9.60'}, {'page': '2', 'x': '310.99', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '319.75', 'y': '220.94', 'h': '207.62', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '237.02', 'h': '442.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '253.25', 'h': '128.26', 'w': '9.60'}], [{'page': '2', 'x': '217.73', 'y': '253.25', 'h': '309.57', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '269.33', 'h': '240.41', 'w': '9.60'}], [{'page': '2', 'x': '329.07', 'y': '269.33', 'h': '198.16', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '285.53', 'h': '442.25', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '301.73', 'h': '110.33', 'w': '9.60'}]]\", \"text\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2402.02547.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"df806d2c-c726-4362-a3ac-6afa7ec64d08\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '500.05', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '512.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '523.96', 'h': '59.12', 'w': '8.64'}], [{'page': '1', 'x': '113.25', 'y': '523.96', 'h': '186.77', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '535.92', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '547.87', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '559.83', 'h': '67.72', 'w': '8.64'}], [{'page': '1', 'x': '120.49', 'y': '559.83', 'h': '179.54', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '571.78', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '583.74', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '595.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '607.65', 'h': '32.38', 'w': '8.64'}]]\", \"text\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"d2503d91-b1d2-4d61-998d-0a003066fd74\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"metadata\": {\"text\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"paper_title\": \"The Neural MMO Platform for Massively Multiagent Research\", \"section_title\": \"Configuration\", \"bboxes\": \"[[{'page': '2', 'x': '108.00', 'y': '636.76', 'h': '396.00', 'w': '9.03'}, {'page': '2', 'x': '108.00', 'y': '648.06', 'h': '240.75', 'w': '8.64'}], [{'page': '2', 'x': '351.86', 'y': '648.06', 'h': '152.14', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '658.97', 'h': '396.17', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '669.88', 'h': '69.14', 'w': '8.64'}], [{'page': '2', 'x': '180.22', 'y': '669.88', 'h': '323.78', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '680.79', 'h': '234.15', 'w': '8.64'}], [{'page': '2', 'x': '345.26', 'y': '680.79', 'h': '158.73', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '691.70', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '702.61', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '713.51', 'h': '163.51', 'w': '8.64'}], [{'page': '3', 'x': '108.00', 'y': '255.32', 'h': '395.99', 'w': '9.03'}, {'page': '3', 'x': '108.00', 'y': '266.62', 'h': '314.19', 'w': '8.64'}], [{'page': '3', 'x': '425.14', 'y': '266.62', 'h': '78.86', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '277.35', 'h': '396.00', 'w': '8.82'}, {'page': '3', 'x': '108.00', 'y': '288.44', 'h': '267.92', 'w': '8.64'}], [{'page': '3', 'x': '379.03', 'y': '288.44', 'h': '124.97', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '299.35', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '310.26', 'h': '130.97', 'w': '8.64'}], [{'page': '3', 'x': '242.05', 'y': '310.26', 'h': '261.94', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '321.16', 'h': '186.52', 'w': '8.64'}]]\", \"pages\": \"('2', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2110.07594.pdf\", \"section_number\": \"2.1\", \"para\": \"7\", \"_id\": \"91d435c1-be88-4fde-8e9f-263d78c2bb0c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"metadata\": {\"text\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"paper_title\": \"General Board Geometry\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '149.71', 'y': '532.90', 'h': '330.88', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '544.86', 'h': '188.03', 'w': '8.74'}], [{'page': '1', 'x': '326.23', 'y': '544.86', 'h': '154.36', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '556.81', 'h': '282.20', 'w': '8.74'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2111.11329.pdf\", \"section_number\": \"1\", \"para\": \"1\", \"_id\": \"e346bc45-87af-4bb6-a81b-44970631f61c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '408.22', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '420.18', 'h': '43.60', 'w': '8.64'}], [{'page': '9', 'x': '358.49', 'y': '420.18', 'h': '204.54', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '432.13', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '444.09', 'h': '194.64', 'w': '8.64'}], [{'page': '9', 'x': '509.84', 'y': '444.09', 'h': '53.20', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '456.04', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '468.00', 'h': '96.88', 'w': '8.64'}], [{'page': '9', 'x': '413.29', 'y': '468.00', 'h': '149.75', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '479.96', 'h': '238.97', 'w': '8.64'}], [{'page': '9', 'x': '321.94', 'y': '492.69', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '504.65', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '516.60', 'h': '61.95', 'w': '8.64'}], [{'page': '9', 'x': '376.59', 'y': '516.60', 'h': '186.45', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '528.56', 'h': '227.83', 'w': '8.64'}], [{'page': '9', 'x': '543.51', 'y': '528.56', 'h': '19.53', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '540.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '552.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '564.42', 'h': '205.95', 'w': '8.64'}]]\", \"text\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"263f8343-a94d-4f8d-baac-90b85b2b33e7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"metadata\": {\"pages\": \"('8', '8')\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"B. Future Work\", \"bboxes\": \"[[{'page': '8', 'x': '58.93', 'y': '126.29', 'h': '241.09', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '138.24', 'h': '251.06', 'w': '8.64'}], [{'page': '8', 'x': '48.96', 'y': '150.20', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '162.15', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '174.11', 'h': '84.00', 'w': '8.64'}], [{'page': '8', 'x': '136.91', 'y': '174.11', 'h': '163.11', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '186.06', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '198.02', 'h': '201.19', 'w': '8.64'}], [{'page': '8', 'x': '253.30', 'y': '198.02', 'h': '46.73', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '209.97', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '221.93', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '233.88', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '245.84', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '257.79', 'h': '136.47', 'w': '8.64'}], [{'page': '8', 'x': '189.50', 'y': '257.79', 'h': '110.53', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '269.75', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '281.70', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '293.66', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '305.61', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '317.57', 'h': '142.72', 'w': '8.64'}]]\", \"text\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"dea85849-9565-44f1-8f1e-6bc4b7f2b505\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Sokoban\", \"bboxes\": \"[[{'page': '3', 'x': '134.77', 'y': '512.51', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '524.46', 'h': '167.14', 'w': '8.74'}], [{'page': '3', 'x': '305.03', 'y': '524.46', 'h': '175.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '536.42', 'h': '86.55', 'w': '8.74'}], [{'page': '3', 'x': '224.84', 'y': '536.42', 'h': '197.65', 'w': '8.74'}], [{'page': '3', 'x': '426.01', 'y': '536.42', 'h': '54.58', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '548.37', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '560.33', 'h': '70.24', 'w': '8.74'}], [{'page': '3', 'x': '208.21', 'y': '560.33', 'h': '272.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '572.28', 'h': '53.32', 'w': '8.74'}], [{'page': '3', 'x': '192.65', 'y': '572.28', 'h': '287.94', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '584.24', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '596.19', 'h': '139.61', 'w': '8.74'}]]\", \"text\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.2\", \"para\": \"5\", \"_id\": \"bf6d7428-caa3-4aa1-80b5-d330762f3de5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Typed Graph Networks\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '515.50', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '527.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '48.96', 'y': '539.41', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '551.37', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '563.00', 'h': '251.06', 'w': '8.96'}, {'page': '1', 'x': '48.96', 'y': '575.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '587.23', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '599.19', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '611.14', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '623.10', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '635.05', 'h': '30.74', 'w': '8.64'}], [{'page': '1', 'x': '83.17', 'y': '635.05', 'h': '216.85', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '647.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '153.59', 'h': '142.90', 'w': '8.64'}], [{'page': '1', 'x': '458.01', 'y': '153.59', 'h': '105.38', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '165.55', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '177.50', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '189.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '311.51', 'y': '201.41', 'h': '251.53', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '213.37', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.33', 'h': '251.23', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.24', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.19', 'h': '140.93', 'w': '8.64'}], [{'page': '1', 'x': '456.08', 'y': '261.19', 'h': '106.96', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.15', 'h': '251.40', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.10', 'h': '136.56', 'w': '8.64'}]]\", \"text\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1901.07984.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"6aba2813-4b28-48fe-95fb-95e48ddd7549\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\"}", "mlflow.spanOutputs": "{\"output_text\": \" The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x6cfb3aa03fe9df32", "trace_id": "0x5a36d5209b8e2fc5873ca84ecfd6dcb0"}, "parent_id": "0x8bb4eca32f9e645b", "start_time": 1724841654230520232, "end_time": 1724841656026627104, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"2f0000cd2b7e47afafd18a95fbc6e88d\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\", \"context\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\\n\\nThe ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\\n\\nLarge language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\\n\\nIn computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\\n\\nModular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\\n\\nA key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\\n\\nA work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\\n\\nWhile this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\\n\\nSokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\\n\\nMachine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\"}", "mlflow.spanOutputs": "{\"text\": \" The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.\"}"}, "events": [{"name": "text", "timestamp": 1724841654230612, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nThe algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\n\nThe ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\n\nLarge language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\n\nIn computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\n\nModular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\n\nA key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\n\nA work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\n\nWhile this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\n\nSokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\n\nMachine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\n\nQuestion: What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x8de3dee436e6b25d", "trace_id": "0x5a36d5209b8e2fc5873ca84ecfd6dcb0"}, "parent_id": "0x6cfb3aa03fe9df32", "start_time": 1724841654230981122, "end_time": 1724841656026294886, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"2f0000cd2b7e47afafd18a95fbc6e88d\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nThe algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\\n\\nThe ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\\n\\nLarge language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\\n\\nIn computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\\n\\nModular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\\n\\nA key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\\n\\nA work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\\n\\nWhile this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\\n\\nSokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\\n\\nMachine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\\n\\nQuestion: What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 1609, \"completion_tokens\": 88, \"total_tokens\": 1697}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?\"}", "response": "{\"result\": \" The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.\", \"source_documents\": [{\"page_content\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"II. ALGORITHMS AND METHODS\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '641.49', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '653.44', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '665.40', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '677.35', 'h': '13.00', 'w': '8.64'}], [{'page': '1', 'x': '327.61', 'y': '677.35', 'h': '235.43', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '689.31', 'h': '39.68', 'w': '8.64'}], [{'page': '1', 'x': '355.15', 'y': '689.31', 'h': '187.21', 'w': '8.64'}, {'page': '1', 'x': '542.35', 'y': '687.64', 'h': '3.49', 'w': '6.05'}, {'page': '1', 'x': '546.34', 'y': '689.31', 'h': '2.49', 'w': '8.64'}]]\", \"text\": \"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"e19a113d-e501-4f37-b4db-dc80fe890142\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Expert-Driven Genetic Algorithms for Simulating Evaluation Functions \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '146.02', 'h': '330.67', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '157.42', 'h': '94.11', 'w': '8.97'}], [{'page': '2', 'x': '169.44', 'y': '157.42', 'h': '248.26', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '168.94', 'h': '345.51', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '180.34', 'h': '277.72', 'w': '8.97'}], [{'page': '2', 'x': '353.40', 'y': '180.34', 'h': '64.34', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '191.86', 'h': '345.64', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '203.26', 'h': '345.55', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '214.78', 'h': '321.28', 'w': '8.97'}]]\", \"text\": \"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1711.06841.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"c6addab5-dc64-4994-9fd1-00b986893d5d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Integration of cognitive tasks into artificial general intelligence test for large models\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '106.10', 'y': '88.70', 'h': '421.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '105.62', 'h': '185.51', 'w': '9.60'}, {'page': '2', 'x': '270.65', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '279.05', 'y': '105.62', 'h': '55.60', 'w': '9.60'}, {'page': '2', 'x': '334.63', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '348.31', 'y': '105.62', 'h': '34.09', 'w': '9.60'}, {'page': '2', 'x': '382.39', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '390.79', 'y': '105.62', 'h': '34.69', 'w': '9.60'}, {'page': '2', 'x': '425.50', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '439.06', 'y': '105.62', 'h': '47.29', 'w': '9.60'}, {'page': '2', 'x': '486.34', 'y': '104.16', 'h': '10.44', 'w': '6.33'}, {'page': '2', 'x': '501.70', 'y': '105.62', 'h': '25.80', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '121.82', 'h': '276.08', 'w': '9.60'}], [{'page': '2', 'x': '364.17', 'y': '121.82', 'h': '163.01', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '138.02', 'h': '442.30', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '154.82', 'h': '100.58', 'w': '9.60'}, {'page': '2', 'x': '185.69', 'y': '153.36', 'h': '15.72', 'w': '6.33'}], [{'page': '2', 'x': '203.09', 'y': '154.82', 'h': '324.18', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '171.74', 'h': '322.27', 'w': '9.60'}, {'page': '2', 'x': '407.47', 'y': '170.28', 'h': '6.96', 'w': '6.33'}], [{'page': '2', 'x': '418.78', 'y': '171.74', 'h': '108.71', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '187.82', 'h': '414.70', 'w': '9.60'}], [{'page': '2', 'x': '502.65', 'y': '187.82', 'h': '24.63', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '204.02', 'h': '442.26', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '220.94', 'h': '57.59', 'w': '9.60'}, {'page': '2', 'x': '142.70', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '151.58', 'y': '220.94', 'h': '159.32', 'w': '9.60'}, {'page': '2', 'x': '310.99', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '319.75', 'y': '220.94', 'h': '207.62', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '237.02', 'h': '442.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '253.25', 'h': '128.26', 'w': '9.60'}], [{'page': '2', 'x': '217.73', 'y': '253.25', 'h': '309.57', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '269.33', 'h': '240.41', 'w': '9.60'}], [{'page': '2', 'x': '329.07', 'y': '269.33', 'h': '198.16', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '285.53', 'h': '442.25', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '301.73', 'h': '110.33', 'w': '9.60'}]]\", \"text\": \"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2402.02547.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"df806d2c-c726-4362-a3ac-6afa7ec64d08\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '500.05', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '512.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '523.96', 'h': '59.12', 'w': '8.64'}], [{'page': '1', 'x': '113.25', 'y': '523.96', 'h': '186.77', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '535.92', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '547.87', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '559.83', 'h': '67.72', 'w': '8.64'}], [{'page': '1', 'x': '120.49', 'y': '559.83', 'h': '179.54', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '571.78', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '583.74', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '595.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '607.65', 'h': '32.38', 'w': '8.64'}]]\", \"text\": \"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"2\", \"_id\": \"d2503d91-b1d2-4d61-998d-0a003066fd74\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"metadata\": {\"text\": \"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.\", \"paper_title\": \"The Neural MMO Platform for Massively Multiagent Research\", \"section_title\": \"Configuration\", \"bboxes\": \"[[{'page': '2', 'x': '108.00', 'y': '636.76', 'h': '396.00', 'w': '9.03'}, {'page': '2', 'x': '108.00', 'y': '648.06', 'h': '240.75', 'w': '8.64'}], [{'page': '2', 'x': '351.86', 'y': '648.06', 'h': '152.14', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '658.97', 'h': '396.17', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '669.88', 'h': '69.14', 'w': '8.64'}], [{'page': '2', 'x': '180.22', 'y': '669.88', 'h': '323.78', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '680.79', 'h': '234.15', 'w': '8.64'}], [{'page': '2', 'x': '345.26', 'y': '680.79', 'h': '158.73', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '691.70', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '702.61', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '713.51', 'h': '163.51', 'w': '8.64'}], [{'page': '3', 'x': '108.00', 'y': '255.32', 'h': '395.99', 'w': '9.03'}, {'page': '3', 'x': '108.00', 'y': '266.62', 'h': '314.19', 'w': '8.64'}], [{'page': '3', 'x': '425.14', 'y': '266.62', 'h': '78.86', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '277.35', 'h': '396.00', 'w': '8.82'}, {'page': '3', 'x': '108.00', 'y': '288.44', 'h': '267.92', 'w': '8.64'}], [{'page': '3', 'x': '379.03', 'y': '288.44', 'h': '124.97', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '299.35', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '310.26', 'h': '130.97', 'w': '8.64'}], [{'page': '3', 'x': '242.05', 'y': '310.26', 'h': '261.94', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '321.16', 'h': '186.52', 'w': '8.64'}]]\", \"pages\": \"('2', '3')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2110.07594.pdf\", \"section_number\": \"2.1\", \"para\": \"7\", \"_id\": \"91d435c1-be88-4fde-8e9f-263d78c2bb0c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"metadata\": {\"text\": \"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.\", \"paper_title\": \"General Board Geometry\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '149.71', 'y': '532.90', 'h': '330.88', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '544.86', 'h': '188.03', 'w': '8.74'}], [{'page': '1', 'x': '326.23', 'y': '544.86', 'h': '154.36', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '556.81', 'h': '282.20', 'w': '8.74'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2111.11329.pdf\", \"section_number\": \"1\", \"para\": \"1\", \"_id\": \"e346bc45-87af-4bb6-a81b-44970631f61c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"metadata\": {\"pages\": \"('9', '9')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Related work\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '408.22', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '420.18', 'h': '43.60', 'w': '8.64'}], [{'page': '9', 'x': '358.49', 'y': '420.18', 'h': '204.54', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '432.13', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '444.09', 'h': '194.64', 'w': '8.64'}], [{'page': '9', 'x': '509.84', 'y': '444.09', 'h': '53.20', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '456.04', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '468.00', 'h': '96.88', 'w': '8.64'}], [{'page': '9', 'x': '413.29', 'y': '468.00', 'h': '149.75', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '479.96', 'h': '238.97', 'w': '8.64'}], [{'page': '9', 'x': '321.94', 'y': '492.69', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '504.65', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '516.60', 'h': '61.95', 'w': '8.64'}], [{'page': '9', 'x': '376.59', 'y': '516.60', 'h': '186.45', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '528.56', 'h': '227.83', 'w': '8.64'}], [{'page': '9', 'x': '543.51', 'y': '528.56', 'h': '19.53', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '540.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '552.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '564.42', 'h': '205.95', 'w': '8.64'}]]\", \"text\": \"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"263f8343-a94d-4f8d-baac-90b85b2b33e7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"metadata\": {\"pages\": \"('8', '8')\", \"paper_title\": \"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking\", \"section_title\": \"B. Future Work\", \"bboxes\": \"[[{'page': '8', 'x': '58.93', 'y': '126.29', 'h': '241.09', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '138.24', 'h': '251.06', 'w': '8.64'}], [{'page': '8', 'x': '48.96', 'y': '150.20', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '162.15', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '174.11', 'h': '84.00', 'w': '8.64'}], [{'page': '8', 'x': '136.91', 'y': '174.11', 'h': '163.11', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '186.06', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '198.02', 'h': '201.19', 'w': '8.64'}], [{'page': '8', 'x': '253.30', 'y': '198.02', 'h': '46.73', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '209.97', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '221.93', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '233.88', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '245.84', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '257.79', 'h': '136.47', 'w': '8.64'}], [{'page': '8', 'x': '189.50', 'y': '257.79', 'h': '110.53', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '269.75', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '281.70', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '293.66', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '305.61', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '317.57', 'h': '142.72', 'w': '8.64'}]]\", \"text\": \"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1809.02904.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"dea85849-9565-44f1-8f1e-6bc4b7f2b505\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"metadata\": {\"pages\": \"('3', '3')\", \"paper_title\": \"Potential-based Reward Shaping in Sokoban\", \"section_title\": \"Sokoban\", \"bboxes\": \"[[{'page': '3', 'x': '134.77', 'y': '512.51', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '524.46', 'h': '167.14', 'w': '8.74'}], [{'page': '3', 'x': '305.03', 'y': '524.46', 'h': '175.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '536.42', 'h': '86.55', 'w': '8.74'}], [{'page': '3', 'x': '224.84', 'y': '536.42', 'h': '197.65', 'w': '8.74'}], [{'page': '3', 'x': '426.01', 'y': '536.42', 'h': '54.58', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '548.37', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '560.33', 'h': '70.24', 'w': '8.74'}], [{'page': '3', 'x': '208.21', 'y': '560.33', 'h': '272.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '572.28', 'h': '53.32', 'w': '8.74'}], [{'page': '3', 'x': '192.65', 'y': '572.28', 'h': '287.94', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '584.24', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '596.19', 'h': '139.61', 'w': '8.74'}]]\", \"text\": \"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2109.05022.pdf\", \"section_number\": \"2.2\", \"para\": \"5\", \"_id\": \"bf6d7428-caa3-4aa1-80b5-d330762f3de5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Typed Graph Networks\", \"section_title\": \"I. INTRODUCTION\", \"bboxes\": \"[[{'page': '1', 'x': '58.93', 'y': '515.50', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '527.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '48.96', 'y': '539.41', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '551.37', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '563.00', 'h': '251.06', 'w': '8.96'}, {'page': '1', 'x': '48.96', 'y': '575.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '587.23', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '599.19', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '611.14', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '623.10', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '635.05', 'h': '30.74', 'w': '8.64'}], [{'page': '1', 'x': '83.17', 'y': '635.05', 'h': '216.85', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '647.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '153.59', 'h': '142.90', 'w': '8.64'}], [{'page': '1', 'x': '458.01', 'y': '153.59', 'h': '105.38', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '165.55', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '177.50', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '189.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '311.51', 'y': '201.41', 'h': '251.53', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '213.37', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.33', 'h': '251.23', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.24', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.19', 'h': '140.93', 'w': '8.64'}], [{'page': '1', 'x': '456.08', 'y': '261.19', 'h': '106.96', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.15', 'h': '251.40', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.10', 'h': '136.56', 'w': '8.64'}]]\", \"text\": \"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1901.07984.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"6aba2813-4b28-48fe-95fb-95e48ddd7549\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}