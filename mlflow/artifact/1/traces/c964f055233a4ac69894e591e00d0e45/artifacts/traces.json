{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x358a5ebf1b27844c", "trace_id": "0xa264258efb1e4110323a416357ca32a2"}, "parent_id": null, "start_time": 1724842088355034679, "end_time": 1724842089009897714, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c964f055233a4ac69894e591e00d0e45\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\"}", "mlflow.spanOutputs": "{\"result\": \" The deep network was trained for 44 hours on a 32-core server with 3 GPUs.\", \"source_documents\": [{\"page_content\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"metadata\": {\"text\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Comparison with Other RL Research\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '165.59', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '177.55', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '189.50', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '201.46', 'h': '35.54', 'w': '8.64'}], [{'page': '10', 'x': '351.78', 'y': '201.46', 'h': '211.26', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '213.41', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '225.37', 'h': '25.09', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9e430404-97b3-4346-b0c0-e2cdecb1b206\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"metadata\": {\"pages\": \"('11', '11')\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Results\", \"bboxes\": \"[[{'page': '11', 'x': '72.00', 'y': '319.27', 'h': '468.00', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '331.23', 'h': '86.63', 'w': '9.58'}], [{'page': '11', 'x': '163.27', 'y': '331.23', 'h': '376.74', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '343.18', 'h': '157.98', 'w': '9.58'}], [{'page': '11', 'x': '236.88', 'y': '343.18', 'h': '303.12', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '355.14', 'h': '40.86', 'w': '9.58'}], [{'page': '11', 'x': '115.95', 'y': '355.14', 'h': '357.39', 'w': '9.58'}]]\", \"text\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"5\", \"para\": \"3\", \"_id\": \"57ab9ca6-bf2c-466c-a038-7e355bc92112\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML\", \"section_title\": \"Experimental Settings\", \"bboxes\": \"[[{'page': '6', 'x': '320.82', 'y': '222.23', 'h': '97.24', 'w': '8.96'}], [{'page': '6', 'x': '424.80', 'y': '222.62', 'h': '120.31', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '234.58', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '246.53', 'h': '236.25', 'w': '8.64'}], [{'page': '6', 'x': '308.86', 'y': '258.49', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '270.44', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '282.08', 'h': '236.25', 'w': '8.96'}, {'page': '6', 'x': '308.86', 'y': '294.35', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '306.31', 'h': '217.95', 'w': '8.64'}], [{'page': '6', 'x': '531.43', 'y': '306.31', 'h': '13.69', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '318.27', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '330.22', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '342.18', 'h': '77.99', 'w': '8.64'}], [{'page': '6', 'x': '389.93', 'y': '342.18', 'h': '155.19', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '354.13', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '366.09', 'h': '110.83', 'w': '8.64'}], [{'page': '6', 'x': '423.22', 'y': '366.09', 'h': '121.89', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '378.04', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '390.00', 'h': '125.11', 'w': '8.64'}], [{'page': '6', 'x': '438.09', 'y': '390.00', 'h': '107.03', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '401.95', 'h': '182.68', 'w': '8.64'}]]\", \"text\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1903.08362.pdf\", \"section_number\": \"4.1.\", \"para\": \"6\", \"_id\": \"a9a4072a-074f-4a06-b613-8e50c185e8a3\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"metadata\": {\"pages\": \"('7', '8')\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"VISUAL SUDOKU CLASSIFICATION\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '659.91', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '670.65', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '681.39', 'h': '246.77', 'w': '8.64'}], [{'page': '7', 'x': '359.04', 'y': '681.39', 'h': '144.97', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '692.13', 'h': '396.00', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '85.34', 'h': '57.64', 'w': '8.64'}], [{'page': '8', 'x': '169.07', 'y': '85.34', 'h': '262.10', 'w': '8.64'}], [{'page': '8', 'x': '434.59', 'y': '85.34', 'h': '69.41', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '96.08', 'h': '251.01', 'w': '8.64'}], [{'page': '8', 'x': '364.07', 'y': '96.08', 'h': '139.93', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '106.82', 'h': '280.72', 'w': '8.64'}]]\", \"text\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"5.2\", \"para\": \"4\", \"_id\": \"f49e6c09-0ea0-4e96-bf8e-32d4de6eed60\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"metadata\": {\"text\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Rubik's Cube\", \"bboxes\": \"[[{'page': '6', 'x': '321.94', 'y': '510.57', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '522.52', 'h': '27.40', 'w': '8.64'}], [{'page': '6', 'x': '344.95', 'y': '522.52', 'h': '218.08', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '534.16', 'h': '251.06', 'w': '9.65'}, {'page': '6', 'x': '311.98', 'y': '546.11', 'h': '98.03', 'w': '9.65'}, {'page': '6', 'x': '410.01', 'y': '544.76', 'h': '3.49', 'w': '6.05'}, {'page': '6', 'x': '413.99', 'y': '546.43', 'h': '100.67', 'w': '8.64'}], [{'page': '6', 'x': '517.14', 'y': '546.43', 'h': '45.90', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '558.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '570.34', 'h': '234.33', 'w': '8.64'}], [{'page': '6', 'x': '550.00', 'y': '570.34', 'h': '13.03', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '582.30', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '593.93', 'h': '251.06', 'w': '8.96'}, {'page': '6', 'x': '311.98', 'y': '606.21', 'h': '59.69', 'w': '8.64'}], [{'page': '6', 'x': '375.72', 'y': '606.21', 'h': '187.32', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '618.16', 'h': '21.30', 'w': '8.64'}], [{'page': '6', 'x': '336.64', 'y': '617.98', 'h': '226.40', 'w': '8.82'}, {'page': '6', 'x': '311.98', 'y': '629.80', 'h': '135.98', 'w': '9.65'}], [{'page': '6', 'x': '450.83', 'y': '630.12', 'h': '112.21', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '642.07', 'h': '72.71', 'w': '8.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"0bd48af0-1080-4d25-9e89-d61d94b6d93d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Logic Guided Genetic Algorithms\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '329.46', 'y': '375.30', 'h': '228.54', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '386.26', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '397.22', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '408.18', 'h': '238.50', 'w': '8.64'}], [{'page': '1', 'x': '319.50', 'y': '419.14', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '430.10', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '441.06', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '452.02', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '462.98', 'h': '140.40', 'w': '8.64'}]]\", \"text\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2010.11328.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"e79804f7-d454-4216-a49e-f63822bf79ef\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"metadata\": {\"text\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Training Algorithms\", \"bboxes\": \"[[{'page': '9', 'x': '72.00', 'y': '515.54', 'h': '202.08', 'w': '9.58'}], [{'page': '9', 'x': '278.00', 'y': '515.54', 'h': '262.01', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '527.49', 'h': '82.48', 'w': '9.58'}], [{'page': '9', 'x': '158.51', 'y': '527.49', 'h': '269.13', 'w': '9.58'}], [{'page': '9', 'x': '431.65', 'y': '527.49', 'h': '108.35', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '539.45', 'h': '129.83', 'w': '9.58'}], [{'page': '9', 'x': '204.10', 'y': '539.45', 'h': '335.90', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '551.40', 'h': '102.56', 'w': '9.58'}], [{'page': '10', 'x': '215.61', 'y': '181.49', 'h': '248.47', 'w': '7.60'}], [{'page': '10', 'x': '469.07', 'y': '181.49', 'h': '70.93', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '190.96', 'h': '153.06', 'w': '7.60'}], [{'page': '10', 'x': '230.10', 'y': '190.96', 'h': '205.94', 'w': '7.60'}], [{'page': '10', 'x': '441.09', 'y': '190.96', 'h': '98.91', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '200.42', 'h': '206.11', 'w': '7.60'}], [{'page': '10', 'x': '281.40', 'y': '200.42', 'h': '258.60', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '209.88', 'h': '126.29', 'w': '7.60'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"3.3.1\", \"para\": \"9\", \"_id\": \"44847082-6d11-4b98-bc58-7b3cd6fb659a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"metadata\": {\"pages\": \"('17', '18')\", \"paper_title\": \"THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES\", \"section_title\": \"D.2 n-ARY LAYERS\", \"bboxes\": \"[[{'page': '17', 'x': '108.00', 'y': '710.06', 'h': '374.35', 'w': '9.96'}, {'page': '17', 'x': '482.48', 'y': '714.73', 'h': '3.97', 'w': '6.97'}, {'page': '17', 'x': '486.95', 'y': '710.06', 'h': '17.05', 'w': '9.96'}, {'page': '17', 'x': '108.00', 'y': '723.38', 'h': '215.44', 'w': '8.64'}, {'page': '17', 'x': '327.90', 'y': '722.14', 'h': '20.48', 'w': '17.29'}, {'page': '17', 'x': '350.04', 'y': '722.39', 'h': '9.96', 'w': '9.96'}, {'page': '17', 'x': '360.01', 'y': '721.02', 'h': '25.34', 'w': '8.05'}, {'page': '17', 'x': '385.85', 'y': '723.38', 'h': '2.49', 'w': '8.64'}], [{'page': '17', 'x': '397.34', 'y': '722.39', 'h': '106.67', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '85.34', 'h': '123.98', 'w': '8.64'}], [{'page': '18', 'x': '239.23', 'y': '84.35', 'h': '181.07', 'w': '9.96'}, {'page': '18', 'x': '425.91', 'y': '84.10', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '438.98', 'y': '84.35', 'h': '65.02', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '96.69', 'h': '6.37', 'w': '9.96'}, {'page': '18', 'x': '117.18', 'y': '96.44', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '126.59', 'y': '96.69', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '139.87', 'y': '96.44', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '153.71', 'y': '95.31', 'h': '4.92', 'w': '6.97'}, {'page': '18', 'x': '159.13', 'y': '97.67', 'h': '2.49', 'w': '8.64'}], [{'page': '18', 'x': '164.61', 'y': '96.69', 'h': '54.84', 'w': '9.96'}, {'page': '18', 'x': '219.59', 'y': '101.36', 'h': '3.97', 'w': '6.97'}, {'page': '18', 'x': '224.06', 'y': '96.69', 'h': '13.73', 'w': '9.96'}, {'page': '18', 'x': '238.95', 'y': '96.44', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '247.85', 'y': '96.69', 'h': '196.00', 'w': '9.96'}, {'page': '18', 'x': '446.06', 'y': '96.44', 'h': '20.48', 'w': '17.29'}, {'page': '18', 'x': '468.19', 'y': '96.69', 'h': '9.96', 'w': '9.96'}, {'page': '18', 'x': '478.16', 'y': '95.31', 'h': '25.35', 'w': '8.05'}, {'page': '18', 'x': '108.00', 'y': '107.64', 'h': '396.00', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '118.60', 'h': '115.94', 'w': '9.96'}, {'page': '18', 'x': '229.40', 'y': '118.35', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '241.82', 'y': '118.60', 'h': '186.58', 'w': '9.96'}, {'page': '18', 'x': '433.07', 'y': '118.35', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '444.39', 'y': '118.60', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '457.67', 'y': '118.35', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '475.03', 'y': '119.59', 'h': '28.97', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '128.74', 'h': '41.82', 'w': '12.48'}], [{'page': '18', 'x': '156.60', 'y': '131.26', 'h': '226.47', 'w': '9.96'}, {'page': '18', 'x': '388.27', 'y': '131.01', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '400.13', 'y': '131.26', 'h': '103.87', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '142.22', 'h': '65.24', 'w': '9.96'}, {'page': '18', 'x': '177.48', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '188.37', 'y': '142.22', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '213.25', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '224.15', 'y': '142.22', 'h': '50.63', 'w': '9.96'}, {'page': '18', 'x': '277.92', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '288.82', 'y': '142.22', 'h': '7.47', 'w': '9.96'}], [{'page': '18', 'x': '303.18', 'y': '143.21', 'h': '200.83', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '152.36', 'h': '59.88', 'w': '12.48'}, {'page': '18', 'x': '172.69', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '182.66', 'y': '154.88', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '206.59', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '216.56', 'y': '154.88', 'h': '63.42', 'w': '9.96'}, {'page': '18', 'x': '282.19', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '292.15', 'y': '154.88', 'h': '7.47', 'w': '9.96'}]]\", \"text\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1611.00712.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"3b42039c-697d-41c1-bc3c-c8534474c72f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"metadata\": {\"text\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"paper_title\": \"Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge\", \"section_title\": \"Overview of evaluated algorithms\", \"bboxes\": \"[[{'page': '4', 'x': '62.25', 'y': '483.25', 'h': '421.80', 'w': '9.14'}], [{'page': '4', 'x': '487.76', 'y': '483.25', 'h': '76.24', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '494.79', 'h': '176.57', 'w': '9.14'}], [{'page': '4', 'x': '228.04', 'y': '494.79', 'h': '335.96', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '506.33', 'h': '516.00', 'w': '9.14'}], [{'page': '4', 'x': '48.00', 'y': '517.87', 'h': '516.00', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '529.41', 'h': '291.38', 'w': '9.14'}], [{'page': '4', 'x': '342.79', 'y': '529.41', 'h': '221.22', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '540.95', 'h': '250.91', 'w': '9.14'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2308.05862.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"644f09e8-fa9c-4d98-84d7-0a6761c90cd5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0x35bd247a9ecef373", "trace_id": "0xa264258efb1e4110323a416357ca32a2"}, "parent_id": "0x358a5ebf1b27844c", "start_time": 1724842088359141807, "end_time": 1724842088389643233, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c964f055233a4ac69894e591e00d0e45\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\"", "mlflow.spanOutputs": "[{\"page_content\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"metadata\": {\"text\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Comparison with Other RL Research\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '165.59', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '177.55', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '189.50', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '201.46', 'h': '35.54', 'w': '8.64'}], [{'page': '10', 'x': '351.78', 'y': '201.46', 'h': '211.26', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '213.41', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '225.37', 'h': '25.09', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9e430404-97b3-4346-b0c0-e2cdecb1b206\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"metadata\": {\"pages\": \"('11', '11')\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Results\", \"bboxes\": \"[[{'page': '11', 'x': '72.00', 'y': '319.27', 'h': '468.00', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '331.23', 'h': '86.63', 'w': '9.58'}], [{'page': '11', 'x': '163.27', 'y': '331.23', 'h': '376.74', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '343.18', 'h': '157.98', 'w': '9.58'}], [{'page': '11', 'x': '236.88', 'y': '343.18', 'h': '303.12', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '355.14', 'h': '40.86', 'w': '9.58'}], [{'page': '11', 'x': '115.95', 'y': '355.14', 'h': '357.39', 'w': '9.58'}]]\", \"text\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"5\", \"para\": \"3\", \"_id\": \"57ab9ca6-bf2c-466c-a038-7e355bc92112\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML\", \"section_title\": \"Experimental Settings\", \"bboxes\": \"[[{'page': '6', 'x': '320.82', 'y': '222.23', 'h': '97.24', 'w': '8.96'}], [{'page': '6', 'x': '424.80', 'y': '222.62', 'h': '120.31', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '234.58', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '246.53', 'h': '236.25', 'w': '8.64'}], [{'page': '6', 'x': '308.86', 'y': '258.49', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '270.44', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '282.08', 'h': '236.25', 'w': '8.96'}, {'page': '6', 'x': '308.86', 'y': '294.35', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '306.31', 'h': '217.95', 'w': '8.64'}], [{'page': '6', 'x': '531.43', 'y': '306.31', 'h': '13.69', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '318.27', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '330.22', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '342.18', 'h': '77.99', 'w': '8.64'}], [{'page': '6', 'x': '389.93', 'y': '342.18', 'h': '155.19', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '354.13', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '366.09', 'h': '110.83', 'w': '8.64'}], [{'page': '6', 'x': '423.22', 'y': '366.09', 'h': '121.89', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '378.04', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '390.00', 'h': '125.11', 'w': '8.64'}], [{'page': '6', 'x': '438.09', 'y': '390.00', 'h': '107.03', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '401.95', 'h': '182.68', 'w': '8.64'}]]\", \"text\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1903.08362.pdf\", \"section_number\": \"4.1.\", \"para\": \"6\", \"_id\": \"a9a4072a-074f-4a06-b613-8e50c185e8a3\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"metadata\": {\"pages\": \"('7', '8')\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"VISUAL SUDOKU CLASSIFICATION\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '659.91', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '670.65', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '681.39', 'h': '246.77', 'w': '8.64'}], [{'page': '7', 'x': '359.04', 'y': '681.39', 'h': '144.97', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '692.13', 'h': '396.00', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '85.34', 'h': '57.64', 'w': '8.64'}], [{'page': '8', 'x': '169.07', 'y': '85.34', 'h': '262.10', 'w': '8.64'}], [{'page': '8', 'x': '434.59', 'y': '85.34', 'h': '69.41', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '96.08', 'h': '251.01', 'w': '8.64'}], [{'page': '8', 'x': '364.07', 'y': '96.08', 'h': '139.93', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '106.82', 'h': '280.72', 'w': '8.64'}]]\", \"text\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"5.2\", \"para\": \"4\", \"_id\": \"f49e6c09-0ea0-4e96-bf8e-32d4de6eed60\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"metadata\": {\"text\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Rubik's Cube\", \"bboxes\": \"[[{'page': '6', 'x': '321.94', 'y': '510.57', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '522.52', 'h': '27.40', 'w': '8.64'}], [{'page': '6', 'x': '344.95', 'y': '522.52', 'h': '218.08', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '534.16', 'h': '251.06', 'w': '9.65'}, {'page': '6', 'x': '311.98', 'y': '546.11', 'h': '98.03', 'w': '9.65'}, {'page': '6', 'x': '410.01', 'y': '544.76', 'h': '3.49', 'w': '6.05'}, {'page': '6', 'x': '413.99', 'y': '546.43', 'h': '100.67', 'w': '8.64'}], [{'page': '6', 'x': '517.14', 'y': '546.43', 'h': '45.90', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '558.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '570.34', 'h': '234.33', 'w': '8.64'}], [{'page': '6', 'x': '550.00', 'y': '570.34', 'h': '13.03', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '582.30', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '593.93', 'h': '251.06', 'w': '8.96'}, {'page': '6', 'x': '311.98', 'y': '606.21', 'h': '59.69', 'w': '8.64'}], [{'page': '6', 'x': '375.72', 'y': '606.21', 'h': '187.32', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '618.16', 'h': '21.30', 'w': '8.64'}], [{'page': '6', 'x': '336.64', 'y': '617.98', 'h': '226.40', 'w': '8.82'}, {'page': '6', 'x': '311.98', 'y': '629.80', 'h': '135.98', 'w': '9.65'}], [{'page': '6', 'x': '450.83', 'y': '630.12', 'h': '112.21', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '642.07', 'h': '72.71', 'w': '8.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"0bd48af0-1080-4d25-9e89-d61d94b6d93d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Logic Guided Genetic Algorithms\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '329.46', 'y': '375.30', 'h': '228.54', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '386.26', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '397.22', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '408.18', 'h': '238.50', 'w': '8.64'}], [{'page': '1', 'x': '319.50', 'y': '419.14', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '430.10', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '441.06', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '452.02', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '462.98', 'h': '140.40', 'w': '8.64'}]]\", \"text\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2010.11328.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"e79804f7-d454-4216-a49e-f63822bf79ef\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"metadata\": {\"text\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Training Algorithms\", \"bboxes\": \"[[{'page': '9', 'x': '72.00', 'y': '515.54', 'h': '202.08', 'w': '9.58'}], [{'page': '9', 'x': '278.00', 'y': '515.54', 'h': '262.01', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '527.49', 'h': '82.48', 'w': '9.58'}], [{'page': '9', 'x': '158.51', 'y': '527.49', 'h': '269.13', 'w': '9.58'}], [{'page': '9', 'x': '431.65', 'y': '527.49', 'h': '108.35', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '539.45', 'h': '129.83', 'w': '9.58'}], [{'page': '9', 'x': '204.10', 'y': '539.45', 'h': '335.90', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '551.40', 'h': '102.56', 'w': '9.58'}], [{'page': '10', 'x': '215.61', 'y': '181.49', 'h': '248.47', 'w': '7.60'}], [{'page': '10', 'x': '469.07', 'y': '181.49', 'h': '70.93', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '190.96', 'h': '153.06', 'w': '7.60'}], [{'page': '10', 'x': '230.10', 'y': '190.96', 'h': '205.94', 'w': '7.60'}], [{'page': '10', 'x': '441.09', 'y': '190.96', 'h': '98.91', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '200.42', 'h': '206.11', 'w': '7.60'}], [{'page': '10', 'x': '281.40', 'y': '200.42', 'h': '258.60', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '209.88', 'h': '126.29', 'w': '7.60'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"3.3.1\", \"para\": \"9\", \"_id\": \"44847082-6d11-4b98-bc58-7b3cd6fb659a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"metadata\": {\"pages\": \"('17', '18')\", \"paper_title\": \"THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES\", \"section_title\": \"D.2 n-ARY LAYERS\", \"bboxes\": \"[[{'page': '17', 'x': '108.00', 'y': '710.06', 'h': '374.35', 'w': '9.96'}, {'page': '17', 'x': '482.48', 'y': '714.73', 'h': '3.97', 'w': '6.97'}, {'page': '17', 'x': '486.95', 'y': '710.06', 'h': '17.05', 'w': '9.96'}, {'page': '17', 'x': '108.00', 'y': '723.38', 'h': '215.44', 'w': '8.64'}, {'page': '17', 'x': '327.90', 'y': '722.14', 'h': '20.48', 'w': '17.29'}, {'page': '17', 'x': '350.04', 'y': '722.39', 'h': '9.96', 'w': '9.96'}, {'page': '17', 'x': '360.01', 'y': '721.02', 'h': '25.34', 'w': '8.05'}, {'page': '17', 'x': '385.85', 'y': '723.38', 'h': '2.49', 'w': '8.64'}], [{'page': '17', 'x': '397.34', 'y': '722.39', 'h': '106.67', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '85.34', 'h': '123.98', 'w': '8.64'}], [{'page': '18', 'x': '239.23', 'y': '84.35', 'h': '181.07', 'w': '9.96'}, {'page': '18', 'x': '425.91', 'y': '84.10', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '438.98', 'y': '84.35', 'h': '65.02', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '96.69', 'h': '6.37', 'w': '9.96'}, {'page': '18', 'x': '117.18', 'y': '96.44', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '126.59', 'y': '96.69', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '139.87', 'y': '96.44', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '153.71', 'y': '95.31', 'h': '4.92', 'w': '6.97'}, {'page': '18', 'x': '159.13', 'y': '97.67', 'h': '2.49', 'w': '8.64'}], [{'page': '18', 'x': '164.61', 'y': '96.69', 'h': '54.84', 'w': '9.96'}, {'page': '18', 'x': '219.59', 'y': '101.36', 'h': '3.97', 'w': '6.97'}, {'page': '18', 'x': '224.06', 'y': '96.69', 'h': '13.73', 'w': '9.96'}, {'page': '18', 'x': '238.95', 'y': '96.44', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '247.85', 'y': '96.69', 'h': '196.00', 'w': '9.96'}, {'page': '18', 'x': '446.06', 'y': '96.44', 'h': '20.48', 'w': '17.29'}, {'page': '18', 'x': '468.19', 'y': '96.69', 'h': '9.96', 'w': '9.96'}, {'page': '18', 'x': '478.16', 'y': '95.31', 'h': '25.35', 'w': '8.05'}, {'page': '18', 'x': '108.00', 'y': '107.64', 'h': '396.00', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '118.60', 'h': '115.94', 'w': '9.96'}, {'page': '18', 'x': '229.40', 'y': '118.35', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '241.82', 'y': '118.60', 'h': '186.58', 'w': '9.96'}, {'page': '18', 'x': '433.07', 'y': '118.35', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '444.39', 'y': '118.60', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '457.67', 'y': '118.35', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '475.03', 'y': '119.59', 'h': '28.97', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '128.74', 'h': '41.82', 'w': '12.48'}], [{'page': '18', 'x': '156.60', 'y': '131.26', 'h': '226.47', 'w': '9.96'}, {'page': '18', 'x': '388.27', 'y': '131.01', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '400.13', 'y': '131.26', 'h': '103.87', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '142.22', 'h': '65.24', 'w': '9.96'}, {'page': '18', 'x': '177.48', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '188.37', 'y': '142.22', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '213.25', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '224.15', 'y': '142.22', 'h': '50.63', 'w': '9.96'}, {'page': '18', 'x': '277.92', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '288.82', 'y': '142.22', 'h': '7.47', 'w': '9.96'}], [{'page': '18', 'x': '303.18', 'y': '143.21', 'h': '200.83', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '152.36', 'h': '59.88', 'w': '12.48'}, {'page': '18', 'x': '172.69', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '182.66', 'y': '154.88', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '206.59', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '216.56', 'y': '154.88', 'h': '63.42', 'w': '9.96'}, {'page': '18', 'x': '282.19', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '292.15', 'y': '154.88', 'h': '7.47', 'w': '9.96'}]]\", \"text\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1611.00712.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"3b42039c-697d-41c1-bc3c-c8534474c72f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"metadata\": {\"text\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"paper_title\": \"Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge\", \"section_title\": \"Overview of evaluated algorithms\", \"bboxes\": \"[[{'page': '4', 'x': '62.25', 'y': '483.25', 'h': '421.80', 'w': '9.14'}], [{'page': '4', 'x': '487.76', 'y': '483.25', 'h': '76.24', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '494.79', 'h': '176.57', 'w': '9.14'}], [{'page': '4', 'x': '228.04', 'y': '494.79', 'h': '335.96', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '506.33', 'h': '516.00', 'w': '9.14'}], [{'page': '4', 'x': '48.00', 'y': '517.87', 'h': '516.00', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '529.41', 'h': '291.38', 'w': '9.14'}], [{'page': '4', 'x': '342.79', 'y': '529.41', 'h': '221.22', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '540.95', 'h': '250.91', 'w': '9.14'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2308.05862.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"644f09e8-fa9c-4d98-84d7-0a6761c90cd5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0xf291a52346166c8d", "trace_id": "0xa264258efb1e4110323a416357ca32a2"}, "parent_id": "0x358a5ebf1b27844c", "start_time": 1724842088390794867, "end_time": 1724842089009239188, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c964f055233a4ac69894e591e00d0e45\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"metadata\": {\"text\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Comparison with Other RL Research\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '165.59', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '177.55', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '189.50', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '201.46', 'h': '35.54', 'w': '8.64'}], [{'page': '10', 'x': '351.78', 'y': '201.46', 'h': '211.26', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '213.41', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '225.37', 'h': '25.09', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9e430404-97b3-4346-b0c0-e2cdecb1b206\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"metadata\": {\"pages\": \"('11', '11')\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Results\", \"bboxes\": \"[[{'page': '11', 'x': '72.00', 'y': '319.27', 'h': '468.00', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '331.23', 'h': '86.63', 'w': '9.58'}], [{'page': '11', 'x': '163.27', 'y': '331.23', 'h': '376.74', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '343.18', 'h': '157.98', 'w': '9.58'}], [{'page': '11', 'x': '236.88', 'y': '343.18', 'h': '303.12', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '355.14', 'h': '40.86', 'w': '9.58'}], [{'page': '11', 'x': '115.95', 'y': '355.14', 'h': '357.39', 'w': '9.58'}]]\", \"text\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"5\", \"para\": \"3\", \"_id\": \"57ab9ca6-bf2c-466c-a038-7e355bc92112\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML\", \"section_title\": \"Experimental Settings\", \"bboxes\": \"[[{'page': '6', 'x': '320.82', 'y': '222.23', 'h': '97.24', 'w': '8.96'}], [{'page': '6', 'x': '424.80', 'y': '222.62', 'h': '120.31', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '234.58', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '246.53', 'h': '236.25', 'w': '8.64'}], [{'page': '6', 'x': '308.86', 'y': '258.49', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '270.44', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '282.08', 'h': '236.25', 'w': '8.96'}, {'page': '6', 'x': '308.86', 'y': '294.35', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '306.31', 'h': '217.95', 'w': '8.64'}], [{'page': '6', 'x': '531.43', 'y': '306.31', 'h': '13.69', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '318.27', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '330.22', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '342.18', 'h': '77.99', 'w': '8.64'}], [{'page': '6', 'x': '389.93', 'y': '342.18', 'h': '155.19', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '354.13', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '366.09', 'h': '110.83', 'w': '8.64'}], [{'page': '6', 'x': '423.22', 'y': '366.09', 'h': '121.89', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '378.04', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '390.00', 'h': '125.11', 'w': '8.64'}], [{'page': '6', 'x': '438.09', 'y': '390.00', 'h': '107.03', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '401.95', 'h': '182.68', 'w': '8.64'}]]\", \"text\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1903.08362.pdf\", \"section_number\": \"4.1.\", \"para\": \"6\", \"_id\": \"a9a4072a-074f-4a06-b613-8e50c185e8a3\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"metadata\": {\"pages\": \"('7', '8')\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"VISUAL SUDOKU CLASSIFICATION\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '659.91', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '670.65', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '681.39', 'h': '246.77', 'w': '8.64'}], [{'page': '7', 'x': '359.04', 'y': '681.39', 'h': '144.97', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '692.13', 'h': '396.00', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '85.34', 'h': '57.64', 'w': '8.64'}], [{'page': '8', 'x': '169.07', 'y': '85.34', 'h': '262.10', 'w': '8.64'}], [{'page': '8', 'x': '434.59', 'y': '85.34', 'h': '69.41', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '96.08', 'h': '251.01', 'w': '8.64'}], [{'page': '8', 'x': '364.07', 'y': '96.08', 'h': '139.93', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '106.82', 'h': '280.72', 'w': '8.64'}]]\", \"text\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"5.2\", \"para\": \"4\", \"_id\": \"f49e6c09-0ea0-4e96-bf8e-32d4de6eed60\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"metadata\": {\"text\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Rubik's Cube\", \"bboxes\": \"[[{'page': '6', 'x': '321.94', 'y': '510.57', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '522.52', 'h': '27.40', 'w': '8.64'}], [{'page': '6', 'x': '344.95', 'y': '522.52', 'h': '218.08', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '534.16', 'h': '251.06', 'w': '9.65'}, {'page': '6', 'x': '311.98', 'y': '546.11', 'h': '98.03', 'w': '9.65'}, {'page': '6', 'x': '410.01', 'y': '544.76', 'h': '3.49', 'w': '6.05'}, {'page': '6', 'x': '413.99', 'y': '546.43', 'h': '100.67', 'w': '8.64'}], [{'page': '6', 'x': '517.14', 'y': '546.43', 'h': '45.90', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '558.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '570.34', 'h': '234.33', 'w': '8.64'}], [{'page': '6', 'x': '550.00', 'y': '570.34', 'h': '13.03', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '582.30', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '593.93', 'h': '251.06', 'w': '8.96'}, {'page': '6', 'x': '311.98', 'y': '606.21', 'h': '59.69', 'w': '8.64'}], [{'page': '6', 'x': '375.72', 'y': '606.21', 'h': '187.32', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '618.16', 'h': '21.30', 'w': '8.64'}], [{'page': '6', 'x': '336.64', 'y': '617.98', 'h': '226.40', 'w': '8.82'}, {'page': '6', 'x': '311.98', 'y': '629.80', 'h': '135.98', 'w': '9.65'}], [{'page': '6', 'x': '450.83', 'y': '630.12', 'h': '112.21', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '642.07', 'h': '72.71', 'w': '8.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"0bd48af0-1080-4d25-9e89-d61d94b6d93d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Logic Guided Genetic Algorithms\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '329.46', 'y': '375.30', 'h': '228.54', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '386.26', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '397.22', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '408.18', 'h': '238.50', 'w': '8.64'}], [{'page': '1', 'x': '319.50', 'y': '419.14', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '430.10', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '441.06', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '452.02', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '462.98', 'h': '140.40', 'w': '8.64'}]]\", \"text\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2010.11328.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"e79804f7-d454-4216-a49e-f63822bf79ef\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"metadata\": {\"text\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Training Algorithms\", \"bboxes\": \"[[{'page': '9', 'x': '72.00', 'y': '515.54', 'h': '202.08', 'w': '9.58'}], [{'page': '9', 'x': '278.00', 'y': '515.54', 'h': '262.01', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '527.49', 'h': '82.48', 'w': '9.58'}], [{'page': '9', 'x': '158.51', 'y': '527.49', 'h': '269.13', 'w': '9.58'}], [{'page': '9', 'x': '431.65', 'y': '527.49', 'h': '108.35', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '539.45', 'h': '129.83', 'w': '9.58'}], [{'page': '9', 'x': '204.10', 'y': '539.45', 'h': '335.90', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '551.40', 'h': '102.56', 'w': '9.58'}], [{'page': '10', 'x': '215.61', 'y': '181.49', 'h': '248.47', 'w': '7.60'}], [{'page': '10', 'x': '469.07', 'y': '181.49', 'h': '70.93', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '190.96', 'h': '153.06', 'w': '7.60'}], [{'page': '10', 'x': '230.10', 'y': '190.96', 'h': '205.94', 'w': '7.60'}], [{'page': '10', 'x': '441.09', 'y': '190.96', 'h': '98.91', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '200.42', 'h': '206.11', 'w': '7.60'}], [{'page': '10', 'x': '281.40', 'y': '200.42', 'h': '258.60', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '209.88', 'h': '126.29', 'w': '7.60'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"3.3.1\", \"para\": \"9\", \"_id\": \"44847082-6d11-4b98-bc58-7b3cd6fb659a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"metadata\": {\"pages\": \"('17', '18')\", \"paper_title\": \"THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES\", \"section_title\": \"D.2 n-ARY LAYERS\", \"bboxes\": \"[[{'page': '17', 'x': '108.00', 'y': '710.06', 'h': '374.35', 'w': '9.96'}, {'page': '17', 'x': '482.48', 'y': '714.73', 'h': '3.97', 'w': '6.97'}, {'page': '17', 'x': '486.95', 'y': '710.06', 'h': '17.05', 'w': '9.96'}, {'page': '17', 'x': '108.00', 'y': '723.38', 'h': '215.44', 'w': '8.64'}, {'page': '17', 'x': '327.90', 'y': '722.14', 'h': '20.48', 'w': '17.29'}, {'page': '17', 'x': '350.04', 'y': '722.39', 'h': '9.96', 'w': '9.96'}, {'page': '17', 'x': '360.01', 'y': '721.02', 'h': '25.34', 'w': '8.05'}, {'page': '17', 'x': '385.85', 'y': '723.38', 'h': '2.49', 'w': '8.64'}], [{'page': '17', 'x': '397.34', 'y': '722.39', 'h': '106.67', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '85.34', 'h': '123.98', 'w': '8.64'}], [{'page': '18', 'x': '239.23', 'y': '84.35', 'h': '181.07', 'w': '9.96'}, {'page': '18', 'x': '425.91', 'y': '84.10', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '438.98', 'y': '84.35', 'h': '65.02', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '96.69', 'h': '6.37', 'w': '9.96'}, {'page': '18', 'x': '117.18', 'y': '96.44', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '126.59', 'y': '96.69', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '139.87', 'y': '96.44', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '153.71', 'y': '95.31', 'h': '4.92', 'w': '6.97'}, {'page': '18', 'x': '159.13', 'y': '97.67', 'h': '2.49', 'w': '8.64'}], [{'page': '18', 'x': '164.61', 'y': '96.69', 'h': '54.84', 'w': '9.96'}, {'page': '18', 'x': '219.59', 'y': '101.36', 'h': '3.97', 'w': '6.97'}, {'page': '18', 'x': '224.06', 'y': '96.69', 'h': '13.73', 'w': '9.96'}, {'page': '18', 'x': '238.95', 'y': '96.44', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '247.85', 'y': '96.69', 'h': '196.00', 'w': '9.96'}, {'page': '18', 'x': '446.06', 'y': '96.44', 'h': '20.48', 'w': '17.29'}, {'page': '18', 'x': '468.19', 'y': '96.69', 'h': '9.96', 'w': '9.96'}, {'page': '18', 'x': '478.16', 'y': '95.31', 'h': '25.35', 'w': '8.05'}, {'page': '18', 'x': '108.00', 'y': '107.64', 'h': '396.00', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '118.60', 'h': '115.94', 'w': '9.96'}, {'page': '18', 'x': '229.40', 'y': '118.35', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '241.82', 'y': '118.60', 'h': '186.58', 'w': '9.96'}, {'page': '18', 'x': '433.07', 'y': '118.35', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '444.39', 'y': '118.60', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '457.67', 'y': '118.35', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '475.03', 'y': '119.59', 'h': '28.97', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '128.74', 'h': '41.82', 'w': '12.48'}], [{'page': '18', 'x': '156.60', 'y': '131.26', 'h': '226.47', 'w': '9.96'}, {'page': '18', 'x': '388.27', 'y': '131.01', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '400.13', 'y': '131.26', 'h': '103.87', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '142.22', 'h': '65.24', 'w': '9.96'}, {'page': '18', 'x': '177.48', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '188.37', 'y': '142.22', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '213.25', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '224.15', 'y': '142.22', 'h': '50.63', 'w': '9.96'}, {'page': '18', 'x': '277.92', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '288.82', 'y': '142.22', 'h': '7.47', 'w': '9.96'}], [{'page': '18', 'x': '303.18', 'y': '143.21', 'h': '200.83', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '152.36', 'h': '59.88', 'w': '12.48'}, {'page': '18', 'x': '172.69', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '182.66', 'y': '154.88', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '206.59', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '216.56', 'y': '154.88', 'h': '63.42', 'w': '9.96'}, {'page': '18', 'x': '282.19', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '292.15', 'y': '154.88', 'h': '7.47', 'w': '9.96'}]]\", \"text\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1611.00712.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"3b42039c-697d-41c1-bc3c-c8534474c72f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"metadata\": {\"text\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"paper_title\": \"Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge\", \"section_title\": \"Overview of evaluated algorithms\", \"bboxes\": \"[[{'page': '4', 'x': '62.25', 'y': '483.25', 'h': '421.80', 'w': '9.14'}], [{'page': '4', 'x': '487.76', 'y': '483.25', 'h': '76.24', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '494.79', 'h': '176.57', 'w': '9.14'}], [{'page': '4', 'x': '228.04', 'y': '494.79', 'h': '335.96', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '506.33', 'h': '516.00', 'w': '9.14'}], [{'page': '4', 'x': '48.00', 'y': '517.87', 'h': '516.00', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '529.41', 'h': '291.38', 'w': '9.14'}], [{'page': '4', 'x': '342.79', 'y': '529.41', 'h': '221.22', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '540.95', 'h': '250.91', 'w': '9.14'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2308.05862.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"644f09e8-fa9c-4d98-84d7-0a6761c90cd5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\"}", "mlflow.spanOutputs": "{\"output_text\": \" The deep network was trained for 44 hours on a 32-core server with 3 GPUs.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0xec150817989214e6", "trace_id": "0xa264258efb1e4110323a416357ca32a2"}, "parent_id": "0xf291a52346166c8d", "start_time": 1724842088391787004, "end_time": 1724842089009059964, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c964f055233a4ac69894e591e00d0e45\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\", \"context\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\\n\\nWe split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\\n\\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\\n\\nBase network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\\n\\nWe next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\\n\\nWe investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\\n\\nRecently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\\n\\nAll three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\\n\\nAll our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\\n\\nWe analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\"}", "mlflow.spanOutputs": "{\"text\": \" The deep network was trained for 44 hours on a 32-core server with 3 GPUs.\"}"}, "events": [{"name": "text", "timestamp": 1724842088391953, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nConcerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\n\nWe split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\n\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\n\nBase network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\n\nWe next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\n\nWe investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\n\nRecently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\n\nAll three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\n\nAll our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \u223c Discrete(\u03b1) for \u03b1 \u2208 (0, \u221e) n .If C is the log 2 (n) \u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \u223c Concrete(\u03b1, \u03bb) for some fixed temperature \u03bb \u2208 (0, \u221e) and set \u1ef8 = CX.For the binary case, this amounts to simply sampling U \u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \u03b1) -1.The corresponding Binary Concrete relaxation is \u1ef8 = 2\u03c3((log Ulog(1 -U ) + log \u03b1)/\u03bb) -1.\n\nWe analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\n\nQuestion: What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0xc53a3b5ce799669a", "trace_id": "0xa264258efb1e4110323a416357ca32a2"}, "parent_id": "0xec150817989214e6", "start_time": 1724842088392307877, "end_time": 1724842089008814697, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"c964f055233a4ac69894e591e00d0e45\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nConcerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\\n\\nWe split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\\n\\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\\n\\nBase network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\\n\\nWe next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\\n\\nWe investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\\n\\nRecently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\\n\\nAll three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\\n\\nAll our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\\n\\nWe analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\\n\\nQuestion: What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" The deep network was trained for 44 hours on a 32-core server with 3 GPUs.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 1591, \"completion_tokens\": 20, \"total_tokens\": 1611}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?\"}", "response": "{\"result\": \" The deep network was trained for 44 hours on a 32-core server with 3 GPUs.\", \"source_documents\": [{\"page_content\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"metadata\": {\"text\": \"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Comparison with Other RL Research\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '165.59', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '177.55', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '189.50', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '201.46', 'h': '35.54', 'w': '8.64'}], [{'page': '10', 'x': '351.78', 'y': '201.46', 'h': '211.26', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '213.41', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '225.37', 'h': '25.09', 'w': '8.64'}]]\", \"pages\": \"('10', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"9e430404-97b3-4346-b0c0-e2cdecb1b206\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"metadata\": {\"pages\": \"('11', '11')\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Results\", \"bboxes\": \"[[{'page': '11', 'x': '72.00', 'y': '319.27', 'h': '468.00', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '331.23', 'h': '86.63', 'w': '9.58'}], [{'page': '11', 'x': '163.27', 'y': '331.23', 'h': '376.74', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '343.18', 'h': '157.98', 'w': '9.58'}], [{'page': '11', 'x': '236.88', 'y': '343.18', 'h': '303.12', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '355.14', 'h': '40.86', 'w': '9.58'}], [{'page': '11', 'x': '115.95', 'y': '355.14', 'h': '357.39', 'w': '9.58'}]]\", \"text\": \"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"5\", \"para\": \"3\", \"_id\": \"57ab9ca6-bf2c-466c-a038-7e355bc92112\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"metadata\": {\"pages\": \"('6', '6')\", \"paper_title\": \"Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML\", \"section_title\": \"Experimental Settings\", \"bboxes\": \"[[{'page': '6', 'x': '320.82', 'y': '222.23', 'h': '97.24', 'w': '8.96'}], [{'page': '6', 'x': '424.80', 'y': '222.62', 'h': '120.31', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '234.58', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '246.53', 'h': '236.25', 'w': '8.64'}], [{'page': '6', 'x': '308.86', 'y': '258.49', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '270.44', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '282.08', 'h': '236.25', 'w': '8.96'}, {'page': '6', 'x': '308.86', 'y': '294.35', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '306.31', 'h': '217.95', 'w': '8.64'}], [{'page': '6', 'x': '531.43', 'y': '306.31', 'h': '13.69', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '318.27', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '330.22', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '342.18', 'h': '77.99', 'w': '8.64'}], [{'page': '6', 'x': '389.93', 'y': '342.18', 'h': '155.19', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '354.13', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '366.09', 'h': '110.83', 'w': '8.64'}], [{'page': '6', 'x': '423.22', 'y': '366.09', 'h': '121.89', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '378.04', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '390.00', 'h': '125.11', 'w': '8.64'}], [{'page': '6', 'x': '438.09', 'y': '390.00', 'h': '107.03', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '401.95', 'h': '182.68', 'w': '8.64'}]]\", \"text\": \"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \\u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1903.08362.pdf\", \"section_number\": \"4.1.\", \"para\": \"6\", \"_id\": \"a9a4072a-074f-4a06-b613-8e50c185e8a3\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"metadata\": {\"pages\": \"('7', '8')\", \"paper_title\": \"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS\", \"section_title\": \"VISUAL SUDOKU CLASSIFICATION\", \"bboxes\": \"[[{'page': '7', 'x': '108.00', 'y': '659.91', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '670.65', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '681.39', 'h': '246.77', 'w': '8.64'}], [{'page': '7', 'x': '359.04', 'y': '681.39', 'h': '144.97', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '692.13', 'h': '396.00', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '85.34', 'h': '57.64', 'w': '8.64'}], [{'page': '8', 'x': '169.07', 'y': '85.34', 'h': '262.10', 'w': '8.64'}], [{'page': '8', 'x': '434.59', 'y': '85.34', 'h': '69.41', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '96.08', 'h': '251.01', 'w': '8.64'}], [{'page': '8', 'x': '364.07', 'y': '96.08', 'h': '139.93', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '106.82', 'h': '280.72', 'w': '8.64'}]]\", \"text\": \"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2403.00323.pdf\", \"section_number\": \"5.2\", \"para\": \"4\", \"_id\": \"f49e6c09-0ea0-4e96-bf8e-32d4de6eed60\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"metadata\": {\"text\": \"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"C. Rubik's Cube\", \"bboxes\": \"[[{'page': '6', 'x': '321.94', 'y': '510.57', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '522.52', 'h': '27.40', 'w': '8.64'}], [{'page': '6', 'x': '344.95', 'y': '522.52', 'h': '218.08', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '534.16', 'h': '251.06', 'w': '9.65'}, {'page': '6', 'x': '311.98', 'y': '546.11', 'h': '98.03', 'w': '9.65'}, {'page': '6', 'x': '410.01', 'y': '544.76', 'h': '3.49', 'w': '6.05'}, {'page': '6', 'x': '413.99', 'y': '546.43', 'h': '100.67', 'w': '8.64'}], [{'page': '6', 'x': '517.14', 'y': '546.43', 'h': '45.90', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '558.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '570.34', 'h': '234.33', 'w': '8.64'}], [{'page': '6', 'x': '550.00', 'y': '570.34', 'h': '13.03', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '582.30', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '593.93', 'h': '251.06', 'w': '8.96'}, {'page': '6', 'x': '311.98', 'y': '606.21', 'h': '59.69', 'w': '8.64'}], [{'page': '6', 'x': '375.72', 'y': '606.21', 'h': '187.32', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '618.16', 'h': '21.30', 'w': '8.64'}], [{'page': '6', 'x': '336.64', 'y': '617.98', 'h': '226.40', 'w': '8.82'}, {'page': '6', 'x': '311.98', 'y': '629.80', 'h': '135.98', 'w': '9.65'}], [{'page': '6', 'x': '450.83', 'y': '630.12', 'h': '112.21', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '642.07', 'h': '72.71', 'w': '8.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"6\", \"_id\": \"0bd48af0-1080-4d25-9e89-d61d94b6d93d\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"metadata\": {\"pages\": \"('1', '1')\", \"paper_title\": \"Logic Guided Genetic Algorithms\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '1', 'x': '329.46', 'y': '375.30', 'h': '228.54', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '386.26', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '397.22', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '408.18', 'h': '238.50', 'w': '8.64'}], [{'page': '1', 'x': '319.50', 'y': '419.14', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '430.10', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '441.06', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '452.02', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '462.98', 'h': '140.40', 'w': '8.64'}]]\", \"text\": \"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2010.11328.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"e79804f7-d454-4216-a49e-f63822bf79ef\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"metadata\": {\"text\": \"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.\", \"paper_title\": \"Algorithmically-consistent deep learning frameworks for structural topology optimization\", \"section_title\": \"Training Algorithms\", \"bboxes\": \"[[{'page': '9', 'x': '72.00', 'y': '515.54', 'h': '202.08', 'w': '9.58'}], [{'page': '9', 'x': '278.00', 'y': '515.54', 'h': '262.01', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '527.49', 'h': '82.48', 'w': '9.58'}], [{'page': '9', 'x': '158.51', 'y': '527.49', 'h': '269.13', 'w': '9.58'}], [{'page': '9', 'x': '431.65', 'y': '527.49', 'h': '108.35', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '539.45', 'h': '129.83', 'w': '9.58'}], [{'page': '9', 'x': '204.10', 'y': '539.45', 'h': '335.90', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '551.40', 'h': '102.56', 'w': '9.58'}], [{'page': '10', 'x': '215.61', 'y': '181.49', 'h': '248.47', 'w': '7.60'}], [{'page': '10', 'x': '469.07', 'y': '181.49', 'h': '70.93', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '190.96', 'h': '153.06', 'w': '7.60'}], [{'page': '10', 'x': '230.10', 'y': '190.96', 'h': '205.94', 'w': '7.60'}], [{'page': '10', 'x': '441.09', 'y': '190.96', 'h': '98.91', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '200.42', 'h': '206.11', 'w': '7.60'}], [{'page': '10', 'x': '281.40', 'y': '200.42', 'h': '258.60', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '209.88', 'h': '126.29', 'w': '7.60'}]]\", \"pages\": \"('9', '10')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2012.05359.pdf\", \"section_number\": \"3.3.1\", \"para\": \"9\", \"_id\": \"44847082-6d11-4b98-bc58-7b3cd6fb659a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"metadata\": {\"pages\": \"('17', '18')\", \"paper_title\": \"THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES\", \"section_title\": \"D.2 n-ARY LAYERS\", \"bboxes\": \"[[{'page': '17', 'x': '108.00', 'y': '710.06', 'h': '374.35', 'w': '9.96'}, {'page': '17', 'x': '482.48', 'y': '714.73', 'h': '3.97', 'w': '6.97'}, {'page': '17', 'x': '486.95', 'y': '710.06', 'h': '17.05', 'w': '9.96'}, {'page': '17', 'x': '108.00', 'y': '723.38', 'h': '215.44', 'w': '8.64'}, {'page': '17', 'x': '327.90', 'y': '722.14', 'h': '20.48', 'w': '17.29'}, {'page': '17', 'x': '350.04', 'y': '722.39', 'h': '9.96', 'w': '9.96'}, {'page': '17', 'x': '360.01', 'y': '721.02', 'h': '25.34', 'w': '8.05'}, {'page': '17', 'x': '385.85', 'y': '723.38', 'h': '2.49', 'w': '8.64'}], [{'page': '17', 'x': '397.34', 'y': '722.39', 'h': '106.67', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '85.34', 'h': '123.98', 'w': '8.64'}], [{'page': '18', 'x': '239.23', 'y': '84.35', 'h': '181.07', 'w': '9.96'}, {'page': '18', 'x': '425.91', 'y': '84.10', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '438.98', 'y': '84.35', 'h': '65.02', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '96.69', 'h': '6.37', 'w': '9.96'}, {'page': '18', 'x': '117.18', 'y': '96.44', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '126.59', 'y': '96.69', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '139.87', 'y': '96.44', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '153.71', 'y': '95.31', 'h': '4.92', 'w': '6.97'}, {'page': '18', 'x': '159.13', 'y': '97.67', 'h': '2.49', 'w': '8.64'}], [{'page': '18', 'x': '164.61', 'y': '96.69', 'h': '54.84', 'w': '9.96'}, {'page': '18', 'x': '219.59', 'y': '101.36', 'h': '3.97', 'w': '6.97'}, {'page': '18', 'x': '224.06', 'y': '96.69', 'h': '13.73', 'w': '9.96'}, {'page': '18', 'x': '238.95', 'y': '96.44', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '247.85', 'y': '96.69', 'h': '196.00', 'w': '9.96'}, {'page': '18', 'x': '446.06', 'y': '96.44', 'h': '20.48', 'w': '17.29'}, {'page': '18', 'x': '468.19', 'y': '96.69', 'h': '9.96', 'w': '9.96'}, {'page': '18', 'x': '478.16', 'y': '95.31', 'h': '25.35', 'w': '8.05'}, {'page': '18', 'x': '108.00', 'y': '107.64', 'h': '396.00', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '118.60', 'h': '115.94', 'w': '9.96'}, {'page': '18', 'x': '229.40', 'y': '118.35', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '241.82', 'y': '118.60', 'h': '186.58', 'w': '9.96'}, {'page': '18', 'x': '433.07', 'y': '118.35', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '444.39', 'y': '118.60', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '457.67', 'y': '118.35', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '475.03', 'y': '119.59', 'h': '28.97', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '128.74', 'h': '41.82', 'w': '12.48'}], [{'page': '18', 'x': '156.60', 'y': '131.26', 'h': '226.47', 'w': '9.96'}, {'page': '18', 'x': '388.27', 'y': '131.01', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '400.13', 'y': '131.26', 'h': '103.87', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '142.22', 'h': '65.24', 'w': '9.96'}, {'page': '18', 'x': '177.48', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '188.37', 'y': '142.22', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '213.25', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '224.15', 'y': '142.22', 'h': '50.63', 'w': '9.96'}, {'page': '18', 'x': '277.92', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '288.82', 'y': '142.22', 'h': '7.47', 'w': '9.96'}], [{'page': '18', 'x': '303.18', 'y': '143.21', 'h': '200.83', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '152.36', 'h': '59.88', 'w': '12.48'}, {'page': '18', 'x': '172.69', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '182.66', 'y': '154.88', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '206.59', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '216.56', 'y': '154.88', 'h': '63.42', 'w': '9.96'}, {'page': '18', 'x': '282.19', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '292.15', 'y': '154.88', 'h': '7.47', 'w': '9.96'}]]\", \"text\": \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \\u223c Discrete(\\u03b1) for \\u03b1 \\u2208 (0, \\u221e) n .If C is the log 2 (n) \\u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \\u223c Concrete(\\u03b1, \\u03bb) for some fixed temperature \\u03bb \\u2208 (0, \\u221e) and set \\u1ef8 = CX.For the binary case, this amounts to simply sampling U \\u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \\u03b1) -1.The corresponding Binary Concrete relaxation is \\u1ef8 = 2\\u03c3((log Ulog(1 -U ) + log \\u03b1)/\\u03bb) -1.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1611.00712.pdf\", \"section_number\": \"None\", \"para\": \"5\", \"_id\": \"3b42039c-697d-41c1-bc3c-c8534474c72f\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"metadata\": {\"text\": \"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).\", \"paper_title\": \"Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge\", \"section_title\": \"Overview of evaluated algorithms\", \"bboxes\": \"[[{'page': '4', 'x': '62.25', 'y': '483.25', 'h': '421.80', 'w': '9.14'}], [{'page': '4', 'x': '487.76', 'y': '483.25', 'h': '76.24', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '494.79', 'h': '176.57', 'w': '9.14'}], [{'page': '4', 'x': '228.04', 'y': '494.79', 'h': '335.96', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '506.33', 'h': '516.00', 'w': '9.14'}], [{'page': '4', 'x': '48.00', 'y': '517.87', 'h': '516.00', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '529.41', 'h': '291.38', 'w': '9.14'}], [{'page': '4', 'x': '342.79', 'y': '529.41', 'h': '221.22', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '540.95', 'h': '250.91', 'w': '9.14'}]]\", \"pages\": \"('4', '4')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2308.05862.pdf\", \"section_number\": \"None\", \"para\": \"4\", \"_id\": \"644f09e8-fa9c-4d98-84d7-0a6761c90cd5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}