{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x74bf194e8a332018", "trace_id": "0x758a9c7f769de65bc55419e3a3fd7e8e"}, "parent_id": null, "start_time": 1724842084477347531, "end_time": 1724842085059246634, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"72ebe3bf2ace40a7948b6b5bba70b232\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\"}", "mlflow.spanOutputs": "{\"result\": \" The Expectimax approach.\", \"source_documents\": [{\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '625.25', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '637.21', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '649.16', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '661.12', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '673.07', 'h': '39.00', 'w': '8.64'}]]\", \"text\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"a0db5e7e-0642-4a7a-86ef-53633a90b914\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. Computation times\", \"bboxes\": \"[[{'page': '7', 'x': '321.94', 'y': '500.24', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '512.20', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '523.97', 'h': '251.06', 'w': '8.82'}, {'page': '7', 'x': '311.98', 'y': '536.11', 'h': '33.48', 'w': '8.64'}]]\", \"text\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"0443fa92-bb62-4e30-b65b-7cfa7a538da1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '149.76', 'y': '204.69', 'h': '332.48', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '216.57', 'h': '345.92', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '228.57', 'h': '235.32', 'w': '9.96'}], [{'page': '2', 'x': '373.56', 'y': '228.57', 'h': '107.05', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '240.45', 'h': '347.59', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '252.45', 'h': '345.80', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '264.45', 'h': '105.00', 'w': '9.96'}], [{'page': '2', 'x': '243.48', 'y': '264.45', 'h': '238.76', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '276.33', 'h': '346.01', 'w': '9.96'}, {'page': '2', 'x': '134.40', 'y': '288.33', 'h': '188.52', 'w': '9.96'}], [{'page': '2', 'x': '325.56', 'y': '288.33', 'h': '156.68', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '300.33', 'h': '230.04', 'w': '9.96'}]]\", \"text\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"1\", \"para\": \"3\", \"_id\": \"990632c9-79dc-413c-8bed-752d72a7dba1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"metadata\": {\"text\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"paper_title\": \"Cautious Belief and Iterated Admissibility *\", \"section_title\": \"Our contribution\", \"bboxes\": \"[[{'page': '6', 'x': '87.84', 'y': '482.60', 'h': '436.55', 'w': '10.91'}, {'page': '6', 'x': '70.92', 'y': '496.16', 'h': '188.54', 'w': '10.91'}, {'page': '6', 'x': '259.44', 'y': '494.40', 'h': '4.23', 'w': '7.97'}], [{'page': '6', 'x': '70.92', 'y': '528.62', 'h': '8.06', 'w': '13.64'}, {'page': '6', 'x': '95.04', 'y': '528.62', 'h': '388.79', 'w': '13.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.15330.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"6529f59b-311b-4064-bf50-48de3de15a90\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"metadata\": {\"text\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"paper_title\": \"\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '122.37', 'h': '453.08', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '134.25', 'h': '240.36', 'w': '9.96'}], [{'page': '2', 'x': '316.32', 'y': '134.25', 'h': '223.69', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '146.25', 'h': '467.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '158.13', 'h': '261.48', 'w': '9.96'}], [{'page': '2', 'x': '337.68', 'y': '158.13', 'h': '202.38', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '170.13', 'h': '58.56', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/cs_0309016.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"ad5a36e0-435d-4b5e-bf46-cde788468364\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Main Contributions\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '220.50', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '234.05', 'h': '198.88', 'w': '9.46'}], [{'page': '2', 'x': '274.24', 'y': '234.05', 'h': '265.76', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '247.60', 'h': '328.30', 'w': '9.46'}]]\", \"text\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1.1\", \"para\": \"1\", \"_id\": \"c1d2468e-cb72-4550-abca-dde62b2353f0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"metadata\": {\"text\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"paper_title\": \"Learning Regularized Monotone Graphon Mean-Field Games\", \"section_title\": \"Existence of the NEs in Regularized GMFGs and MFGs\", \"bboxes\": \"[[{'page': '6', 'x': '59.53', 'y': '627.90', 'h': '70.71', 'w': '10.52'}], [{'page': '6', 'x': '136.09', 'y': '627.94', 'h': '399.66', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '642.38', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '656.83', 'h': '30.50', 'w': '10.48'}], [{'page': '6', 'x': '93.93', 'y': '656.83', 'h': '441.83', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '671.27', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '685.72', 'h': '55.88', 'w': '10.48'}], [{'page': '6', 'x': '119.30', 'y': '685.72', 'h': '416.44', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '700.16', 'h': '389.94', 'w': '10.48'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.08089.pdf\", \"section_number\": \"4\", \"para\": \"3\", \"_id\": \"e24ca2a0-bc0f-4fae-83b3-01e9b700f2d5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0x49c69eff68316146", "trace_id": "0x758a9c7f769de65bc55419e3a3fd7e8e"}, "parent_id": "0x74bf194e8a332018", "start_time": 1724842084482298619, "end_time": 1724842084519351249, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"72ebe3bf2ace40a7948b6b5bba70b232\"", "mlflow.spanType": "\"RETRIEVER\"", "mlflow.spanInputs": "\"What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\"", "mlflow.spanOutputs": "[{\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '625.25', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '637.21', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '649.16', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '661.12', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '673.07', 'h': '39.00', 'w': '8.64'}]]\", \"text\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"a0db5e7e-0642-4a7a-86ef-53633a90b914\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. Computation times\", \"bboxes\": \"[[{'page': '7', 'x': '321.94', 'y': '500.24', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '512.20', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '523.97', 'h': '251.06', 'w': '8.82'}, {'page': '7', 'x': '311.98', 'y': '536.11', 'h': '33.48', 'w': '8.64'}]]\", \"text\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"0443fa92-bb62-4e30-b65b-7cfa7a538da1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '149.76', 'y': '204.69', 'h': '332.48', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '216.57', 'h': '345.92', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '228.57', 'h': '235.32', 'w': '9.96'}], [{'page': '2', 'x': '373.56', 'y': '228.57', 'h': '107.05', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '240.45', 'h': '347.59', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '252.45', 'h': '345.80', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '264.45', 'h': '105.00', 'w': '9.96'}], [{'page': '2', 'x': '243.48', 'y': '264.45', 'h': '238.76', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '276.33', 'h': '346.01', 'w': '9.96'}, {'page': '2', 'x': '134.40', 'y': '288.33', 'h': '188.52', 'w': '9.96'}], [{'page': '2', 'x': '325.56', 'y': '288.33', 'h': '156.68', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '300.33', 'h': '230.04', 'w': '9.96'}]]\", \"text\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"1\", \"para\": \"3\", \"_id\": \"990632c9-79dc-413c-8bed-752d72a7dba1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"metadata\": {\"text\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"paper_title\": \"Cautious Belief and Iterated Admissibility *\", \"section_title\": \"Our contribution\", \"bboxes\": \"[[{'page': '6', 'x': '87.84', 'y': '482.60', 'h': '436.55', 'w': '10.91'}, {'page': '6', 'x': '70.92', 'y': '496.16', 'h': '188.54', 'w': '10.91'}, {'page': '6', 'x': '259.44', 'y': '494.40', 'h': '4.23', 'w': '7.97'}], [{'page': '6', 'x': '70.92', 'y': '528.62', 'h': '8.06', 'w': '13.64'}, {'page': '6', 'x': '95.04', 'y': '528.62', 'h': '388.79', 'w': '13.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.15330.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"6529f59b-311b-4064-bf50-48de3de15a90\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"metadata\": {\"text\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"paper_title\": \"\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '122.37', 'h': '453.08', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '134.25', 'h': '240.36', 'w': '9.96'}], [{'page': '2', 'x': '316.32', 'y': '134.25', 'h': '223.69', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '146.25', 'h': '467.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '158.13', 'h': '261.48', 'w': '9.96'}], [{'page': '2', 'x': '337.68', 'y': '158.13', 'h': '202.38', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '170.13', 'h': '58.56', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/cs_0309016.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"ad5a36e0-435d-4b5e-bf46-cde788468364\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Main Contributions\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '220.50', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '234.05', 'h': '198.88', 'w': '9.46'}], [{'page': '2', 'x': '274.24', 'y': '234.05', 'h': '265.76', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '247.60', 'h': '328.30', 'w': '9.46'}]]\", \"text\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1.1\", \"para\": \"1\", \"_id\": \"c1d2468e-cb72-4550-abca-dde62b2353f0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"metadata\": {\"text\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"paper_title\": \"Learning Regularized Monotone Graphon Mean-Field Games\", \"section_title\": \"Existence of the NEs in Regularized GMFGs and MFGs\", \"bboxes\": \"[[{'page': '6', 'x': '59.53', 'y': '627.90', 'h': '70.71', 'w': '10.52'}], [{'page': '6', 'x': '136.09', 'y': '627.94', 'h': '399.66', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '642.38', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '656.83', 'h': '30.50', 'w': '10.48'}], [{'page': '6', 'x': '93.93', 'y': '656.83', 'h': '441.83', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '671.27', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '685.72', 'h': '55.88', 'w': '10.48'}], [{'page': '6', 'x': '119.30', 'y': '685.72', 'h': '416.44', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '700.16', 'h': '389.94', 'w': '10.48'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.08089.pdf\", \"section_number\": \"4\", \"para\": \"3\", \"_id\": \"e24ca2a0-bc0f-4fae-83b3-01e9b700f2d5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0x0e85b095b2c7c32c", "trace_id": "0x758a9c7f769de65bc55419e3a3fd7e8e"}, "parent_id": "0x74bf194e8a332018", "start_time": 1724842084520187667, "end_time": 1724842085058139889, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"72ebe3bf2ace40a7948b6b5bba70b232\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '625.25', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '637.21', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '649.16', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '661.12', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '673.07', 'h': '39.00', 'w': '8.64'}]]\", \"text\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"a0db5e7e-0642-4a7a-86ef-53633a90b914\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. Computation times\", \"bboxes\": \"[[{'page': '7', 'x': '321.94', 'y': '500.24', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '512.20', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '523.97', 'h': '251.06', 'w': '8.82'}, {'page': '7', 'x': '311.98', 'y': '536.11', 'h': '33.48', 'w': '8.64'}]]\", \"text\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"0443fa92-bb62-4e30-b65b-7cfa7a538da1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '149.76', 'y': '204.69', 'h': '332.48', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '216.57', 'h': '345.92', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '228.57', 'h': '235.32', 'w': '9.96'}], [{'page': '2', 'x': '373.56', 'y': '228.57', 'h': '107.05', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '240.45', 'h': '347.59', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '252.45', 'h': '345.80', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '264.45', 'h': '105.00', 'w': '9.96'}], [{'page': '2', 'x': '243.48', 'y': '264.45', 'h': '238.76', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '276.33', 'h': '346.01', 'w': '9.96'}, {'page': '2', 'x': '134.40', 'y': '288.33', 'h': '188.52', 'w': '9.96'}], [{'page': '2', 'x': '325.56', 'y': '288.33', 'h': '156.68', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '300.33', 'h': '230.04', 'w': '9.96'}]]\", \"text\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"1\", \"para\": \"3\", \"_id\": \"990632c9-79dc-413c-8bed-752d72a7dba1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"metadata\": {\"text\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"paper_title\": \"Cautious Belief and Iterated Admissibility *\", \"section_title\": \"Our contribution\", \"bboxes\": \"[[{'page': '6', 'x': '87.84', 'y': '482.60', 'h': '436.55', 'w': '10.91'}, {'page': '6', 'x': '70.92', 'y': '496.16', 'h': '188.54', 'w': '10.91'}, {'page': '6', 'x': '259.44', 'y': '494.40', 'h': '4.23', 'w': '7.97'}], [{'page': '6', 'x': '70.92', 'y': '528.62', 'h': '8.06', 'w': '13.64'}, {'page': '6', 'x': '95.04', 'y': '528.62', 'h': '388.79', 'w': '13.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.15330.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"6529f59b-311b-4064-bf50-48de3de15a90\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"metadata\": {\"text\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"paper_title\": \"\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '122.37', 'h': '453.08', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '134.25', 'h': '240.36', 'w': '9.96'}], [{'page': '2', 'x': '316.32', 'y': '134.25', 'h': '223.69', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '146.25', 'h': '467.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '158.13', 'h': '261.48', 'w': '9.96'}], [{'page': '2', 'x': '337.68', 'y': '158.13', 'h': '202.38', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '170.13', 'h': '58.56', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/cs_0309016.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"ad5a36e0-435d-4b5e-bf46-cde788468364\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Main Contributions\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '220.50', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '234.05', 'h': '198.88', 'w': '9.46'}], [{'page': '2', 'x': '274.24', 'y': '234.05', 'h': '265.76', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '247.60', 'h': '328.30', 'w': '9.46'}]]\", \"text\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1.1\", \"para\": \"1\", \"_id\": \"c1d2468e-cb72-4550-abca-dde62b2353f0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"metadata\": {\"text\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"paper_title\": \"Learning Regularized Monotone Graphon Mean-Field Games\", \"section_title\": \"Existence of the NEs in Regularized GMFGs and MFGs\", \"bboxes\": \"[[{'page': '6', 'x': '59.53', 'y': '627.90', 'h': '70.71', 'w': '10.52'}], [{'page': '6', 'x': '136.09', 'y': '627.94', 'h': '399.66', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '642.38', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '656.83', 'h': '30.50', 'w': '10.48'}], [{'page': '6', 'x': '93.93', 'y': '656.83', 'h': '441.83', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '671.27', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '685.72', 'h': '55.88', 'w': '10.48'}], [{'page': '6', 'x': '119.30', 'y': '685.72', 'h': '416.44', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '700.16', 'h': '389.94', 'w': '10.48'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.08089.pdf\", \"section_number\": \"4\", \"para\": \"3\", \"_id\": \"e24ca2a0-bc0f-4fae-83b3-01e9b700f2d5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}], \"question\": \"What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\"}", "mlflow.spanOutputs": "{\"output_text\": \" The Expectimax approach.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x35aaa9c6a5e62db8", "trace_id": "0x758a9c7f769de65bc55419e3a3fd7e8e"}, "parent_id": "0x0e85b095b2c7c32c", "start_time": 1724842084521071973, "end_time": 1724842085057915760, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"72ebe3bf2ace40a7948b6b5bba70b232\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\", \"context\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\\n\\nWe also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\\n\\nThe MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\\n\\nMost classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\\n\\nA comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\\n\\nHowever, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\\n\\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\\n\\nWe address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\\n\\nRemark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\\n\\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\"}", "mlflow.spanOutputs": "{\"text\": \" The Expectimax approach.\"}"}, "events": [{"name": "text", "timestamp": 1724842084521158, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nOur implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.\n\nWe also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\n\nThe MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\n\nMost classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\n\nA comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\n\nHowever, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\n\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\n\nWe address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\n\nRemark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\n\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\n\nQuestion: What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x3c0486c8fccbd939", "trace_id": "0x758a9c7f769de65bc55419e3a3fd7e8e"}, "parent_id": "0x35aaa9c6a5e62db8", "start_time": 1724842084521472761, "end_time": 1724842085057649101, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"72ebe3bf2ace40a7948b6b5bba70b232\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nOur implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\\n\\nWe also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\\n\\nThe MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\\n\\nMost classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\\n\\nA comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\\n\\nHowever, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\\n\\nThere are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\\n\\nWe address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\\n\\nRemark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\\n\\nThe main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\\n\\nQuestion: What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" The Expectimax approach.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"prompt_tokens\": 886, \"completion_tokens\": 5, \"total_tokens\": 891}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?\"}", "response": "{\"result\": \" The Expectimax approach.\", \"source_documents\": [{\"page_content\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"metadata\": {\"text\": \"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\\u03ba = -1) in the case of 2-player games.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. MCTS Wrapper\", \"bboxes\": \"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"3\", \"_id\": \"778dc672-a89f-4a58-a142-ba642132da6c\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"metadata\": {\"pages\": \"('10', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"VI. CONCLUSION AND FUTURE WORK\", \"bboxes\": \"[[{'page': '10', 'x': '321.94', 'y': '625.25', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '637.21', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '649.16', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '661.12', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '673.07', 'h': '39.00', 'w': '8.64'}]]\", \"text\": \"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"a0db5e7e-0642-4a7a-86ef-53633a90b914\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"metadata\": {\"pages\": \"('7', '7')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"D. Computation times\", \"bboxes\": \"[[{'page': '7', 'x': '321.94', 'y': '500.24', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '512.20', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '523.97', 'h': '251.06', 'w': '8.82'}, {'page': '7', 'x': '311.98', 'y': '536.11', 'h': '33.48', 'w': '8.64'}]]\", \"text\": \"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"0\", \"_id\": \"0443fa92-bb62-4e30-b65b-7cfa7a538da1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Qualitative Analysis of VASS-Induced MDPs \\u22c6\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '149.76', 'y': '204.69', 'h': '332.48', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '216.57', 'h': '345.92', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '228.57', 'h': '235.32', 'w': '9.96'}], [{'page': '2', 'x': '373.56', 'y': '228.57', 'h': '107.05', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '240.45', 'h': '347.59', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '252.45', 'h': '345.80', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '264.45', 'h': '105.00', 'w': '9.96'}], [{'page': '2', 'x': '243.48', 'y': '264.45', 'h': '238.76', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '276.33', 'h': '346.01', 'w': '9.96'}, {'page': '2', 'x': '134.40', 'y': '288.33', 'h': '188.52', 'w': '9.96'}], [{'page': '2', 'x': '325.56', 'y': '288.33', 'h': '156.68', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '300.33', 'h': '230.04', 'w': '9.96'}]]\", \"text\": \"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/1512.08824.pdf\", \"section_number\": \"1\", \"para\": \"3\", \"_id\": \"990632c9-79dc-413c-8bed-752d72a7dba1\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"metadata\": {\"text\": \"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures\", \"paper_title\": \"Cautious Belief and Iterated Admissibility *\", \"section_title\": \"Our contribution\", \"bboxes\": \"[[{'page': '6', 'x': '87.84', 'y': '482.60', 'h': '436.55', 'w': '10.91'}, {'page': '6', 'x': '70.92', 'y': '496.16', 'h': '188.54', 'w': '10.91'}, {'page': '6', 'x': '259.44', 'y': '494.40', 'h': '4.23', 'w': '7.97'}], [{'page': '6', 'x': '70.92', 'y': '528.62', 'h': '8.06', 'w': '13.64'}, {'page': '6', 'x': '95.04', 'y': '528.62', 'h': '388.79', 'w': '13.64'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2305.15330.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"6529f59b-311b-4064-bf50-48de3de15a90\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"metadata\": {\"text\": \"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.\", \"paper_title\": \"\", \"section_title\": \"Introduction\", \"bboxes\": \"[[{'page': '2', 'x': '87.00', 'y': '122.37', 'h': '453.08', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '134.25', 'h': '240.36', 'w': '9.96'}], [{'page': '2', 'x': '316.32', 'y': '134.25', 'h': '223.69', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '146.25', 'h': '467.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '158.13', 'h': '261.48', 'w': '9.96'}], [{'page': '2', 'x': '337.68', 'y': '158.13', 'h': '202.38', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '170.13', 'h': '58.56', 'w': '9.96'}]]\", \"pages\": \"('2', '2')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/cs_0309016.pdf\", \"section_number\": \"1\", \"para\": \"2\", \"_id\": \"ad5a36e0-435d-4b5e-bf46-cde788468364\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"metadata\": {\"pages\": \"('9', '10')\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"B. Related Work in N-Tuple Research\", \"bboxes\": \"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]\", \"text\": \"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"1f958ad8-f8c9-4c82-bc74-8755781e702a\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"metadata\": {\"pages\": \"('2', '2')\", \"paper_title\": \"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games\", \"section_title\": \"Main Contributions\", \"bboxes\": \"[[{'page': '2', 'x': '72.00', 'y': '220.50', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '234.05', 'h': '198.88', 'w': '9.46'}], [{'page': '2', 'x': '274.24', 'y': '234.05', 'h': '265.76', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '247.60', 'h': '328.30', 'w': '9.46'}]]\", \"text\": \"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2303.04865.pdf\", \"section_number\": \"1.1\", \"para\": \"1\", \"_id\": \"c1d2468e-cb72-4550-abca-dde62b2353f0\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"metadata\": {\"text\": \"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.\", \"paper_title\": \"Learning Regularized Monotone Graphon Mean-Field Games\", \"section_title\": \"Existence of the NEs in Regularized GMFGs and MFGs\", \"bboxes\": \"[[{'page': '6', 'x': '59.53', 'y': '627.90', 'h': '70.71', 'w': '10.52'}], [{'page': '6', 'x': '136.09', 'y': '627.94', 'h': '399.66', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '642.38', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '656.83', 'h': '30.50', 'w': '10.48'}], [{'page': '6', 'x': '93.93', 'y': '656.83', 'h': '441.83', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '671.27', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '685.72', 'h': '55.88', 'w': '10.48'}], [{'page': '6', 'x': '119.30', 'y': '685.72', 'h': '416.44', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '700.16', 'h': '389.94', 'w': '10.48'}]]\", \"pages\": \"('6', '6')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2310.08089.pdf\", \"section_number\": \"4\", \"para\": \"3\", \"_id\": \"e24ca2a0-bc0f-4fae-83b3-01e9b700f2d5\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}, {\"page_content\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"metadata\": {\"text\": \"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.\", \"paper_title\": \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time\", \"section_title\": \"A. Motivation\", \"bboxes\": \"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]\", \"pages\": \"('1', '1')\", \"file_path\": \"/home/dakoro/Github/Projet_Simplon/pdfs/2204.13307.pdf\", \"section_number\": \"None\", \"para\": \"1\", \"_id\": \"2fddfe2e-5f6e-4594-b966-24d92c3d82c7\", \"_collection_name\": \"Papers\"}, \"type\": \"Document\"}]}"}