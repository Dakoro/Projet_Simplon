{"columns":["question","outputs","source_documents","latency","token_count","flesch_kincaid_grade_level\/v1\/score","ari_grade_level\/v1\/score","relevance\/v1\/score","relevance\/v1\/justification"],"data":[["What are the properties and characteristics of functions in the class F \u03b1 that are analytic and univalent in D?"," The functions in the class F \u03b1 are nonvanishing, analytic, and univalent in the unit disk D. They also satisfy the normalization conditions f(1) = \u221e and f(0) = 1, and the complement of their image in the complex plane is a convex set. Additionally, the angle at infinity for these functions is less than or equal to \u03c0\u03b1, where \u03b1 is a parameter in the range (1,2].",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '2')","paper_title":"CONCAVE UNIVALENT FUNCTIONS AND DIRICHLET FINITE INTEGRAL","section_title":"Introduction","bboxes":"[[{'page': '1', 'x': '84.00', 'y': '641.65', 'h': '291.77', 'w': '11.97'}, {'page': '1', 'x': '381.96', 'y': '641.65', 'h': '8.60', 'w': '20.44'}, {'page': '1', 'x': '390.60', 'y': '646.44', 'h': '5.44', 'w': '7.97'}, {'page': '1', 'x': '402.60', 'y': '641.65', 'h': '94.62', 'w': '11.97'}, {'page': '1', 'x': '72.00', 'y': '655.57', 'h': '424.90', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '118.21', 'h': '93.22', 'w': '11.97'}, {'page': '2', 'x': '165.24', 'y': '116.88', 'h': '3.67', 'w': '7.97'}, {'page': '2', 'x': '173.52', 'y': '118.21', 'h': '117.34', 'w': '11.97'}, {'page': '2', 'x': '294.60', 'y': '118.21', 'h': '15.24', 'w': '20.44'}, {'page': '2', 'x': '313.92', 'y': '118.21', 'h': '133.30', 'w': '11.97'}, {'page': '2', 'x': '451.32', 'y': '118.21', 'h': '11.96', 'w': '20.44'}, {'page': '2', 'x': '467.52', 'y': '118.21', 'h': '29.57', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '132.13', 'h': '161.28', 'w': '11.97'}, {'page': '2', 'x': '236.76', 'y': '132.13', 'h': '7.96', 'w': '20.44'}, {'page': '2', 'x': '248.04', 'y': '132.13', 'h': '28.08', 'w': '11.97'}], [{'page': '2', 'x': '279.60', 'y': '132.13', 'h': '217.36', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '146.05', 'h': '425.22', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '159.97', 'h': '425.20', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '174.01', 'h': '110.74', 'w': '11.97'}, {'page': '2', 'x': '187.44', 'y': '174.01', 'h': '15.24', 'w': '20.44'}, {'page': '2', 'x': '207.60', 'y': '174.01', 'h': '46.32', 'w': '11.97'}, {'page': '2', 'x': '255.24', 'y': '172.56', 'h': '2.30', 'w': '7.97'}, {'page': '2', 'x': '258.00', 'y': '174.01', 'h': '14.98', 'w': '11.97'}, {'page': '2', 'x': '276.24', 'y': '174.01', 'h': '9.29', 'w': '20.44'}, {'page': '2', 'x': '288.72', 'y': '174.01', 'h': '33.48', 'w': '11.97'}], [{'page': '2', 'x': '329.88', 'y': '174.01', 'h': '167.20', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '187.93', 'h': '299.86', 'w': '11.97'}, {'page': '2', 'x': '376.32', 'y': '187.93', 'h': '11.96', 'w': '20.44'}, {'page': '2', 'x': '392.76', 'y': '187.93', 'h': '104.28', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '201.85', 'h': '44.04', 'w': '11.97'}, {'page': '2', 'x': '119.76', 'y': '201.85', 'h': '7.96', 'w': '20.44'}, {'page': '2', 'x': '131.52', 'y': '201.85', 'h': '28.08', 'w': '11.97'}], [{'page': '2', 'x': '165.36', 'y': '201.85', 'h': '331.69', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '215.77', 'h': '425.16', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '229.81', 'h': '288.24', 'w': '11.97'}], [{'page': '2', 'x': '368.52', 'y': '229.81', 'h': '128.41', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '243.73', 'h': '377.76', 'w': '11.97'}], [{'page': '2', 'x': '455.04', 'y': '243.73', 'h': '42.16', 'w': '11.97'}, {'page': '2', 'x': '72.00', 'y': '257.65', 'h': '50.64', 'w': '11.97'}, {'page': '2', 'x': '128.64', 'y': '257.65', 'h': '7.96', 'w': '20.44'}, {'page': '2', 'x': '141.36', 'y': '257.65', 'h': '50.76', 'w': '11.96'}, {'page': '2', 'x': '196.92', 'y': '257.65', 'h': '7.96', 'w': '20.44'}, {'page': '2', 'x': '209.52', 'y': '257.65', 'h': '287.52', 'w': '11.97'}], [{'page': '2', 'x': '72.00', 'y': '271.57', 'h': '304.31', 'w': '11.97'}, {'page': '2', 'x': '379.80', 'y': '271.57', 'h': '7.96', 'w': '20.44'}, {'page': '2', 'x': '391.08', 'y': '271.57', 'h': '28.08', 'w': '11.97'}]]","text":"In this article we are primarily interested in the class F \u03b1 consisting of nonvanishing functions f that are analytic and univalent in D such that the complement C\\f (D) := (f (D)) c is a convex set, f (1) = \u221e, f (0) = 1 and the angle at \u221e is less than or equal to \u03c0\u03b1, for some \u03b1 \u2208 (1,2].Related to this class is the class of concave univalent mappings in D; the family of analytic functions f that map D conformally onto a set whose complement with respect to C is convex and that satisfy the normalization f (1) = \u221e, f (0) = f \u2032 (0) -1 = 0.In addition, we impose on these functions the condition that the opening angle of f (D) at \u221e is less than or equal to \u03c0\u03b1, \u03b1 \u2208 (1,2].As with the standard practice, we will denote the family of such functions by CO(\u03b1) and call it as the class of concave univalent functions [6,9] which has been extensively studied in the recent years.For a detailed discussion about concave functions, we refer to [4,6,8,9] and the references therein.We note that for f \u2208 CO(\u03b1), \u03b1 \u2208 (1, 2], the closed set C\\f (D) is convex and unbounded.Also, we observe that CO(2) contains the classes CO(\u03b1), \u03b1 \u2208 (1,2].","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1511.08300.pdf","section_number":"1.","para":"6","_id":"2df3aa5b-e9fe-483b-a69b-26dd8c165525","_collection_name":"Papers"},"page_content":"In this article we are primarily interested in the class F \u03b1 consisting of nonvanishing functions f that are analytic and univalent in D such that the complement C\\f (D) := (f (D)) c is a convex set, f (1) = \u221e, f (0) = 1 and the angle at \u221e is less than or equal to \u03c0\u03b1, for some \u03b1 \u2208 (1,2].Related to this class is the class of concave univalent mappings in D; the family of analytic functions f that map D conformally onto a set whose complement with respect to C is convex and that satisfy the normalization f (1) = \u221e, f (0) = f \u2032 (0) -1 = 0.In addition, we impose on these functions the condition that the opening angle of f (D) at \u221e is less than or equal to \u03c0\u03b1, \u03b1 \u2208 (1,2].As with the standard practice, we will denote the family of such functions by CO(\u03b1) and call it as the class of concave univalent functions [6,9] which has been extensively studied in the recent years.For a detailed discussion about concave functions, we refer to [4,6,8,9] and the references therein.We note that for f \u2208 CO(\u03b1), \u03b1 \u2208 (1, 2], the closed set C\\f (D) is convex and unbounded.Also, we observe that CO(2) contains the classes CO(\u03b1), \u03b1 \u2208 (1,2].","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We propose to study the properties (asymptotic behaviour, analyticity and more) of the integral I \u03b5 +-that can be considered a function of two complex variables (\u03b1 1 , \u03b1 2 ) and is defined by","paper_title":"On the asymptotic properties of a canonical diffraction integral","section_title":"Introduction and motivation","bboxes":"[[{'page': '1', 'x': '56.69', 'y': '497.11', 'h': '498.61', 'w': '10.48'}, {'page': '1', 'x': '56.69', 'y': '511.55', 'h': '5.16', 'w': '10.48'}, {'page': '1', 'x': '62.80', 'y': '509.93', 'h': '3.91', 'w': '6.99'}, {'page': '1', 'x': '61.86', 'y': '517.23', 'h': '13.17', 'w': '6.99'}, {'page': '1', 'x': '79.43', 'y': '511.55', 'h': '317.46', 'w': '10.48'}, {'page': '1', 'x': '396.89', 'y': '516.07', 'h': '4.23', 'w': '6.99'}, {'page': '1', 'x': '401.62', 'y': '511.55', 'h': '12.69', 'w': '10.48'}, {'page': '1', 'x': '414.31', 'y': '516.07', 'h': '4.23', 'w': '6.99'}, {'page': '1', 'x': '419.04', 'y': '511.55', 'h': '95.66', 'w': '10.48'}]]","pages":"('1', '1')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2003.00237.pdf","section_number":"1","para":"0","_id":"afd0c01b-4e49-4166-9f09-62e73a0c9066","_collection_name":"Papers"},"page_content":"We propose to study the properties (asymptotic behaviour, analyticity and more) of the integral I \u03b5 +-that can be considered a function of two complex variables (\u03b1 1 , \u03b1 2 ) and is defined by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":". By using the fact that \u03b1, \u03b2 are characteristics curves and f * is a strict convex function, we obtain","paper_title":"Exact and optimal controllability for scalar conservation laws with discontinuous flux Adimurthi *","section_title":"T","bboxes":"[[{'page': '7', 'x': '409.20', 'y': '630.92', 'h': '67.24', 'w': '10.91'}, {'page': '7', 'x': '117.84', 'y': '646.76', 'h': '220.25', 'w': '10.91'}, {'page': '7', 'x': '339.36', 'y': '645.00', 'h': '4.23', 'w': '7.97'}, {'page': '7', 'x': '348.12', 'y': '646.76', 'h': '128.42', 'w': '10.91'}, {'page': '7', 'x': '117.84', 'y': '660.20', 'h': '46.25', 'w': '10.91'}]]","pages":"('7', '7')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1603.01473.pdf","section_number":"None","para":"0","_id":"52597062-5ef5-4889-a327-78ebc411ff14","_collection_name":"Papers"},"page_content":". By using the fact that \u03b1, \u03b2 are characteristics curves and f * is a strict convex function, we obtain","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('5', '5')","paper_title":"CERTAIN SUBCLASSES OF ANALYTIC FUNCTIONS WITH FIXED SECOND COEFFICIENT","section_title":"Introduction","bboxes":"[[{'page': '5', 'x': '117.72', 'y': '118.21', 'h': '98.81', 'w': '11.97'}, {'page': '5', 'x': '221.88', 'y': '118.21', 'h': '7.10', 'w': '20.44'}, {'page': '5', 'x': '229.68', 'y': '116.88', 'h': '4.23', 'w': '7.97'}, {'page': '5', 'x': '228.96', 'y': '124.32', 'h': '3.62', 'w': '7.97'}, {'page': '5', 'x': '234.36', 'y': '118.21', 'h': '37.56', 'w': '11.97'}, {'page': '5', 'x': '272.64', 'y': '117.10', 'h': '4.23', 'w': '6.17'}, {'page': '5', 'x': '277.32', 'y': '118.21', 'h': '129.40', 'w': '11.97'}, {'page': '5', 'x': '406.68', 'y': '123.00', 'h': '4.23', 'w': '7.97'}, {'page': '5', 'x': '417.00', 'y': '118.21', 'h': '7.96', 'w': '20.44'}, {'page': '5', 'x': '430.56', 'y': '118.21', 'h': '73.32', 'w': '11.97'}], [{'page': '5', 'x': '513.12', 'y': '118.21', 'h': '20.08', 'w': '11.97'}, {'page': '5', 'x': '117.72', 'y': '132.13', 'h': '51.60', 'w': '11.97'}, {'page': '5', 'x': '169.32', 'y': '136.92', 'h': '4.23', 'w': '7.97'}, {'page': '5', 'x': '177.96', 'y': '132.13', 'h': '64.37', 'w': '11.97'}]]","text":"Thus, for the class G 1 b , the S * (\u03b1) -radius is the root \u03c1 1 \u2208 (0, 1) of (2.2).The function f 1 is defined by","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2203.08704.pdf","section_number":"1.","para":"1","_id":"39e2282c-86d5-4101-93fc-e4bd5b71b108","_collection_name":"Papers"},"page_content":"Thus, for the class G 1 b , the S * (\u03b1) -radius is the root \u03c1 1 \u2208 (0, 1) of (2.2).The function f 1 is defined by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('5', '5')","paper_title":"\u2206-WEAKLY MIXING SUBSETS ALONG A COLLECTION OF SEQUENCES OF INTEGERS","section_title":"2.2.","bboxes":"[[{'page': '5', 'x': '89.76', 'y': '327.25', 'h': '76.36', 'w': '12.23'}, {'page': '5', 'x': '166.08', 'y': '326.18', 'h': '4.23', 'w': '7.97'}, {'page': '5', 'x': '170.88', 'y': '327.25', 'h': '326.26', 'w': '12.23'}, {'page': '5', 'x': '109.80', 'y': '341.43', 'h': '173.52', 'w': '11.97'}]]","text":"(1) For f \u2208 L 1 (X, B, \u00b5), the image function E(f |A ) is characterized almost everywhere by the two properties:","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2009.07677.pdf","section_number":"None","para":"0","_id":"f8fc2f9f-ead0-4ad8-8eb7-b1f5eaa4d5bd","_collection_name":"Papers"},"page_content":"(1) For f \u2208 L 1 (X, B, \u00b5), the image function E(f |A ) is characterized almost everywhere by the two properties:","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"Tractability properties of the weighted star discrepancy of the Halton sequence","section_title":"Weighted star discrepancy and tractability","bboxes":"[[{'page': '1', 'x': '66.00', 'y': '515.05', 'h': '71.52', 'w': '11.97'}, {'page': '1', 'x': '137.52', 'y': '520.21', 'h': '4.23', 'w': '7.60'}, {'page': '1', 'x': '142.32', 'y': '515.05', 'h': '19.08', 'w': '11.96'}], [{'page': '1', 'x': '163.32', 'y': '515.05', 'h': '12.72', 'w': '11.96'}, {'page': '1', 'x': '176.04', 'y': '519.84', 'h': '4.35', 'w': '7.97'}, {'page': '1', 'x': '180.84', 'y': '515.05', 'h': '4.54', 'w': '11.97'}, {'page': '1', 'x': '188.88', 'y': '514.79', 'h': '7.96', 'w': '20.71'}, {'page': '1', 'x': '200.28', 'y': '515.05', 'h': '23.52', 'w': '11.97'}, {'page': '1', 'x': '223.80', 'y': '513.72', 'h': '4.35', 'w': '7.97'}, {'page': '1', 'x': '228.60', 'y': '515.05', 'h': '108.00', 'w': '11.97'}, {'page': '1', 'x': '336.60', 'y': '520.21', 'h': '4.23', 'w': '7.60'}, {'page': '1', 'x': '341.28', 'y': '515.05', 'h': '4.54', 'w': '11.97'}, {'page': '1', 'x': '348.60', 'y': '514.79', 'h': '9.29', 'w': '20.71'}, {'page': '1', 'x': '360.60', 'y': '515.05', 'h': '21.84', 'w': '11.97'}, {'page': '1', 'x': '382.44', 'y': '520.21', 'h': '4.23', 'w': '7.60'}, {'page': '1', 'x': '387.12', 'y': '515.05', 'h': '4.54', 'w': '11.97'}, {'page': '1', 'x': '394.44', 'y': '514.79', 'h': '9.29', 'w': '20.71'}, {'page': '1', 'x': '406.44', 'y': '515.05', 'h': '13.80', 'w': '11.96'}], [{'page': '1', 'x': '422.88', 'y': '514.79', 'h': '9.29', 'w': '20.71'}, {'page': '1', 'x': '434.88', 'y': '515.05', 'h': '21.84', 'w': '11.97'}, {'page': '1', 'x': '456.72', 'y': '519.84', 'h': '4.35', 'w': '7.97'}, {'page': '1', 'x': '461.64', 'y': '515.05', 'h': '38.04', 'w': '11.97'}, {'page': '1', 'x': '499.68', 'y': '520.21', 'h': '19.37', 'w': '7.60'}, {'page': '1', 'x': '66.00', 'y': '529.45', 'h': '227.40', 'w': '11.97'}], [{'page': '1', 'x': '83.64', 'y': '543.97', 'h': '45.94', 'w': '11.97'}, {'page': '1', 'x': '132.84', 'y': '543.71', 'h': '15.12', 'w': '20.71'}, {'page': '1', 'x': '150.00', 'y': '543.97', 'h': '24.96', 'w': '11.97'}], [{'page': '1', 'x': '176.76', 'y': '543.97', 'h': '56.98', 'w': '11.97'}]]","text":"for all \u03b1 = (\u03b1 1 , . . ., \u03b1 d ) \u2208 [0, 1] d , where [0, \u03b1) = [0, \u03b1 1 ) \u00d7 [0, \u03b1 2 ) \u00d7 . . .\u00d7 [0, \u03b1 d ) and 1 [0,\u03b1) is the characteristic function of this interval.Let [d] = {1, 2, . . ., d} and let","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1803.06101.pdf","section_number":"1","para":"4","_id":"29b84e7c-09cd-43e0-b6b4-65431bd55c49","_collection_name":"Papers"},"page_content":"for all \u03b1 = (\u03b1 1 , . . ., \u03b1 d ) \u2208 [0, 1] d , where [0, \u03b1) = [0, \u03b1 1 ) \u00d7 [0, \u03b1 2 ) \u00d7 . . .\u00d7 [0, \u03b1 d ) and 1 [0,\u03b1) is the characteristic function of this interval.Let [d] = {1, 2, . . ., d} and let","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Let B 2 (D d ) denote the family of all functions g that are analytic in D d , such that:","paper_title":"The Double Exponential Sinc Collocation Method for Singular Sturm-Liouville Problems","section_title":"Definitions and basic properties","bboxes":"[[{'page': '3', 'x': '72.00', 'y': '454.75', 'h': '358.75', 'w': '10.68'}]]","pages":"('3', '3')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1409.7471.pdf","section_number":"2","para":"0","_id":"de549850-1080-48a1-bf07-712c3b323cbc","_collection_name":"Papers"},"page_content":"Let B 2 (D d ) denote the family of all functions g that are analytic in D d , such that:","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Under assumptions (f 1 ) and (f 2 ), the functions u(r, \u03b1) and u \u2032 (r, \u03b1) = \u2202u \u2202r (r, \u03b1) are of class","paper_title":"ON THE UNIQUENESS OF SIGN CHANGING BOUND STATE SOLUTIONS OF A SEMILINEAR EQUATION","section_title":"Preliminaries","bboxes":"[[{'page': '6', 'x': '98.16', 'y': '677.74', 'h': '103.25', 'w': '10.91'}, {'page': '6', 'x': '201.48', 'y': '681.91', 'h': '4.23', 'w': '7.68'}, {'page': '6', 'x': '206.28', 'y': '677.74', 'h': '37.25', 'w': '10.91'}, {'page': '6', 'x': '243.60', 'y': '681.91', 'h': '4.23', 'w': '7.68'}, {'page': '6', 'x': '248.40', 'y': '677.74', 'h': '135.96', 'w': '10.91'}, {'page': '6', 'x': '384.36', 'y': '676.43', 'h': '2.30', 'w': '7.08'}, {'page': '6', 'x': '387.12', 'y': '677.74', 'h': '36.56', 'w': '10.91'}, {'page': '6', 'x': '427.92', 'y': '675.74', 'h': '9.82', 'w': '7.97'}, {'page': '6', 'x': '428.40', 'y': '683.78', 'h': '8.74', 'w': '7.97'}, {'page': '6', 'x': '438.84', 'y': '677.74', 'h': '79.18', 'w': '10.91'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1001.4729.pdf","section_number":"2.","para":"0","_id":"b33e6593-699b-4699-9578-303935c937d7","_collection_name":"Papers"},"page_content":"Under assumptions (f 1 ) and (f 2 ), the functions u(r, \u03b1) and u \u2032 (r, \u03b1) = \u2202u \u2202r (r, \u03b1) are of class","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"This property leads to consider the Ces\u00e1ro operator of order \u03b1 or \u03b1-Ces\u00e1ro operator (see [41]) on the space H(D) of analytic functions f (z) = \u221e n=0 a n z n in D, which is defined by","paper_title":"BOHR RADIUS FOR BANACH SPACES ON SIMPLY CONNECTED DOMAINS","section_title":"Bohr inequality for Ces\u00e1ro operator","bboxes":"[[{'page': '14', 'x': '72.00', 'y': '237.37', 'h': '459.22', 'w': '10.56'}, {'page': '14', 'x': '72.00', 'y': '251.32', 'h': '269.06', 'w': '10.56'}, {'page': '14', 'x': '357.00', 'y': '248.30', 'h': '8.47', 'w': '6.99'}, {'page': '14', 'x': '357.00', 'y': '257.60', 'h': '15.96', 'w': '6.99'}, {'page': '14', 'x': '375.45', 'y': '251.39', 'h': '6.15', 'w': '10.48'}, {'page': '14', 'x': '381.60', 'y': '255.91', 'h': '5.14', 'w': '6.99'}, {'page': '14', 'x': '387.23', 'y': '251.39', 'h': '5.44', 'w': '10.48'}, {'page': '14', 'x': '393.20', 'y': '249.78', 'h': '5.14', 'w': '6.99'}, {'page': '14', 'x': '402.74', 'y': '251.32', 'h': '127.35', 'w': '11.23'}]]","pages":"('14', '14')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2111.10880.pdf","section_number":"2.","para":"0","_id":"9bbb40f7-9d5c-409f-a3e9-5a88e6ec3a46","_collection_name":"Papers"},"page_content":"This property leads to consider the Ces\u00e1ro operator of order \u03b1 or \u03b1-Ces\u00e1ro operator (see [41]) on the space H(D) of analytic functions f (z) = \u221e n=0 a n z n in D, which is defined by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('14', '14')","paper_title":"TWO PREDUALITIES AND THREE OPERATORS OVER ANALYTIC CAMPANATO SPACES","section_title":"Three operators","bboxes":"[[{'page': '14', 'x': '94.68', 'y': '271.91', 'h': '242.63', 'w': '10.76'}], [{'page': '14', 'x': '341.64', 'y': '271.91', 'h': '172.66', 'w': '10.76'}, {'page': '14', 'x': '94.68', 'y': '285.83', 'h': '177.71', 'w': '10.76'}], [{'page': '14', 'x': '275.88', 'y': '285.83', 'h': '238.35', 'w': '10.76'}, {'page': '14', 'x': '94.68', 'y': '299.75', 'h': '419.54', 'w': '10.76'}, {'page': '14', 'x': '94.68', 'y': '313.67', 'h': '106.91', 'w': '10.76'}], [{'page': '14', 'x': '205.68', 'y': '313.67', 'h': '308.31', 'w': '10.76'}, {'page': '14', 'x': '94.68', 'y': '327.71', 'h': '62.13', 'w': '10.76'}]]","text":"then f is univalent on D, for more details see [23].Moreover, S( f ) vanishes identically if and only if f is a M\u00f6bius mapping.Recently, there have been some results linking the Schwarzian derivative of a univalent analytic function to the characterizion of some analytic function spaces.According to Astala-Zinsmeister [6] and P\u00e9rez-Gonz\u00e1lez-R\u00e4tty\u00e4 [21], one has","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1402.4377.pdf","section_number":"3.","para":"3","_id":"980ca90b-83bf-46ec-8dd2-ebe552b83604","_collection_name":"Papers"},"page_content":"then f is univalent on D, for more details see [23].Moreover, S( f ) vanishes identically if and only if f is a M\u00f6bius mapping.Recently, there have been some results linking the Schwarzian derivative of a univalent analytic function to the characterizion of some analytic function spaces.According to Astala-Zinsmeister [6] and P\u00e9rez-Gonz\u00e1lez-R\u00e4tty\u00e4 [21], one has","type":"Document"}],2.1261079311,91,10.6,10.8,5,"The output comprehensively addresses the properties and characteristics of functions in the class F \u03b1 that are analytic and univalent in D, directly answering the question posed in the input. It aligns well with the provided context, detailing the specific conditions and characteristics relevant to the class F \u03b1, thus demonstrating a thorough understanding of the topic."],["How were n-tuple systems first applied to game learning and what is their main goal?"," N-tuple systems were first applied to game learning by Lucas in 2008, although they were already introduced in 1959 for character recognition purposes. Their main goal is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('3', '3')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"C. N-Tuple Systems","bboxes":"[[{'page': '3', 'x': '58.93', 'y': '715.94', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '70.36', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '82.31', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '94.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '106.22', 'h': '115.54', 'w': '8.64'}], [{'page': '3', 'x': '430.00', 'y': '106.22', 'h': '133.04', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '118.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '130.13', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '142.09', 'h': '103.38', 'w': '8.64'}], [{'page': '3', 'x': '418.61', 'y': '142.09', 'h': '144.43', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '154.04', 'h': '140.29', 'w': '8.64'}], [{'page': '3', 'x': '457.49', 'y': '154.04', 'h': '105.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '165.68', 'h': '152.14', 'w': '8.96'}], [{'page': '3', 'x': '468.84', 'y': '165.68', 'h': '94.20', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '177.95', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '189.91', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '201.55', 'h': '227.41', 'w': '8.96'}], [{'page': '3', 'x': '543.12', 'y': '201.86', 'h': '19.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '213.82', 'h': '251.06', 'w': '8.64'}], [{'page': '3', 'x': '311.98', 'y': '225.46', 'h': '224.53', 'w': '8.96'}], [{'page': '3', 'x': '539.71', 'y': '225.77', 'h': '16.79', 'w': '8.64'}], [{'page': '3', 'x': '559.72', 'y': '225.77', 'h': '3.32', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '237.73', 'h': '207.62', 'w': '8.64'}], [{'page': '3', 'x': '522.50', 'y': '237.73', 'h': '40.54', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '249.51', 'h': '251.06', 'w': '8.82'}, {'page': '3', 'x': '311.98', 'y': '261.46', 'h': '21.31', 'w': '8.59'}], [{'page': '3', 'x': '336.15', 'y': '261.64', 'h': '226.89', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '273.60', 'h': '129.27', 'w': '8.64'}, {'page': '3', 'x': '441.25', 'y': '271.93', 'h': '3.49', 'w': '6.05'}, {'page': '3', 'x': '449.18', 'y': '273.60', 'h': '113.86', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '285.55', 'h': '46.38', 'w': '8.64'}], [{'page': '3', 'x': '361.84', 'y': '285.55', 'h': '192.99', 'w': '8.64'}]]","text":"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"13","_id":"69534c6a-07b2-4fd2-87b0-527fd47552b2","_collection_name":"Papers"},"page_content":"N-tuple systems coupled with TD were first applied to game learning by Lucas in 2008 [5], although n-tuples were already introduced in 1959 for character recognition purposes [18].The remarkable success of n-tuples in learning to play Othello [5] motivated other authors to benefit from this approach for a number of other games.The main goal of n-tuple systems is to map a highly non-linear function in a low dimensional space to a high dimensional space where it is easier to separate 'good' and 'bad' regions.This can be compared to the kernel trick of support-vector machines.An n-tuple is defined as a sequence of n cells of the board.Each cell can have m positional values representing the possible states of that cell.Therefore, every n-tuple will have a (possibly large) look-up table indexed in form of an n-digit number in base m.Each entry corresponds to a feature and carries a trainable weight.An n-tuple system is a system consisting of k n-tuples.Tab.I shows the n-tuple systems that we use in this work.Each time a new agent is constructed, all n-tuples are formed by random walk.That is, all cells are placed randomly with the constraint that each cell must be adjacent 4 to at least one other cell in the n-tuple.An example n-tuple system is shown in Fig. 2.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '2')","paper_title":"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization","section_title":"Introduction and Motivation","bboxes":"[[{'page': '1', 'x': '107.53', 'y': '641.95', 'h': '396.72', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '652.86', 'h': '277.34', 'w': '8.64'}], [{'page': '1', 'x': '388.46', 'y': '652.86', 'h': '115.54', 'w': '8.64'}, {'page': '1', 'x': '107.64', 'y': '663.77', 'h': '396.36', 'w': '8.64'}, {'page': '1', 'x': '108.00', 'y': '674.68', 'h': '396.00', 'w': '8.64'}, {'page': '1', 'x': '107.67', 'y': '685.59', 'h': '26.85', 'w': '8.64'}], [{'page': '1', 'x': '107.69', 'y': '701.80', 'h': '396.31', 'w': '8.82'}, {'page': '1', 'x': '108.00', 'y': '712.89', 'h': '271.65', 'w': '8.64'}], [{'page': '1', 'x': '382.14', 'y': '712.89', 'h': '122.11', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '75.48', 'h': '396.34', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '86.39', 'h': '56.44', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '86.39', 'h': '334.18', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '97.30', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '108.20', 'h': '104.76', 'w': '8.64'}]]","text":"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games\/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1807.01672.pdf","section_number":"1","para":"4","_id":"22f24231-c49d-46f8-8f6b-90244defd1a7","_collection_name":"Papers"},"page_content":"While self-play algorithms have proven successful for two-player games, there has been little work on applying similar principles to single-player games\/problems [10].These games include several well-known combinatorial problems that are particularly relevant to industry and represent real-world optimization challenges, such as the traveling salesman problem (TSP) and the bin packing problem (BPP).This paper describes the Ranked Reward (R2) algorithm and results from its application to 2D and 3D BPP formulated as a single-player Markov decision process (MDP).R2 uses a deep neural network to estimate a policy and a value function, as well as Monte Carlo tree search (MCTS) for policy improvement.In addition, it uses a reward ranking mechanism to build a single-player training curriculum that provides advantages comparable to those produced by self-play in competitive multi-agent environments.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Related Work in N-Tuple Research","bboxes":"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]","pages":"('9', '10')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"1f958ad8-f8c9-4c82-bc74-8755781e702a","_collection_name":"Papers"},"page_content":"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].","paper_title":"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking","section_title":"I. INTRODUCTION","bboxes":"[[{'page': '1', 'x': '321.94', 'y': '482.81', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '494.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '506.72', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '518.68', 'h': '48.70', 'w': '8.64'}], [{'page': '1', 'x': '363.71', 'y': '518.68', 'h': '199.32', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '530.63', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '542.59', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '554.54', 'h': '70.18', 'w': '8.64'}]]","pages":"('1', '1')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1809.02904.pdf","section_number":"None","para":"1","_id":"9f1cb79f-9152-46c9-af5f-5eb2e6eb2e10","_collection_name":"Papers"},"page_content":"In this paper we will try to address these questions in general -and provide an application to the General Video Game AI (GVGAI) framework as an example and test of our method.The GVGAI library includes more than 100 mini video games [6], and several dozen agents that can play these games [7]- [11] have been submitted to the associated GVGAI competition [12].","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('9', '9')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Related Work in N-Tuple Research","bboxes":"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]","text":"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"f44fc6ea-68b1-483c-94e5-9150ebad6361","_collection_name":"Papers"},"page_content":"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('5', '5')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Common Settings","bboxes":"[[{'page': '5', 'x': '58.93', 'y': '257.47', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '269.43', 'h': '46.22', 'w': '8.64'}], [{'page': '5', 'x': '99.80', 'y': '269.43', 'h': '200.22', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '281.38', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '293.34', 'h': '80.28', 'w': '8.64'}], [{'page': '5', 'x': '133.04', 'y': '293.34', 'h': '166.98', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '305.29', 'h': '187.80', 'w': '8.64'}], [{'page': '5', 'x': '239.50', 'y': '305.29', 'h': '60.52', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '317.25', 'h': '62.13', 'w': '8.64'}]]","text":"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"29fe50cd-f0a8-4a63-ad7d-2149f11b30a4","_collection_name":"Papers"},"page_content":"The hyperparameters for each game were found by manual fine-tuning.For reasons of space we give the exact explanation and the setting of all parameters as supplementary material in Appendix G of [8].A short version of these settings is: The chosen n-tuple configurations are given in Tab.I, and the main parameters are:","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('4', '4')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"D. TD Learning and FARL","bboxes":"[[{'page': '4', 'x': '48.96', 'y': '394.21', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '405.85', 'h': '199.96', 'w': '8.96'}], [{'page': '4', 'x': '58.93', 'y': '418.54', 'h': '241.09', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '430.50', 'h': '251.06', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '442.45', 'h': '153.73', 'w': '8.64'}], [{'page': '4', 'x': '205.94', 'y': '442.45', 'h': '94.08', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '454.41', 'h': '95.65', 'w': '8.64'}], [{'page': '4', 'x': '150.02', 'y': '454.09', 'h': '150.00', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '466.36', 'h': '251.05', 'w': '8.64'}, {'page': '4', 'x': '48.96', 'y': '478.00', 'h': '251.05', 'w': '8.96'}, {'page': '4', 'x': '48.96', 'y': '490.27', 'h': '223.43', 'w': '8.64'}]]","text":"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\u03bb) with arbitrary \u03bb \u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"4d604c9d-6f9d-4da1-b222-4df1eeae3b73","_collection_name":"Papers"},"page_content":"This FARL step was found to be crucial for reaching training success in N -player games with arbitrary N [9].TD-FARL performs its training purely by -greedy selfplay: The agent plays against itself and makes random moves with probability in order to explore.No external knowledge or databases are used.More details on TD-FARL for Nplayer games, including the extension to TD(\u03bb) with arbitrary \u03bb \u2208 [0, 1], based on the eligibility mechanism for n-tuple systems [11], are described in our previous work [10].","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1","paper_title":"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS","section_title":"SOFTENING SYMBOL GROUNDING","bboxes":"[[{'page': '3', 'x': '108.00', 'y': '581.75', 'h': '302.26', 'w': '8.64'}], [{'page': '3', 'x': '413.23', 'y': '581.75', 'h': '90.77', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '592.49', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '602.91', 'h': '131.64', 'w': '8.96'}], [{'page': '3', 'x': '244.10', 'y': '603.23', 'h': '259.90', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '613.65', 'h': '350.00', 'w': '9.65'}], [{'page': '3', 'x': '462.50', 'y': '613.97', 'h': '41.50', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '624.71', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '635.45', 'h': '311.56', 'w': '8.64'}]]","pages":"('3', '3')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2403.00323.pdf","section_number":"2","para":"3","_id":"dd7af29e-9b01-4006-8f52-f196daf712a1","_collection_name":"Papers"},"page_content":"Either problem (1) or ( 2) can be viewed as a pure-strategy Stackelberg game.That is, both the model training and the symbolic reasoning are forced to take a certain action (e.g., selecting a deterministic mapping h) during optimization.In contrast, problem (P) can be seen as a Stackelberg game with mixed strategies, where the player takes a randomized action with the distribution Q \u03d5 .Compared with the pure strategy, a mixed strategy does provide more information of the game, and thus strictly improves the utility of model training (Letchford et al., 2014;Conitzer, 2016).1","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"D. TD Learning and FARL","bboxes":"[[{'page': '3', 'x': '321.94', 'y': '582.82', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '594.45', 'h': '251.06', 'w': '8.96'}, {'page': '3', 'x': '311.98', 'y': '606.73', 'h': '167.22', 'w': '8.64'}], [{'page': '3', 'x': '482.10', 'y': '606.73', 'h': '80.94', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '618.68', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '630.64', 'h': '177.02', 'w': '8.64'}]]","pages":"('3', '3')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"2dd8b318-243d-4eb5-b3b4-c9b7bf1cce30","_collection_name":"Papers"},"page_content":"The goal of n-tuple agent training is to learn a value function V that generates a good policy, i. e. a policy that selects in almost all cases the best action.In our work, we use the TD learning algorithm TD-FARL [9], [10] for training, which is briefly described in the following.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Common Settings","bboxes":"[[{'page': '5', 'x': '58.93', 'y': '186.00', 'h': '241.09', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '197.95', 'h': '105.07', 'w': '8.64'}], [{'page': '5', 'x': '157.24', 'y': '197.95', 'h': '142.78', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '209.91', 'h': '156.81', 'w': '8.64'}], [{'page': '5', 'x': '209.51', 'y': '209.91', 'h': '90.51', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '221.50', 'h': '240.44', 'w': '9.01'}], [{'page': '5', 'x': '293.39', 'y': '221.87', 'h': '6.64', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '233.82', 'h': '251.06', 'w': '8.64'}, {'page': '5', 'x': '48.96', 'y': '245.41', 'h': '198.89', 'w': '9.01'}]]","pages":"('5', '5')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"f9dbab16-ca23-4ced-aec9-96f0c89b23f5","_collection_name":"Papers"},"page_content":"We use for all our experiments the same RL agent based on n-tuple systems and TCL.Only its hyperparameters are tuned to the specific game, as shown below.We refer to this agent as TCL-base whenever it alone is used for game playing.If we wrap this agent by an MCTS wrapper with a given number of iterations, then we refer to this as TCL-wrap.","type":"Document"}],1.3453702927,67,13.6,15.5,5,"The output comprehensively answers the question about the application of n-tuple systems in game learning and their main goal, directly referencing the context provided. It includes specific historical details and a clear explanation of the purpose of n-tuple systems, aligning well with the input query."],["What are near-perfect agents in the game of ConnectFour and how do they differ from each other?"," Near-perfect agents in ConnectFour are agents that use alpha-beta search to make near-perfect moves in the game. They differ from each other in the way they react to losing states, with AB taking a random move and AB-DL searching for a move that postpones the loss as far as possible.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('6', '6')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. ConnectFour","bboxes":"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]","text":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"5","_id":"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1","_collection_name":"Papers"},"page_content":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('6', '6')","paper_title":"MMS Allocations of Chores with Connectivity Constraints: New Methods and New Results","section_title":"Trees with depth 3","bboxes":"[[{'page': '6', 'x': '99.98', 'y': '410.97', 'h': '311.42', 'w': '9.68'}], [{'page': '6', 'x': '418.65', 'y': '411.00', 'h': '91.59', 'w': '8.74'}, {'page': '6', 'x': '85.04', 'y': '422.95', 'h': '66.47', 'w': '9.65'}], [{'page': '6', 'x': '155.91', 'y': '422.95', 'h': '354.32', 'w': '9.65'}, {'page': '6', 'x': '85.04', 'y': '434.91', 'h': '381.21', 'w': '9.65'}], [{'page': '6', 'x': '470.90', 'y': '434.91', 'h': '39.33', 'w': '8.74'}, {'page': '6', 'x': '85.04', 'y': '446.86', 'h': '173.22', 'w': '8.74'}], [{'page': '6', 'x': '262.85', 'y': '446.86', 'h': '247.39', 'w': '9.65'}, {'page': '6', 'x': '85.04', 'y': '458.82', 'h': '115.81', 'w': '8.74'}]]","text":"Case 1: There is a star c q \u2208 C that is x-perfect for some integer x.Assume that agent i x-splits star c q .We consider the x-partition of c q by agent i and assign the bundle containing the center vertex of c q to agent i and assign the left x -1 chores to arbitrary x -1 agents.All the x agents are satisfied with this allocation.By Lemma 4.7, we know that this allocation of c q to the x agents is group-satisfied.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2302.13224.pdf","section_number":"5","para":"4","_id":"a31b66a7-9314-4a93-bd16-1c50d459f157","_collection_name":"Papers"},"page_content":"Case 1: There is a star c q \u2208 C that is x-perfect for some integer x.Assume that agent i x-splits star c q .We consider the x-partition of c q by agent i and assign the bundle containing the center vertex of c q to agent i and assign the left x -1 chores to arbitrary x -1 agents.All the x agents are satisfied with this allocation.By Lemma 4.7, we know that this allocation of c q to the x agents is group-satisfied.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"An alternative definition of perfection has been made Myerson [12, pp 75-76] and is based on the idea that every pure strategy in a player's set of pure strategies has associated with it a small positive probability of at least \u03b5 > 0, but on strategies that are best responses have associated probabilities greater that \u03b5.More formally, for any player i \u2208 N a mixed strategy p i \u2208 \u2206 i is an \u03b5-perfect equilibrium iff it is completely mixed and","paper_title":"","section_title":"Trembling Hand Algorithm 3.1 A MCMC Algorithm for Computing Perfect Equilibria in Strategic Games","bboxes":"[[{'page': '5', 'x': '87.00', 'y': '251.37', 'h': '452.94', 'w': '9.96'}, {'page': '5', 'x': '72.00', 'y': '263.25', 'h': '468.09', 'w': '9.96'}, {'page': '5', 'x': '72.00', 'y': '275.25', 'h': '442.56', 'w': '9.96'}], [{'page': '5', 'x': '517.56', 'y': '275.25', 'h': '22.54', 'w': '9.96'}, {'page': '5', 'x': '72.00', 'y': '287.13', 'h': '467.97', 'w': '10.66'}, {'page': '5', 'x': '72.00', 'y': '299.13', 'h': '45.73', 'w': '9.96'}]]","pages":"('5', '5')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/cs_0309016.pdf","section_number":"3","para":"1","_id":"0c973955-68f6-4f1d-bd04-5871096cc69b","_collection_name":"Papers"},"page_content":"An alternative definition of perfection has been made Myerson [12, pp 75-76] and is based on the idea that every pure strategy in a player's set of pure strategies has associated with it a small positive probability of at least \u03b5 > 0, but on strategies that are best responses have associated probabilities greater that \u03b5.More formally, for any player i \u2208 N a mixed strategy p i \u2208 \u2206 i is an \u03b5-perfect equilibrium iff it is completely mixed and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We define a class of agents to be a subset of indistinguishable agents C \u010e A. More formally, all agents in C share a common preference function p C : A \u00d1 R and no agent in A discriminates between two agents in C. Note that this implies that the lines and columns of the preference matrix corresponding to agents in C are identical, if we adopt the convention that diagonal entries inside a class are all equal but not necessarily zero.We say that preference profile","paper_title":"Stable Dinner Party Seating Arrangements","section_title":"Preliminaries","bboxes":"[[{'page': '7', 'x': '149.71', 'y': '166.99', 'h': '330.88', 'w': '9.35'}, {'page': '7', 'x': '134.77', 'y': '178.95', 'h': '345.83', 'w': '9.71'}, {'page': '7', 'x': '134.77', 'y': '190.91', 'h': '345.82', 'w': '8.80'}, {'page': '7', 'x': '134.77', 'y': '202.86', 'h': '345.25', 'w': '8.80'}, {'page': '7', 'x': '134.77', 'y': '214.82', 'h': '345.82', 'w': '8.80'}, {'page': '7', 'x': '134.77', 'y': '226.77', 'h': '143.66', 'w': '8.80'}], [{'page': '7', 'x': '281.24', 'y': '226.77', 'h': '128.38', 'w': '8.80'}]]","pages":"('7', '7')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2305.09549.pdf","section_number":"3","para":"1","_id":"878f000c-f4c4-4941-ae36-ed025afa384a","_collection_name":"Papers"},"page_content":"We define a class of agents to be a subset of indistinguishable agents C \u010e A. More formally, all agents in C share a common preference function p C : A \u00d1 R and no agent in A discriminates between two agents in C. Note that this implies that the lines and columns of the preference matrix corresponding to agents in C are identical, if we adopt the convention that diagonal entries inside a class are all equal but not necessarily zero.We say that preference profile","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('3', '3')","paper_title":"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking","section_title":"IV. CORRELATION ANALYSIS","bboxes":"[[{'page': '3', 'x': '321.94', 'y': '530.63', 'h': '241.09', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '542.59', 'h': '27.39', 'w': '8.64'}], [{'page': '3', 'x': '344.60', 'y': '542.59', 'h': '218.44', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '554.54', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '566.50', 'h': '22.96', 'w': '8.64'}], [{'page': '3', 'x': '338.64', 'y': '566.50', 'h': '224.40', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '578.45', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '590.41', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '602.36', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '614.32', 'h': '51.15', 'w': '8.64'}], [{'page': '3', 'x': '368.87', 'y': '614.32', 'h': '194.16', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '626.27', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '638.23', 'h': '194.40', 'w': '8.64'}], [{'page': '3', 'x': '510.11', 'y': '638.23', 'h': '52.92', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '650.18', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '662.14', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '674.09', 'h': '251.06', 'w': '8.64'}, {'page': '3', 'x': '311.98', 'y': '686.05', 'h': '177.81', 'w': '8.64'}]]","text":"Figure 1b shows the correlation matrix using the agents' scores.Using this, we can see that the score distributions between most of the games are similar (the matrix is mostly blue).This was not surprising as we already know that most of the games in the framework are designed to have a score distribution that reflects the progress of the agents in the game (good states have high scores, while bad states have low scores).The only exception to this is the first three clusters, which are highly anti-correlated with every game in the framework that isn't within its own cluster.These games appear to be characterized by a delayed score distribution (score only received near the end of the game) which makes them very different from the other games that provide rewards for incremental steps closer to the solution.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1809.02904.pdf","section_number":"None","para":"4","_id":"ea22c475-81d2-4173-9afd-9ced9765c6c7","_collection_name":"Papers"},"page_content":"Figure 1b shows the correlation matrix using the agents' scores.Using this, we can see that the score distributions between most of the games are similar (the matrix is mostly blue).This was not surprising as we already know that most of the games in the framework are designed to have a score distribution that reflects the progress of the agents in the game (good states have high scores, while bad states have low scores).The only exception to this is the first three clusters, which are highly anti-correlated with every game in the framework that isn't within its own cluster.These games appear to be characterized by a delayed score distribution (score only received near the end of the game) which makes them very different from the other games that provide rewards for incremental steps closer to the solution.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We start by describing the initial graph G that we then slightly modify in different scenarios so that it does not meet our conditions on the minimum degree and the connectivity.We now describe G: we start from two complete graphs G 1 , G 2 with sizes n 1 and n 2 , (n 1 \u2265 n 2 ), and connect every agent in n 2 with exactly k 2 agents in n 1 , such that no agent in n 1 receives more than k 2 edges.Furthermore, we sample","paper_title":"Community Consensus: Converging Locally despite Adversaries and Heterogeneous Connectivity","section_title":"A. Setup","bboxes":"[[{'page': '6', 'x': '323.16', 'y': '215.94', 'h': '234.84', 'w': '8.96'}, {'page': '6', 'x': '313.20', 'y': '228.21', 'h': '244.80', 'w': '8.64'}, {'page': '6', 'x': '313.20', 'y': '240.17', 'h': '244.80', 'w': '8.64'}], [{'page': '6', 'x': '313.20', 'y': '251.80', 'h': '244.80', 'w': '8.96'}, {'page': '6', 'x': '313.20', 'y': '263.76', 'h': '244.80', 'w': '9.65'}, {'page': '6', 'x': '313.20', 'y': '275.71', 'h': '244.80', 'w': '9.65'}, {'page': '6', 'x': '313.20', 'y': '287.67', 'h': '142.74', 'w': '9.65'}], [{'page': '6', 'x': '459.25', 'y': '287.99', 'h': '98.75', 'w': '8.64'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2310.08488.pdf","section_number":"None","para":"2","_id":"6d65d023-f319-41b3-aa9a-ad7c1131d399","_collection_name":"Papers"},"page_content":"We start by describing the initial graph G that we then slightly modify in different scenarios so that it does not meet our conditions on the minimum degree and the connectivity.We now describe G: we start from two complete graphs G 1 , G 2 with sizes n 1 and n 2 , (n 1 \u2265 n 2 ), and connect every agent in n 2 with exactly k 2 agents in n 1 , such that no agent in n 1 receives more than k 2 edges.Furthermore, we sample","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('5', '5')","paper_title":"Community Consensus: Converging Locally despite Adversaries and Heterogeneous Connectivity","section_title":"B. Formal Analysis","bboxes":"[[{'page': '5', 'x': '448.23', 'y': '424.42', 'h': '109.78', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '436.38', 'h': '175.98', 'w': '8.96'}], [{'page': '5', 'x': '492.92', 'y': '436.69', 'h': '65.08', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '448.33', 'h': '216.69', 'w': '8.96'}, {'page': '5', 'x': '532.10', 'y': '446.76', 'h': '4.08', 'w': '6.12'}, {'page': '5', 'x': '529.89', 'y': '452.97', 'h': '2.82', 'w': '6.12'}, {'page': '5', 'x': '536.68', 'y': '448.33', 'h': '21.32', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '460.29', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '472.56', 'h': '36.78', 'w': '8.64'}], [{'page': '5', 'x': '352.88', 'y': '472.24', 'h': '174.67', 'w': '8.96'}, {'page': '5', 'x': '529.77', 'y': '470.67', 'h': '4.08', 'w': '6.12'}, {'page': '5', 'x': '527.55', 'y': '476.88', 'h': '2.82', 'w': '6.12'}, {'page': '5', 'x': '534.35', 'y': '472.56', 'h': '23.65', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '484.20', 'h': '122.89', 'w': '9.65'}], [{'page': '5', 'x': '438.60', 'y': '484.52', 'h': '119.40', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '496.15', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '508.11', 'h': '228.95', 'w': '8.96'}], [{'page': '5', 'x': '544.97', 'y': '508.43', 'h': '13.03', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '520.06', 'h': '193.90', 'w': '8.96'}, {'page': '5', 'x': '501.01', 'y': '520.06', 'h': '56.99', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '532.02', 'h': '244.80', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '543.97', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '556.25', 'h': '48.88', 'w': '8.64'}], [{'page': '5', 'x': '365.64', 'y': '555.93', 'h': '192.36', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '567.88', 'h': '197.44', 'w': '8.96'}], [{'page': '5', 'x': '513.56', 'y': '568.20', 'h': '44.43', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '579.84', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '591.79', 'h': '244.30', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '603.75', 'h': '244.80', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '615.70', 'h': '244.30', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '627.98', 'h': '244.80', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '639.61', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '651.89', 'h': '22.42', 'w': '8.64'}], [{'page': '5', 'x': '339.11', 'y': '651.57', 'h': '218.89', 'w': '8.96'}, {'page': '5', 'x': '313.20', 'y': '663.52', 'h': '244.80', 'w': '9.65'}, {'page': '5', 'x': '313.20', 'y': '675.62', 'h': '227.54', 'w': '8.82'}], [{'page': '5', 'x': '543.61', 'y': '675.80', 'h': '14.39', 'w': '8.64'}, {'page': '5', 'x': '313.20', 'y': '687.75', 'h': '201.54', 'w': '8.64'}]]","text":"By definition, any r-excess reachable agent is also 0-excess reachable.Key in this part is to show that any change in the connectivity of G[V * i ] will not lead to the number of 0-excess reachable agents in V i to decrease.We therefore take a subset of agents S \u2286 V * i , such that u \u2208 S and that S\u2229V i \u0338 = \u2205.Without loss of generality, we assume that u is a k i -excess reachable agent in G[V i ], given the choice S, and that u and v are connected by an edge.We observe that there are 2 cases of interest: 1) v \/ \u2208 S 2) v \u2208 S. In the first case, v being external to S increases the count of external neighbors, leading to k i + 1 > k i and maintaining 0 reachability.In case 2) both u and v belong to S, therefore the count of internal neighbors is increased by 1.This yields k i -1 \u2265 0, since the exclusion of the case V i = V implies k i \u2265 1. Extending to the case where k i agents external to V i are connected to u, we observe that, at worst case, S contains all k i external agents, yielding k i -k i \u2265 0. Any set of k i additions is independent of each other, thus not affecting the reachability of the agents in V i with respect to each other.Additionally, for any agent v belonging to a different community G[V j\u0338 =i ], the same reasoning applies from v's perspective, in this way proving the robustness property.See Fig. 3 for a graphic representation the two cases.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2310.08488.pdf","section_number":"None","para":"8","_id":"31218ba1-41f4-4003-9407-bc8d0a03b594","_collection_name":"Papers"},"page_content":"By definition, any r-excess reachable agent is also 0-excess reachable.Key in this part is to show that any change in the connectivity of G[V * i ] will not lead to the number of 0-excess reachable agents in V i to decrease.We therefore take a subset of agents S \u2286 V * i , such that u \u2208 S and that S\u2229V i \u0338 = \u2205.Without loss of generality, we assume that u is a k i -excess reachable agent in G[V i ], given the choice S, and that u and v are connected by an edge.We observe that there are 2 cases of interest: 1) v \/ \u2208 S 2) v \u2208 S. In the first case, v being external to S increases the count of external neighbors, leading to k i + 1 > k i and maintaining 0 reachability.In case 2) both u and v belong to S, therefore the count of internal neighbors is increased by 1.This yields k i -1 \u2265 0, since the exclusion of the case V i = V implies k i \u2265 1. Extending to the case where k i agents external to V i are connected to u, we observe that, at worst case, S contains all k i external agents, yielding k i -k i \u2265 0. Any set of k i additions is independent of each other, thus not affecting the reachability of the agents in V i with respect to each other.Additionally, for any agent v belonging to a different community G[V j\u0338 =i ], the same reasoning applies from v's perspective, in this way proving the robustness property.See Fig. 3 for a graphic representation the two cases.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"As was shown by Selten [16], the perfect equilibria of a game's strategic and extensive forms need not coincide.However he showed that an equivalence relationship holds between the equilibria of any extensive game and its associated agent normal form [16].This is because the agent normal form of any game views each node of the game tree, of the extensive form of the game, as a player in the game.As a consequence each player represents an information set held by the player and will have an identical pay-off function to the player.","paper_title":"","section_title":"An Application to Extensive Form Games","bboxes":"[[{'page': '8', 'x': '87.00', 'y': '409.29', 'h': '453.02', 'w': '9.96'}, {'page': '8', 'x': '72.00', 'y': '421.17', 'h': '37.68', 'w': '9.96'}], [{'page': '8', 'x': '114.12', 'y': '421.17', 'h': '425.98', 'w': '9.96'}, {'page': '8', 'x': '72.00', 'y': '433.17', 'h': '211.44', 'w': '9.96'}], [{'page': '8', 'x': '288.24', 'y': '433.17', 'h': '251.84', 'w': '9.96'}, {'page': '8', 'x': '72.00', 'y': '445.05', 'h': '386.16', 'w': '9.96'}], [{'page': '8', 'x': '463.44', 'y': '445.05', 'h': '76.66', 'w': '9.96'}, {'page': '8', 'x': '72.00', 'y': '457.05', 'h': '467.94', 'w': '9.96'}, {'page': '8', 'x': '72.00', 'y': '469.05', 'h': '46.44', 'w': '9.96'}]]","pages":"('8', '8')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/cs_0309016.pdf","section_number":"4","para":"3","_id":"a7d6f66e-4363-4111-b137-75b7907d8650","_collection_name":"Papers"},"page_content":"As was shown by Selten [16], the perfect equilibria of a game's strategic and extensive forms need not coincide.However he showed that an equivalence relationship holds between the equilibria of any extensive game and its associated agent normal form [16].This is because the agent normal form of any game views each node of the game tree, of the extensive form of the game, as a player in the game.As a consequence each player represents an information set held by the player and will have an identical pay-off function to the player.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('14', '14')","paper_title":"Theory of and Experiments on Minimally Invasive Stability Preservation in Changing Two-Sided Matching Markets","section_title":"We leave open whether the (above shown) W[1]-hardness of IHR-T upholds for the parameter m","bboxes":"[[{'page': '14', 'x': '88.92', 'y': '291.68', 'h': '313.05', 'w': '10.91'}], [{'page': '14', 'x': '404.52', 'y': '291.68', 'h': '117.62', 'w': '11.85'}, {'page': '14', 'x': '72.00', 'y': '306.05', 'h': '304.53', 'w': '9.75'}], [{'page': '14', 'x': '379.80', 'y': '305.24', 'h': '143.32', 'w': '11.86'}, {'page': '14', 'x': '72.00', 'y': '319.61', 'h': '27.93', 'w': '9.75'}], [{'page': '14', 'x': '103.68', 'y': '319.61', 'h': '419.55', 'w': '9.75'}, {'page': '14', 'x': '72.00', 'y': '332.36', 'h': '94.29', 'w': '10.91'}], [{'page': '14', 'x': '170.40', 'y': '332.36', 'h': '352.62', 'w': '10.91'}, {'page': '14', 'x': '72.00', 'y': '346.73', 'h': '10.85', 'w': '9.75'}, {'page': '14', 'x': '86.16', 'y': '345.68', 'h': '18.02', 'w': '18.89'}, {'page': '14', 'x': '105.96', 'y': '345.92', 'h': '59.02', 'w': '10.91'}, {'page': '14', 'x': '168.84', 'y': '345.68', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '180.12', 'y': '345.92', 'h': '8.18', 'w': '10.91'}, {'page': '14', 'x': '188.28', 'y': '345.92', 'h': '35.88', 'w': '11.49'}, {'page': '14', 'x': '228.36', 'y': '345.68', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '239.64', 'y': '345.92', 'h': '8.18', 'w': '10.91'}, {'page': '14', 'x': '247.80', 'y': '345.92', 'h': '57.97', 'w': '11.61'}, {'page': '14', 'x': '309.72', 'y': '345.68', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '321.00', 'y': '345.92', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '327.36', 'y': '345.92', 'h': '35.44', 'w': '11.49'}, {'page': '14', 'x': '367.32', 'y': '345.68', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '378.48', 'y': '345.92', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '384.84', 'y': '346.73', 'h': '12.45', 'w': '10.68'}], [{'page': '14', 'x': '400.44', 'y': '345.92', 'h': '23.92', 'w': '10.91'}, {'page': '14', 'x': '424.92', 'y': '344.16', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '431.64', 'y': '345.68', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '442.92', 'y': '345.92', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '449.28', 'y': '346.73', 'h': '73.95', 'w': '10.68'}, {'page': '14', 'x': '72.00', 'y': '359.36', 'h': '94.45', 'w': '10.91'}, {'page': '14', 'x': '166.44', 'y': '357.60', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '173.40', 'y': '359.12', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '184.80', 'y': '359.36', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '191.16', 'y': '359.36', 'h': '114.45', 'w': '11.49'}], [{'page': '14', 'x': '309.12', 'y': '359.36', 'h': '104.56', 'w': '10.91'}, {'page': '14', 'x': '417.24', 'y': '359.12', 'h': '15.38', 'w': '18.89'}, {'page': '14', 'x': '434.40', 'y': '359.36', 'h': '6.16', 'w': '10.91'}, {'page': '14', 'x': '441.12', 'y': '357.60', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '444.12', 'y': '359.12', 'h': '5.45', 'w': '18.89'}, {'page': '14', 'x': '452.88', 'y': '360.17', 'h': '15.65', 'w': '9.75'}, {'page': '14', 'x': '471.96', 'y': '359.12', 'h': '12.37', 'w': '18.89'}, {'page': '14', 'x': '484.32', 'y': '357.60', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '487.20', 'y': '359.36', 'h': '35.93', 'w': '10.91'}, {'page': '14', 'x': '72.00', 'y': '373.73', 'h': '9.04', 'w': '9.75'}, {'page': '14', 'x': '84.60', 'y': '372.68', 'h': '18.02', 'w': '18.89'}, {'page': '14', 'x': '104.40', 'y': '372.92', 'h': '107.68', 'w': '10.91'}, {'page': '14', 'x': '217.20', 'y': '372.68', 'h': '8.48', 'w': '18.89'}, {'page': '14', 'x': '225.72', 'y': '377.16', 'h': '5.44', 'w': '7.24'}, {'page': '14', 'x': '236.28', 'y': '372.92', 'h': '6.16', 'w': '10.91'}, {'page': '14', 'x': '243.00', 'y': '371.16', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '249.36', 'y': '372.92', 'h': '26.29', 'w': '10.91'}, {'page': '14', 'x': '280.20', 'y': '372.68', 'h': '8.48', 'w': '18.89'}, {'page': '14', 'x': '288.72', 'y': '377.28', 'h': '4.75', 'w': '7.24'}, {'page': '14', 'x': '299.04', 'y': '372.92', 'h': '6.97', 'w': '10.91'}, {'page': '14', 'x': '306.00', 'y': '371.16', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '308.76', 'y': '373.73', 'h': '2.73', 'w': '9.75'}], [{'page': '14', 'x': '315.12', 'y': '372.92', 'h': '169.65', 'w': '10.91'}], [{'page': '14', 'x': '488.88', 'y': '373.73', 'h': '34.17', 'w': '9.75'}, {'page': '14', 'x': '72.00', 'y': '386.48', 'h': '116.19', 'w': '10.91'}, {'page': '14', 'x': '88.92', 'y': '414.41', 'h': '342.93', 'w': '9.75'}], [{'page': '14', 'x': '436.32', 'y': '413.60', 'h': '86.80', 'w': '11.85'}, {'page': '14', 'x': '72.00', 'y': '427.97', 'h': '52.49', 'w': '9.75'}, {'page': '14', 'x': '127.92', 'y': '426.92', 'h': '7.58', 'w': '18.89'}, {'page': '14', 'x': '135.48', 'y': '427.97', 'h': '27.53', 'w': '11.05'}, {'page': '14', 'x': '166.44', 'y': '426.92', 'h': '13.58', 'w': '18.89'}, {'page': '14', 'x': '180.00', 'y': '431.04', 'h': '4.23', 'w': '7.97'}, {'page': '14', 'x': '184.80', 'y': '426.92', 'h': '20.30', 'w': '18.89'}, {'page': '14', 'x': '205.08', 'y': '431.04', 'h': '4.23', 'w': '7.97'}, {'page': '14', 'x': '209.76', 'y': '426.92', 'h': '15.80', 'w': '18.89'}, {'page': '14', 'x': '229.92', 'y': '427.16', 'h': '8.73', 'w': '10.91'}], [{'page': '14', 'x': '242.04', 'y': '427.16', 'h': '281.20', 'w': '11.86'}, {'page': '14', 'x': '72.00', 'y': '440.72', 'h': '231.45', 'w': '11.85'}], [{'page': '14', 'x': '307.44', 'y': '440.72', 'h': '61.37', 'w': '10.91'}, {'page': '14', 'x': '368.76', 'y': '438.96', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '375.36', 'y': '440.72', 'h': '80.77', 'w': '10.91'}, {'page': '14', 'x': '456.12', 'y': '444.96', 'h': '5.72', 'w': '7.24'}, {'page': '14', 'x': '466.44', 'y': '440.48', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '476.88', 'y': '440.72', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '483.24', 'y': '441.53', 'h': '40.00', 'w': '10.68'}, {'page': '14', 'x': '72.00', 'y': '454.28', 'h': '6.97', 'w': '10.91'}, {'page': '14', 'x': '82.80', 'y': '454.04', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '93.96', 'y': '454.28', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '100.32', 'y': '454.28', 'h': '35.32', 'w': '11.49'}, {'page': '14', 'x': '140.04', 'y': '454.04', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '151.08', 'y': '454.28', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '157.44', 'y': '454.28', 'h': '365.20', 'w': '11.49'}, {'page': '14', 'x': '72.00', 'y': '467.84', 'h': '26.49', 'w': '10.91'}], [{'page': '14', 'x': '101.88', 'y': '467.84', 'h': '268.33', 'w': '10.91'}, {'page': '14', 'x': '370.20', 'y': '466.08', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '377.52', 'y': '467.60', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '389.16', 'y': '467.84', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '395.52', 'y': '467.84', 'h': '35.92', 'w': '11.49'}, {'page': '14', 'x': '432.00', 'y': '466.08', 'h': '2.30', 'w': '7.97'}, {'page': '14', 'x': '439.20', 'y': '467.60', 'h': '7.27', 'w': '18.89'}, {'page': '14', 'x': '450.96', 'y': '467.84', 'h': '6.37', 'w': '10.91'}, {'page': '14', 'x': '457.32', 'y': '468.65', 'h': '65.68', 'w': '10.68'}, {'page': '14', 'x': '72.00', 'y': '481.40', 'h': '259.17', 'w': '10.91'}], [{'page': '14', 'x': '334.92', 'y': '481.40', 'h': '188.07', 'w': '10.91'}, {'page': '14', 'x': '72.00', 'y': '495.77', 'h': '117.15', 'w': '9.75'}]]","text":"Assume that the algorithm returns yes because we found a matching M .Let M 2 be the matching M restricted to all agents which are not of one of the two dummy types.We first prove that M 2 is always stable.As dummy types appear only at the end of the preferences of each agent, it is sufficient to argue that M is stable.Assume for the sake of contradiction that the returned matching M is blocked by {m, w}, where m \u2208 A \u03b1 and w \u2208 A \u03b2 for some \u03b1 \u2208 T U and \u03b2 \u2208 T W .Let \u03b2 \u2032 \u2208 T W be the type of woman M (m) and \u03b1 \u2032 \u2208 T U the type of man M (w).Then, G contains edges {\u03b1, \u03b2 \u2032 } and {\u03b1 \u2032 , \u03b2} and as {m, w} blocks M it holds \u03b2 \u227b \u03b1 \u03b2 \u2032 and \u03b1 \u227b \u03b2 \u03b1 \u2032 .Thus, G was rejected, a contradiction.Further, as M is of weight at least It remains to argue why the algorithm always finds a solution if one exists.Let M 2 be a stable matching in P 2 with |M 1 \u25b3M 2 | \u2264 k.Let M be the matching M 2 where we match all agents that are unmatched in M 2 to agents from the dummy types.Further, let G \u2032 be the graph on T U \u222a \u2022 T W where \u03b1 \u2208 T U and \u03b2 \u2208 T W are connected if and only if an agent of type \u03b1 is matched to an agent of type \u03b2 in M .If the graph G would have been rejected because of types \u03b1 \u2032 \u2208 T U and \u03b2 \u2032 \u2208 T W , then agents of these types form a blocking pair for M , a contradiction.Moreover, matching M is clearly a perfect matching of weight at least","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2112.05777.pdf","section_number":"None","para":"13","_id":"62038305-7fcf-4656-9669-256cdc6eb51d","_collection_name":"Papers"},"page_content":"Assume that the algorithm returns yes because we found a matching M .Let M 2 be the matching M restricted to all agents which are not of one of the two dummy types.We first prove that M 2 is always stable.As dummy types appear only at the end of the preferences of each agent, it is sufficient to argue that M is stable.Assume for the sake of contradiction that the returned matching M is blocked by {m, w}, where m \u2208 A \u03b1 and w \u2208 A \u03b2 for some \u03b1 \u2208 T U and \u03b2 \u2208 T W .Let \u03b2 \u2032 \u2208 T W be the type of woman M (m) and \u03b1 \u2032 \u2208 T U the type of man M (w).Then, G contains edges {\u03b1, \u03b2 \u2032 } and {\u03b1 \u2032 , \u03b2} and as {m, w} blocks M it holds \u03b2 \u227b \u03b1 \u03b2 \u2032 and \u03b1 \u227b \u03b2 \u03b1 \u2032 .Thus, G was rejected, a contradiction.Further, as M is of weight at least It remains to argue why the algorithm always finds a solution if one exists.Let M 2 be a stable matching in P 2 with |M 1 \u25b3M 2 | \u2264 k.Let M be the matching M 2 where we match all agents that are unmatched in M 2 to agents from the dummy types.Further, let G \u2032 be the graph on T U \u222a \u2022 T W where \u03b1 \u2208 T U and \u03b2 \u2208 T W are connected if and only if an agent of type \u03b1 is matched to an agent of type \u03b2 in M .If the graph G would have been rejected because of types \u03b1 \u2032 \u2208 T U and \u03b2 \u2032 \u2208 T W , then agents of these types form a blocking pair for M , a contradiction.Moreover, matching M is clearly a perfect matching of weight at least","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We should also point out that the selected subset of games depends both on the original complete set of games and also the set of algorithms studied.Having a different set of agents could mean different games, that might previously have been too hard, would suddenly be more discriminatory.Similarly, adding additional games can affect which games provide us with redundant information.Because of this, while the specific games identified here are interesting today, they might very well change in the future.It is therefore meaningless to ask for the \"true\" set of most discriminatory games, as that will always be relative to the agents available.However, we believe the more important contribution of this paper is the methodology we propose to select these games.","paper_title":"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking","section_title":"A. Considerations","bboxes":"[[{'page': '7', 'x': '321.94', 'y': '614.32', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '626.27', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '638.23', 'h': '118.71', 'w': '8.64'}], [{'page': '7', 'x': '434.01', 'y': '638.23', 'h': '129.03', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '650.18', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '662.14', 'h': '208.04', 'w': '8.64'}], [{'page': '7', 'x': '524.10', 'y': '662.14', 'h': '38.93', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '674.09', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '686.05', 'h': '112.30', 'w': '8.64'}], [{'page': '7', 'x': '426.89', 'y': '686.05', 'h': '136.15', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '698.00', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '709.96', 'h': '102.09', 'w': '8.64'}], [{'page': '7', 'x': '416.68', 'y': '709.96', 'h': '146.35', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '52.42', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '64.38', 'h': '143.75', 'w': '8.64'}], [{'page': '8', 'x': '197.01', 'y': '64.38', 'h': '103.01', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '76.34', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '88.29', 'h': '139.40', 'w': '8.64'}]]","pages":"('7', '8')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1809.02904.pdf","section_number":"None","para":"5","_id":"7991edf4-e5d8-466f-ba1d-e72acdc228fb","_collection_name":"Papers"},"page_content":"We should also point out that the selected subset of games depends both on the original complete set of games and also the set of algorithms studied.Having a different set of agents could mean different games, that might previously have been too hard, would suddenly be more discriminatory.Similarly, adding additional games can affect which games provide us with redundant information.Because of this, while the specific games identified here are interesting today, they might very well change in the future.It is therefore meaningless to ask for the \"true\" set of most discriminatory games, as that will always be relative to the agents available.However, we believe the more important contribution of this paper is the methodology we propose to select these games.","type":"Document"}],1.2765831947,62,10.7,13.2,5,"The output comprehensively answers the question about near-perfect agents in ConnectFour, detailing their differences and how they operate based on the provided context. It effectively utilizes the context to explain the mechanisms of the agents, making it highly relevant and informative."],["What is the purpose of the General Board Game (GBG) learning and playing framework in the field of education and research in AI?"," The purpose of the General Board Game (GBG) learning and playing framework is to allow for easy application of new algorithms to a variety of games, serving as a testbed for spawning innovative AI techniques in domains such as search, automated theorem proving, planning, and learning. It also allows for the evaluation of the capabilities of intermediate models during the evolution of large models, helping to design and adjust training strategies and reduce the expenses of training.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"II. ALGORITHMS AND METHODS","bboxes":"[[{'page': '1', 'x': '321.94', 'y': '641.49', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '653.44', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '665.40', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '677.35', 'h': '13.00', 'w': '8.64'}], [{'page': '1', 'x': '327.61', 'y': '677.35', 'h': '235.43', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '689.31', 'h': '39.68', 'w': '8.64'}], [{'page': '1', 'x': '355.15', 'y': '689.31', 'h': '187.21', 'w': '8.64'}, {'page': '1', 'x': '542.35', 'y': '687.64', 'h': '3.49', 'w': '6.05'}, {'page': '1', 'x': '546.34', 'y': '689.31', 'h': '2.49', 'w': '8.64'}]]","text":"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"2","_id":"e19a113d-e501-4f37-b4db-dc80fe890142","_collection_name":"Papers"},"page_content":"The algorithm presented in this paper is implemented in the General Board Game (GBG) learning and playing framework [7], [8], which was developed for education and research in AI.GBG allows applying the new algorithm easily to a variety of games.GBG is open source and available on GitHub 2 .","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '2')","paper_title":"Expert-Driven Genetic Algorithms for Simulating Evaluation Functions \u22c6","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '87.00', 'y': '146.02', 'h': '330.67', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '157.42', 'h': '94.11', 'w': '8.97'}], [{'page': '2', 'x': '169.44', 'y': '157.42', 'h': '248.26', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '168.94', 'h': '345.51', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '180.34', 'h': '277.72', 'w': '8.97'}], [{'page': '2', 'x': '353.40', 'y': '180.34', 'h': '64.34', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '191.86', 'h': '345.64', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '203.26', 'h': '345.55', 'w': '8.97'}, {'page': '2', 'x': '72.00', 'y': '214.78', 'h': '321.28', 'w': '8.97'}]]","text":"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1711.06841.pdf","section_number":"1","para":"2","_id":"c6addab5-dc64-4994-9fd1-00b986893d5d","_collection_name":"Papers"},"page_content":"The ongoing key role played by and the impact of computer games on AI should not be underestimated.If nothing else, computer games have served as an important testbed for spawning various innovative AI techniques in domains and applications such as search, automated theorem proving, planning, and learning.In addition, the annual World Computer Chess Championship (WCCC) is arguably the longest ongoing performance evaluation of programs in computer science, which has inspired other wellknown competitions in robotics, planning, and natural language understanding.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.","paper_title":"Integration of cognitive tasks into artificial general intelligence test for large models","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '106.10', 'y': '88.70', 'h': '421.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '105.62', 'h': '185.51', 'w': '9.60'}, {'page': '2', 'x': '270.65', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '279.05', 'y': '105.62', 'h': '55.60', 'w': '9.60'}, {'page': '2', 'x': '334.63', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '348.31', 'y': '105.62', 'h': '34.09', 'w': '9.60'}, {'page': '2', 'x': '382.39', 'y': '104.16', 'h': '3.48', 'w': '6.33'}, {'page': '2', 'x': '390.79', 'y': '105.62', 'h': '34.69', 'w': '9.60'}, {'page': '2', 'x': '425.50', 'y': '104.16', 'h': '8.76', 'w': '6.33'}, {'page': '2', 'x': '439.06', 'y': '105.62', 'h': '47.29', 'w': '9.60'}, {'page': '2', 'x': '486.34', 'y': '104.16', 'h': '10.44', 'w': '6.33'}, {'page': '2', 'x': '501.70', 'y': '105.62', 'h': '25.80', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '121.82', 'h': '276.08', 'w': '9.60'}], [{'page': '2', 'x': '364.17', 'y': '121.82', 'h': '163.01', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '138.02', 'h': '442.30', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '154.82', 'h': '100.58', 'w': '9.60'}, {'page': '2', 'x': '185.69', 'y': '153.36', 'h': '15.72', 'w': '6.33'}], [{'page': '2', 'x': '203.09', 'y': '154.82', 'h': '324.18', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '171.74', 'h': '322.27', 'w': '9.60'}, {'page': '2', 'x': '407.47', 'y': '170.28', 'h': '6.96', 'w': '6.33'}], [{'page': '2', 'x': '418.78', 'y': '171.74', 'h': '108.71', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '187.82', 'h': '414.70', 'w': '9.60'}], [{'page': '2', 'x': '502.65', 'y': '187.82', 'h': '24.63', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '204.02', 'h': '442.26', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '220.94', 'h': '57.59', 'w': '9.60'}, {'page': '2', 'x': '142.70', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '151.58', 'y': '220.94', 'h': '159.32', 'w': '9.60'}, {'page': '2', 'x': '310.99', 'y': '219.48', 'h': '6.96', 'w': '6.33'}, {'page': '2', 'x': '319.75', 'y': '220.94', 'h': '207.62', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '237.02', 'h': '442.02', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '253.25', 'h': '128.26', 'w': '9.60'}], [{'page': '2', 'x': '217.73', 'y': '253.25', 'h': '309.57', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '269.33', 'h': '240.41', 'w': '9.60'}], [{'page': '2', 'x': '329.07', 'y': '269.33', 'h': '198.16', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '285.53', 'h': '442.25', 'w': '9.60'}, {'page': '2', 'x': '85.10', 'y': '301.73', 'h': '110.33', 'w': '9.60'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2402.02547.pdf","section_number":"None","para":"6","_id":"df806d2c-c726-4362-a3ac-6afa7ec64d08","_collection_name":"Papers"},"page_content":"Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Motivation","bboxes":"[[{'page': '1', 'x': '58.93', 'y': '500.05', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '512.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '523.96', 'h': '59.12', 'w': '8.64'}], [{'page': '1', 'x': '113.25', 'y': '523.96', 'h': '186.77', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '535.92', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '547.87', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '559.83', 'h': '67.72', 'w': '8.64'}], [{'page': '1', 'x': '120.49', 'y': '559.83', 'h': '179.54', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '571.78', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '583.74', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '595.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '607.65', 'h': '32.38', 'w': '8.64'}]]","pages":"('1', '1')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"2","_id":"d2503d91-b1d2-4d61-998d-0a003066fd74","_collection_name":"Papers"},"page_content":"In computer science, game learning and game playing are interesting test beds for strategic decision making done by computers.Games usually have large state spaces, and they often require complex pattern recognition and strategic planning capabilities to decide which move is the best in a certain situation.If an algorithm is able to learn a game (or, even better, a variety of different games) just by self-play, given no other knowledge than the game rules, it is likely to perform also well on other problems of strategic decision making.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '3')","paper_title":"The Neural MMO Platform for Massively Multiagent Research","section_title":"Configuration","bboxes":"[[{'page': '2', 'x': '108.00', 'y': '636.76', 'h': '396.00', 'w': '9.03'}, {'page': '2', 'x': '108.00', 'y': '648.06', 'h': '240.75', 'w': '8.64'}], [{'page': '2', 'x': '351.86', 'y': '648.06', 'h': '152.14', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '658.97', 'h': '396.17', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '669.88', 'h': '69.14', 'w': '8.64'}], [{'page': '2', 'x': '180.22', 'y': '669.88', 'h': '323.78', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '680.79', 'h': '234.15', 'w': '8.64'}], [{'page': '2', 'x': '345.26', 'y': '680.79', 'h': '158.73', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '691.70', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '702.61', 'h': '396.00', 'w': '8.64'}, {'page': '2', 'x': '108.00', 'y': '713.51', 'h': '163.51', 'w': '8.64'}], [{'page': '3', 'x': '108.00', 'y': '255.32', 'h': '395.99', 'w': '9.03'}, {'page': '3', 'x': '108.00', 'y': '266.62', 'h': '314.19', 'w': '8.64'}], [{'page': '3', 'x': '425.14', 'y': '266.62', 'h': '78.86', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '277.35', 'h': '396.00', 'w': '8.82'}, {'page': '3', 'x': '108.00', 'y': '288.44', 'h': '267.92', 'w': '8.64'}], [{'page': '3', 'x': '379.03', 'y': '288.44', 'h': '124.97', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '299.35', 'h': '396.00', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '310.26', 'h': '130.97', 'w': '8.64'}], [{'page': '3', 'x': '242.05', 'y': '310.26', 'h': '261.94', 'w': '8.64'}, {'page': '3', 'x': '108.00', 'y': '321.16', 'h': '186.52', 'w': '8.64'}]]","text":"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2110.07594.pdf","section_number":"2.1","para":"7","_id":"91d435c1-be88-4fde-8e9f-263d78c2bb0c","_collection_name":"Papers"},"page_content":"Modular Game Systems: The Resource, Combat, Progression, and Equipment & NPC systems are bundles of content and mechanics that define gameplay.We designed each system to engage a specific modality of intelligence, but optimal play typically requires simultaneous reasoning over multiple systems.Users write simple config files to enable and customize the game systems relevant to their application and specify map generation parameters.For example, enabling only the resource system creates environments well-suited to classic artificial life problems including foraging and exploration; in contrast, enabling the combat system creates more obvious conflicts well-suited to team-based play and ad-hoc cooperation.Procedural Generation: Recent works have demonstrated the effectiveness of procedural content generation (PCG) for domain randomization in increasing policy robustness [20].We have reproduced this result in Neural MMO (see Experiments) and provide a PCG module to create distributions of training and evaluation environments rather than a single game map.The algorithm we use is a novel generalization of standard multi-octave noise that varies generation parameters at different points in space to increase visual diversity.All terrain generation parameters are configurable, and we provide full details of the algorithm in the Supplement.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"General Board Geometry","section_title":"Introduction","bboxes":"[[{'page': '1', 'x': '149.71', 'y': '532.90', 'h': '330.88', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '544.86', 'h': '188.03', 'w': '8.74'}], [{'page': '1', 'x': '326.23', 'y': '544.86', 'h': '154.36', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '556.81', 'h': '282.20', 'w': '8.74'}]]","text":"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2111.11329.pdf","section_number":"1","para":"1","_id":"e346bc45-87af-4bb6-a81b-44970631f61c","_collection_name":"Papers"},"page_content":"A key challenge in this task is to allow the user to describe arbitrarily complex game boards in a simple and intuitive way.This paper outlines our method for describing game boards in the Ludii grammar for general games.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Related work","bboxes":"[[{'page': '9', 'x': '321.94', 'y': '408.22', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '420.18', 'h': '43.60', 'w': '8.64'}], [{'page': '9', 'x': '358.49', 'y': '420.18', 'h': '204.54', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '432.13', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '444.09', 'h': '194.64', 'w': '8.64'}], [{'page': '9', 'x': '509.84', 'y': '444.09', 'h': '53.20', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '456.04', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '468.00', 'h': '96.88', 'w': '8.64'}], [{'page': '9', 'x': '413.29', 'y': '468.00', 'h': '149.75', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '479.96', 'h': '238.97', 'w': '8.64'}], [{'page': '9', 'x': '321.94', 'y': '492.69', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '504.65', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '516.60', 'h': '61.95', 'w': '8.64'}], [{'page': '9', 'x': '376.59', 'y': '516.60', 'h': '186.45', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '528.56', 'h': '227.83', 'w': '8.64'}], [{'page': '9', 'x': '543.51', 'y': '528.56', 'h': '19.53', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '540.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '552.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '564.42', 'h': '205.95', 'w': '8.64'}]]","pages":"('9', '9')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"6","_id":"263f8343-a94d-4f8d-baac-90b85b2b33e7","_collection_name":"Papers"},"page_content":"A work related to GBG [7], [8] is the general game system Ludii [37].Ludii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games.Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta).GBG, on the other hand, offers the possibility to train RL-based algorithms on several games.Soemers et al. [38] describe a bridge between Ludii [37] and Polygames [39], the latter providing DNN algorithms for strategy games.Similar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.With 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('8', '8')","paper_title":"A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking","section_title":"B. Future Work","bboxes":"[[{'page': '8', 'x': '58.93', 'y': '126.29', 'h': '241.09', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '138.24', 'h': '251.06', 'w': '8.64'}], [{'page': '8', 'x': '48.96', 'y': '150.20', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '162.15', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '174.11', 'h': '84.00', 'w': '8.64'}], [{'page': '8', 'x': '136.91', 'y': '174.11', 'h': '163.11', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '186.06', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '198.02', 'h': '201.19', 'w': '8.64'}], [{'page': '8', 'x': '253.30', 'y': '198.02', 'h': '46.73', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '209.97', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '221.93', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '233.88', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '245.84', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '257.79', 'h': '136.47', 'w': '8.64'}], [{'page': '8', 'x': '189.50', 'y': '257.79', 'h': '110.53', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '269.75', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '281.70', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '293.66', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '305.61', 'h': '251.06', 'w': '8.64'}, {'page': '8', 'x': '48.96', 'y': '317.57', 'h': '142.72', 'w': '8.64'}]]","text":"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1809.02904.pdf","section_number":"None","para":"4","_id":"dea85849-9565-44f1-8f1e-6bc4b7f2b505","_collection_name":"Papers"},"page_content":"While this method has been applied here on GVGAI games, it could also be used for other sets of problems and algorithms.Our approach can be generalized from just win-rates and scores to include any number of different outcome measures from other domains.This could include problems outside of the traditional game space, as long as the mean and variance of each algorithm's performance can be obtained.An obvious future application would be to analyze the performance of multiple deep reinforcement learning algorithms on the Atari games in the Arcade Learning Environment (ALE) framework and supervised learning algorithms tested on datasets associated with Kaggle competitions.Regarding our specific use case of applications for GVGAI, future work could involve expanding the evaluation criteria to include additional data from agent playthroughs, such as the time required to solve a level or the number of moves used, which may help us to better differentiate between agents.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.","paper_title":"Potential-based Reward Shaping in Sokoban","section_title":"Sokoban","bboxes":"[[{'page': '3', 'x': '134.77', 'y': '512.51', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '524.46', 'h': '167.14', 'w': '8.74'}], [{'page': '3', 'x': '305.03', 'y': '524.46', 'h': '175.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '536.42', 'h': '86.55', 'w': '8.74'}], [{'page': '3', 'x': '224.84', 'y': '536.42', 'h': '197.65', 'w': '8.74'}], [{'page': '3', 'x': '426.01', 'y': '536.42', 'h': '54.58', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '548.37', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '560.33', 'h': '70.24', 'w': '8.74'}], [{'page': '3', 'x': '208.21', 'y': '560.33', 'h': '272.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '572.28', 'h': '53.32', 'w': '8.74'}], [{'page': '3', 'x': '192.65', 'y': '572.28', 'h': '287.94', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '584.24', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '596.19', 'h': '139.61', 'w': '8.74'}]]","pages":"('3', '3')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2109.05022.pdf","section_number":"2.2","para":"5","_id":"bf6d7428-caa3-4aa1-80b5-d330762f3de5","_collection_name":"Papers"},"page_content":"Sokoban is a challenging puzzle game and has been proved to be PSPACEcomplete [3] and NP-hard [4] problem.It also plays an important role in benchmarking RL agents.Many models are proposed to solve Sokoban.Both modelbased methods [9,10,21], as well as model-free methods can reach competitive performance [8].Curriculum learning has been used to solve a difficult Sokoban instance [6].The works mentioned above try to solve Sokoban using specialdesigned models, while we are focusing on using general reward shaping techniques to speed up the learning.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].","paper_title":"Typed Graph Networks","section_title":"I. INTRODUCTION","bboxes":"[[{'page': '1', 'x': '58.93', 'y': '515.50', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '527.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '48.96', 'y': '539.41', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '551.37', 'h': '252.71', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '563.00', 'h': '251.06', 'w': '8.96'}, {'page': '1', 'x': '48.96', 'y': '575.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '587.23', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '599.19', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '611.14', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '623.10', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '635.05', 'h': '30.74', 'w': '8.64'}], [{'page': '1', 'x': '83.17', 'y': '635.05', 'h': '216.85', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '647.01', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '153.59', 'h': '142.90', 'w': '8.64'}], [{'page': '1', 'x': '458.01', 'y': '153.59', 'h': '105.38', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '165.55', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '177.50', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '189.46', 'h': '252.80', 'w': '8.64'}], [{'page': '1', 'x': '311.51', 'y': '201.41', 'h': '251.53', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '213.37', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.33', 'h': '251.23', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.28', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.24', 'h': '251.05', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.19', 'h': '140.93', 'w': '8.64'}], [{'page': '1', 'x': '456.08', 'y': '261.19', 'h': '106.96', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.15', 'h': '251.40', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.10', 'h': '136.56', 'w': '8.64'}]]","pages":"('1', '1')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1901.07984.pdf","section_number":"None","para":"5","_id":"6aba2813-4b28-48fe-95fb-95e48ddd7549","_collection_name":"Papers"},"page_content":"Machine learning in general and deep learning (DL) in particular have sported significant advances in the last decade.Deep convolutional networks have consistently pushed the stateof-the-art in image classification [1], [2]; neural implementations of the Q-function [3] allowed for effectively training reinforcement learning agents on huge combinatorial state spaces such as the pixels of Atari games [4], [5] and the marriage between symbolic tree search and DL evaluation functions has yielded the mastering of the games of Go and chess, a longstanding testbed for artificial intelligence research [6]- [8].Machine learning has been successful against human champions by training itself to superhuman levels in Chess and Shogi solely from self-play [8].Insights from game theory have allowed DL models to transcend classification tasks and produce generative inputs, with Generative Adversarial Neural Networks rapidly nearing photo-realistic results [9].With an ever increasing array of diverse complex scenarios being successfully projected onto the continuous landscapes of DL models' parameter spaces, a new and fundamental frontier in AI research leads to the marriage between DL and the discrete, relational realms up until very recently reserved to the long-parted branch of symbolic AI.Empowering deep learning to tackle combinatorial generalisation is now seen as a key path forward in AI research [10].","type":"Document"}],1.7904164791,88,19.5,22.9,5,"The output comprehensively addresses the purpose of the General Board Game (GBG) learning and playing framework, detailing its application in AI research and education, as well as its role in evaluating algorithms and training strategies. This aligns perfectly with the input question and is well-supported by the provided context."],["What is the significance of subadditivity in determining the maximum for a complex linear functional?"," Subadditivity is a property that allows us to determine the maximum value of a functional by considering smaller subproblems. In the context given, it is used to show that the maximum for the functional is given by extremal functions, which are solutions to smaller subproblems. This allows for a more efficient and systematic approach to finding the maximum value.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We see that L is a complex linear functional such that |L| is subadditive and thus, the maximum for the functional is given by","paper_title":"CONCAVE UNIVALENT FUNCTIONS AND DIRICHLET FINITE INTEGRAL","section_title":"Properties of the families F \u03b1 and CO(\u03b1)","bboxes":"[[{'page': '6', 'x': '368.16', 'y': '247.45', 'h': '128.94', 'w': '11.97'}, {'page': '6', 'x': '72.00', 'y': '261.61', 'h': '162.58', 'w': '11.97'}, {'page': '6', 'x': '239.04', 'y': '261.61', 'h': '14.59', 'w': '20.44'}, {'page': '6', 'x': '258.12', 'y': '261.61', 'h': '239.08', 'w': '11.97'}, {'page': '6', 'x': '72.00', 'y': '275.53', 'h': '109.61', 'w': '11.97'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1511.08300.pdf","section_number":"3.","para":"0","_id":"93976c3c-b31e-4744-a2c0-4129da690751","_collection_name":"Papers"},"page_content":"We see that L is a complex linear functional such that |L| is subadditive and thus, the maximum for the functional is given by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We begin by reviewing known violations of strong subadditivity (SSA) for HRT areas in the presence of a finite radius cutoff.In each case, we will see that SSA is nevertheless satisfied by restricted maximin areas.We then establish notation for the remainder of the paper, formally define our restricted maximin surfaces, and review useful results from [26] regarding null congruences that touch at a point.","paper_title":"Radial Cutoffs and Holographic Entanglement","section_title":"Preliminaries","bboxes":"[[{'page': '6', 'x': '83.34', 'y': '113.34', 'h': '428.60', 'w': '9.63'}, {'page': '6', 'x': '83.34', 'y': '128.25', 'h': '179.34', 'w': '9.63'}], [{'page': '6', 'x': '270.31', 'y': '128.25', 'h': '241.62', 'w': '9.63'}, {'page': '6', 'x': '83.34', 'y': '143.15', 'h': '177.46', 'w': '9.63'}], [{'page': '6', 'x': '265.87', 'y': '143.15', 'h': '246.06', 'w': '9.63'}, {'page': '6', 'x': '83.34', 'y': '158.05', 'h': '428.60', 'w': '9.63'}, {'page': '6', 'x': '83.34', 'y': '172.96', 'h': '233.30', 'w': '9.63'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2008.07022.pdf","section_number":"2","para":"2","_id":"407b7325-76b1-4cb0-83f8-a2b0ef65a806","_collection_name":"Papers"},"page_content":"We begin by reviewing known violations of strong subadditivity (SSA) for HRT areas in the presence of a finite radius cutoff.In each case, we will see that SSA is nevertheless satisfied by restricted maximin areas.We then establish notation for the remainder of the paper, formally define our restricted maximin surfaces, and review useful results from [26] regarding null congruences that touch at a point.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"\u2022 In E-optimality, we minimize the largest eigenvalue of the error covariance matrix M S , \u03bb max , or equivalently maximize","paper_title":"Submodular Observation Selection and Information Gathering for Quadratic Models","section_title":"Submodular Observation Selection","bboxes":"[[{'page': '2', 'x': '317.40', 'y': '456.35', 'h': '224.04', 'w': '8.96'}, {'page': '2', 'x': '327.36', 'y': '468.27', 'h': '214.42', 'w': '9.68'}, {'page': '2', 'x': '327.36', 'y': '480.58', 'h': '39.29', 'w': '8.64'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1905.09919.pdf","section_number":"2.","para":"0","_id":"b41430be-a6ad-46c5-846c-aa5787233ebc","_collection_name":"Papers"},"page_content":"\u2022 In E-optimality, we minimize the largest eigenvalue of the error covariance matrix M S , \u03bb max , or equivalently maximize","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('14', '14')","paper_title":"An ADMM-Newton-CNN Numerical Approach to a TV Model for Identifying Discontinuous Diffusion Coefficients in Elliptic Equations: Convex Case with Gradient Observations","section_title":"11:","bboxes":"[[{'page': '14', 'x': '56.64', 'y': '698.62', 'h': '498.39', 'w': '10.91'}, {'page': '14', 'x': '56.64', 'y': '711.37', 'h': '260.73', 'w': '12.09'}], [{'page': '14', 'x': '321.60', 'y': '713.63', 'h': '194.25', 'w': '9.73'}], [{'page': '14', 'x': '520.44', 'y': '713.63', 'h': '34.84', 'w': '9.73'}, {'page': '14', 'x': '56.64', 'y': '727.91', 'h': '256.65', 'w': '9.73'}]]","text":"Remark 3. Since the convexity of the functional J(\u2022) is kept, the active-set Newton method summarized as Subroutine 3 is guaranteed to be convergent to a solution of (3.2).We refer to, e.g., [22], for rigorous analysis.Also, as analyzed in [22], Subroutine 3 is superlinearly convergent.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2103.11176.pdf","section_number":"None","para":"2","_id":"e2873708-d027-4874-a81c-0bf7bc134152","_collection_name":"Papers"},"page_content":"Remark 3. Since the convexity of the functional J(\u2022) is kept, the active-set Newton method summarized as Subroutine 3 is guaranteed to be convergent to a solution of (3.2).We refer to, e.g., [22], for rigorous analysis.Also, as analyzed in [22], Subroutine 3 is superlinearly convergent.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('12', '12')","paper_title":"Affine Gateaux Differentials and the von Mises Statistical Calculus","section_title":"Affine calculus","bboxes":"[[{'page': '12', 'x': '72.72', 'y': '233.49', 'h': '424.61', 'w': '9.96'}, {'page': '12', 'x': '57.84', 'y': '247.89', 'h': '217.83', 'w': '9.96'}, {'page': '12', 'x': '278.64', 'y': '247.64', 'h': '7.74', 'w': '17.29'}, {'page': '12', 'x': '288.60', 'y': '247.89', 'h': '9.51', 'w': '9.96'}]]","text":"At non-internal points of C the subdifferential of a convex function can thus be strictly larger than its a-differential because the vector space (Cx)","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2403.07827.pdf","section_number":"3.4","para":"0","_id":"f7d67e29-7d0f-40c3-91f1-29f05ae9f26d","_collection_name":"Papers"},"page_content":"At non-internal points of C the subdifferential of a convex function can thus be strictly larger than its a-differential because the vector space (Cx)","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Proof.Since the maxmin problem (67) is standard in the literature (see, for example, [53]), we only demonstrate its main steps briefly.To this end, for each \u03c0 \u2208 \u03a0 [0,T ] and u \u2208 U [0,T ] , we introduce the objective functional","paper_title":"A game theoretical approach to homothetic robust forward investment performance processes in stochastic factor models","section_title":"lim","bboxes":"[[{'page': '30', 'x': '133.80', 'y': '214.81', 'h': '31.02', 'w': '8.95'}], [{'page': '30', 'x': '170.88', 'y': '214.29', 'h': '306.65', 'w': '9.96'}, {'page': '30', 'x': '133.80', 'y': '226.29', 'h': '250.92', 'w': '9.96'}], [{'page': '30', 'x': '389.04', 'y': '226.29', 'h': '88.57', 'w': '9.96'}, {'page': '30', 'x': '133.80', 'y': '238.17', 'h': '25.71', 'w': '9.96'}, {'page': '30', 'x': '159.48', 'y': '242.21', 'h': '16.65', 'w': '6.97'}, {'page': '30', 'x': '179.88', 'y': '238.17', 'h': '43.55', 'w': '9.96'}, {'page': '30', 'x': '223.44', 'y': '242.21', 'h': '16.65', 'w': '6.97'}, {'page': '30', 'x': '240.48', 'y': '238.17', 'h': '168.00', 'w': '9.96'}]]","pages":"('30', '30')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2005.10660.pdf","section_number":"None","para":"2","_id":"d7869735-3b31-44c1-8657-13d2c4fd10e6","_collection_name":"Papers"},"page_content":"Proof.Since the maxmin problem (67) is standard in the literature (see, for example, [53]), we only demonstrate its main steps briefly.To this end, for each \u03c0 \u2208 \u03a0 [0,T ] and u \u2208 U [0,T ] , we introduce the objective functional","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We will refer to (5.4), which can be proved by induction (using the fact that (a + b) \u03ba \u2264 a \u03ba + b \u03ba ), as the subadditivity property; note that it obviously extends to stochastic integrals with respect to discrete measures.Moreover, note that we have","paper_title":"THE STOCHASTIC HEAT EQUATION WITH MULTIPLICATIVE L\u00c9VY NOISE: EXISTENCE, MOMENTS, AND INTERMITTENCY","section_title":"Existence of Lyapunov exponents.","bboxes":"[[{'page': '19', 'x': '72.00', 'y': '612.82', 'h': '402.76', 'w': '10.91'}, {'page': '19', 'x': '474.72', 'y': '611.06', 'h': '4.88', 'w': '7.97'}, {'page': '19', 'x': '483.24', 'y': '612.58', 'h': '8.49', 'w': '18.89'}, {'page': '19', 'x': '495.00', 'y': '612.82', 'h': '5.77', 'w': '10.91'}, {'page': '19', 'x': '500.76', 'y': '611.06', 'h': '4.88', 'w': '7.97'}, {'page': '19', 'x': '508.56', 'y': '612.82', 'h': '15.72', 'w': '10.91'}, {'page': '19', 'x': '524.28', 'y': '611.06', 'h': '4.88', 'w': '7.97'}, {'page': '19', 'x': '529.56', 'y': '612.82', 'h': '7.23', 'w': '10.91'}, {'page': '19', 'x': '72.00', 'y': '625.78', 'h': '464.81', 'w': '10.91'}, {'page': '19', 'x': '72.00', 'y': '638.74', 'h': '86.29', 'w': '10.91'}], [{'page': '19', 'x': '84.00', 'y': '651.70', 'h': '136.26', 'w': '10.91'}]]","pages":"('19', '19')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2111.07988.pdf","section_number":"5.1.","para":"1","_id":"fbabd843-e034-406a-8281-958f22b5bae4","_collection_name":"Papers"},"page_content":"We will refer to (5.4), which can be proved by induction (using the fact that (a + b) \u03ba \u2264 a \u03ba + b \u03ba ), as the subadditivity property; note that it obviously extends to stochastic integrals with respect to discrete measures.Moreover, note that we have","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Expressing that the functional derivative of F (f ) with respect to f (X) vanishes (maximum of F (f )), one obtains","paper_title":"The impact of incorrect social information on collective wisdom in human groups","section_title":"Cauchy and Laplace distributions","bboxes":"[[{'page': '31', 'x': '70.76', 'y': '161.13', 'h': '470.49', 'w': '11.97'}, {'page': '31', 'x': '70.76', 'y': '185.05', 'h': '109.04', 'w': '11.97'}]]","pages":"('31', '31')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2003.06160.pdf","section_number":"None","para":"0","_id":"9341a1a7-a0a7-4163-8494-0b24c1580d17","_collection_name":"Papers"},"page_content":"Expressing that the functional derivative of F (f ) with respect to f (X) vanishes (maximum of F (f )), one obtains","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('11', '12')","paper_title":"Power Control for User Cooperation *","section_title":"Rate Maximization via Subgradient Methods","bboxes":"[[{'page': '11', 'x': '72.00', 'y': '686.07', 'h': '45.38', 'w': '11.97'}, {'page': '11', 'x': '117.36', 'y': '691.14', 'h': '5.15', 'w': '7.32'}, {'page': '11', 'x': '126.96', 'y': '686.07', 'h': '255.60', 'w': '11.97'}, {'page': '11', 'x': '386.52', 'y': '686.07', 'h': '27.72', 'w': '20.44'}, {'page': '11', 'x': '416.28', 'y': '686.07', 'h': '123.73', 'w': '11.97'}, {'page': '11', 'x': '72.00', 'y': '711.15', 'h': '184.32', 'w': '11.97'}], [{'page': '11', 'x': '264.72', 'y': '711.15', 'h': '275.32', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '65.07', 'h': '467.89', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '90.15', 'h': '216.96', 'w': '11.97'}], [{'page': '12', 'x': '294.12', 'y': '90.15', 'h': '60.27', 'w': '11.97'}, {'page': '12', 'x': '354.36', 'y': '97.32', 'h': '6.36', 'w': '5.28'}, {'page': '12', 'x': '364.44', 'y': '90.15', 'h': '175.24', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '115.23', 'h': '467.93', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '140.19', 'h': '291.72', 'w': '11.97'}], [{'page': '12', 'x': '370.08', 'y': '140.19', 'h': '169.81', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '165.27', 'h': '468.00', 'w': '11.97'}, {'page': '12', 'x': '72.00', 'y': '190.35', 'h': '18.24', 'w': '11.97'}]]","text":"where, E S denotes the expectation over the event that case S \u2282 {1, 2, 3, 4} from Proposition 1 occurs, and A is as given by (9).Note that the objective function is concave, and the constraint set is convex, therefore we conclude that any local optimum for the constrained optimization problem is a global optimum.Although R \u00b5 is differentiable almost everywhere since it is concave, its optimal value is attained along the discontinuity of its gradient, namely when the two arguments of the min operation are equal.Hence, we solve the optimization problem using the method of subgradients from non-differentiable optimization theory [14,15].","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/cs_0601124.pdf","section_number":"4","para":"3","_id":"8d953864-64f5-4cc8-b4d1-927245b2a951","_collection_name":"Papers"},"page_content":"where, E S denotes the expectation over the event that case S \u2282 {1, 2, 3, 4} from Proposition 1 occurs, and A is as given by (9).Note that the objective function is concave, and the constraint set is convex, therefore we conclude that any local optimum for the constrained optimization problem is a global optimum.Although R \u00b5 is differentiable almost everywhere since it is concave, its optimal value is attained along the discontinuity of its gradient, namely when the two arguments of the min operation are equal.Hence, we solve the optimization problem using the method of subgradients from non-differentiable optimization theory [14,15].","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('8', '8')","paper_title":"CONCAVE UNIVALENT FUNCTIONS AND DIRICHLET FINITE INTEGRAL","section_title":"Properties of the families F \u03b1 and CO(\u03b1)","bboxes":"[[{'page': '8', 'x': '292.92', 'y': '669.97', 'h': '204.22', 'w': '11.97'}, {'page': '8', 'x': '72.00', 'y': '684.01', 'h': '14.59', 'w': '20.44'}, {'page': '8', 'x': '90.48', 'y': '684.01', 'h': '131.98', 'w': '11.97'}, {'page': '8', 'x': '225.72', 'y': '684.01', 'h': '9.29', 'w': '20.44'}, {'page': '8', 'x': '238.32', 'y': '684.01', 'h': '36.84', 'w': '11.96'}], [{'page': '8', 'x': '278.88', 'y': '684.01', 'h': '142.05', 'w': '11.97'}, {'page': '8', 'x': '424.80', 'y': '684.01', 'h': '14.59', 'w': '20.44'}, {'page': '8', 'x': '443.28', 'y': '684.01', 'h': '53.89', 'w': '11.97'}, {'page': '8', 'x': '72.00', 'y': '697.93', 'h': '112.44', 'w': '11.97'}], [{'page': '8', 'x': '189.72', 'y': '697.93', 'h': '269.33', 'w': '11.97'}]]","text":"is a complex linear functional such that |L| is subadditive and CO(\u03b1) \u2282 CO (2).(Note that the maximum of |L| is attained at extremal function).From (7), extremal functions for CO(2) are given by","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1511.08300.pdf","section_number":"3.","para":"2","_id":"2f55e3a8-da5a-44e6-a7d3-be72722eef2b","_collection_name":"Papers"},"page_content":"is a complex linear functional such that |L| is subadditive and CO(\u03b1) \u2282 CO (2).(Note that the maximum of |L| is attained at extremal function).From (7), extremal functions for CO(2) are given by","type":"Document"}],3.164434433,69,13.3,12.9,4,"The output effectively explains the significance of subadditivity in determining the maximum for a complex linear functional, directly addressing the question posed in the input. It also aligns well with the provided context, discussing extremal functions and the efficiency of solving subproblems, which enhances its relevance. However, it could be more comprehensive by including specific examples or deeper insights into the implications of subadditivity."],["What approach is needed to extend the MCTS wrapper concept for nondeterministic games, based on previous research?"," The Expectimax approach.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. MCTS Wrapper","bboxes":"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"778dc672-a89f-4a58-a142-ba642132da6c","_collection_name":"Papers"},"page_content":"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('10', '10')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"VI. CONCLUSION AND FUTURE WORK","bboxes":"[[{'page': '10', 'x': '321.94', 'y': '625.25', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '637.21', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '649.16', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '661.12', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '673.07', 'h': '39.00', 'w': '8.64'}]]","text":"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"0","_id":"a0db5e7e-0642-4a7a-86ef-53633a90b914","_collection_name":"Papers"},"page_content":"We also plan to extend our MCTS wrapper concept to nondeterministic games (e. g., EWN, 2048, Blackjack, Poker) where previous research [43] has shown that plain MCTS is not sufficient and has to be extended by the Expectimax approach.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('7', '7')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"D. Computation times","bboxes":"[[{'page': '7', 'x': '321.94', 'y': '500.24', 'h': '241.09', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '512.20', 'h': '251.06', 'w': '8.64'}, {'page': '7', 'x': '311.98', 'y': '523.97', 'h': '251.06', 'w': '8.82'}, {'page': '7', 'x': '311.98', 'y': '536.11', 'h': '33.48', 'w': '8.64'}]]","text":"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"0","_id":"0443fa92-bb62-4e30-b65b-7cfa7a538da1","_collection_name":"Papers"},"page_content":"The MCTS wrapper for RL agents, as proposed in this paper, has the advantage that it does not cost any additional training time since it is an enhancement added after agent training.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '2')","paper_title":"Qualitative Analysis of VASS-Induced MDPs \u22c6","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '149.76', 'y': '204.69', 'h': '332.48', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '216.57', 'h': '345.92', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '228.57', 'h': '235.32', 'w': '9.96'}], [{'page': '2', 'x': '373.56', 'y': '228.57', 'h': '107.05', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '240.45', 'h': '347.59', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '252.45', 'h': '345.80', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '264.45', 'h': '105.00', 'w': '9.96'}], [{'page': '2', 'x': '243.48', 'y': '264.45', 'h': '238.76', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '276.33', 'h': '346.01', 'w': '9.96'}, {'page': '2', 'x': '134.40', 'y': '288.33', 'h': '188.52', 'w': '9.96'}], [{'page': '2', 'x': '325.56', 'y': '288.33', 'h': '156.68', 'w': '9.96'}, {'page': '2', 'x': '134.76', 'y': '300.33', 'h': '230.04', 'w': '9.96'}]]","text":"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1512.08824.pdf","section_number":"1","para":"3","_id":"990632c9-79dc-413c-8bed-752d72a7dba1","_collection_name":"Papers"},"page_content":"Most classical work on algorithms for MDPs and stochastic games has focused on finite-state systems (e.g., [14,19,11]), but more recently several classes of infinite-state systems have been considered as well.For instance, MDPs and stochastic games on infinite-state probabilistic recursive systems (i.e., probabilistic pushdown automata with unbounded stacks) [13] and on one-counter systems [7,6] have been studied.Another infinite-state probabilistic model, which is incomparable to recursive systems, is a suitable probabilistic extension of Vector Addition Systems with States (VASS; a.k.a.Petri nets), which have a finite number of unbounded counters holding natural numbers.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures","paper_title":"Cautious Belief and Iterated Admissibility *","section_title":"Our contribution","bboxes":"[[{'page': '6', 'x': '87.84', 'y': '482.60', 'h': '436.55', 'w': '10.91'}, {'page': '6', 'x': '70.92', 'y': '496.16', 'h': '188.54', 'w': '10.91'}, {'page': '6', 'x': '259.44', 'y': '494.40', 'h': '4.23', 'w': '7.97'}], [{'page': '6', 'x': '70.92', 'y': '528.62', 'h': '8.06', 'w': '13.64'}, {'page': '6', 'x': '95.04', 'y': '528.62', 'h': '388.79', 'w': '13.64'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2305.15330.pdf","section_number":"None","para":"1","_id":"6529f59b-311b-4064-bf50-48de3de15a90","_collection_name":"Papers"},"page_content":"A comprehensive analysis of the properties of SAS's in a wide class of games is given by Brandenburger and Friedenberg (2010). 8Lexicographic beliefs and lexicographic type structures","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.","paper_title":"","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '87.00', 'y': '122.37', 'h': '453.08', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '134.25', 'h': '240.36', 'w': '9.96'}], [{'page': '2', 'x': '316.32', 'y': '134.25', 'h': '223.69', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '146.25', 'h': '467.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '158.13', 'h': '261.48', 'w': '9.96'}], [{'page': '2', 'x': '337.68', 'y': '158.13', 'h': '202.38', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '170.13', 'h': '58.56', 'w': '9.96'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/cs_0309016.pdf","section_number":"1","para":"2","_id":"ad5a36e0-435d-4b5e-bf46-cde788468364","_collection_name":"Papers"},"page_content":"However, within game theory there is a history of Monte Carlo methods being applied to solve noncooperative games, e.g. starting with Ulam [18] in 1954.From the view point of applying global optimization techniques to infinite games, Monte Carlo simulation has been used by Georgobiani and Torondzadze as a means of providing Nash equilibria for rectangular games [3].This is the approach that we will be developing in this paper.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('9', '10')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Related Work in N-Tuple Research","bboxes":"[[{'page': '9', 'x': '321.94', 'y': '703.98', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '715.94', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '70.36', 'h': '29.62', 'w': '8.64'}], [{'page': '10', 'x': '82.61', 'y': '70.36', 'h': '217.41', 'w': '8.64'}, {'page': '10', 'x': '48.96', 'y': '82.31', 'h': '179.75', 'w': '8.64'}]]","text":"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"1f958ad8-f8c9-4c82-bc74-8755781e702a","_collection_name":"Papers"},"page_content":"There are two papers in the game learning literature that combine n-tuple networks with MCTS: Sironi et al. [42] use the n-tuple bandit EA to automatically tune a self-adaptive MCTS.This is an interesting approach but for a completely different goal and not related to AlphaZero.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '2')","paper_title":"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games","section_title":"Main Contributions","bboxes":"[[{'page': '2', 'x': '72.00', 'y': '220.50', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '234.05', 'h': '198.88', 'w': '9.46'}], [{'page': '2', 'x': '274.24', 'y': '234.05', 'h': '265.76', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '247.60', 'h': '328.30', 'w': '9.46'}]]","text":"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2303.04865.pdf","section_number":"1.1","para":"1","_id":"c1d2468e-cb72-4550-abca-dde62b2353f0","_collection_name":"Papers"},"page_content":"We address the question above by introducing a class of networked Markov potential games (NMPGs) as the networked counterpart of classical MPGs.Importantly, NMPGs represent a broader class of games than MPGs and draw focus to algorithm design that uses only local information.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.","paper_title":"Learning Regularized Monotone Graphon Mean-Field Games","section_title":"Existence of the NEs in Regularized GMFGs and MFGs","bboxes":"[[{'page': '6', 'x': '59.53', 'y': '627.90', 'h': '70.71', 'w': '10.52'}], [{'page': '6', 'x': '136.09', 'y': '627.94', 'h': '399.66', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '642.38', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '656.83', 'h': '30.50', 'w': '10.48'}], [{'page': '6', 'x': '93.93', 'y': '656.83', 'h': '441.83', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '671.27', 'h': '476.22', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '685.72', 'h': '55.88', 'w': '10.48'}], [{'page': '6', 'x': '119.30', 'y': '685.72', 'h': '416.44', 'w': '10.48'}, {'page': '6', 'x': '59.53', 'y': '700.16', 'h': '389.94', 'w': '10.48'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2310.08089.pdf","section_number":"4","para":"3","_id":"e24ca2a0-bc0f-4fae-83b3-01e9b700f2d5","_collection_name":"Papers"},"page_content":"Remark 4.5.Although we show that an NE of the constructed MFG can be converted to an NE of GMFG, this proof does not imply that GMFG forms a subclass of or is equivalent to MFG.This is because we have only demonstrated the relationship between the NEs of these two games, but the exact realizations of the GMFG and the conceptually constructed MFG may differ.It means that the sample paths of these two games may not be the same, which include the realizations of the states, actions, and rewards of all the players.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Motivation","bboxes":"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]","pages":"('1', '1')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"2fddfe2e-5f6e-4594-b966-24d92c3d82c7","_collection_name":"Papers"},"page_content":"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.","type":"Document"}],0.7052476406,5,12.7,14.6,4,"The output correctly identifies the Expectimax approach as a necessary extension of the MCTS wrapper concept for nondeterministic games, directly addressing the question posed in the input. It aligns well with the provided context, which discusses the limitations of plain MCTS in such games. However, it could be improved by providing more detail on how Expectimax specifically enhances the MCTS wrapper."],["How does the inclusion of AlphaZero-inspired MCTS planning enhance TD-n-tuple network performance in challenging games like Othello, ConnectFour, and Rubik's Cube?"," The inclusion of AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in challenging games by providing a planning component that allows the agent to consider possible future actions and counter-actions of the opponent. This leads to improved performance in games like Othello, ConnectFour, and Rubik's Cube, even when MCTS is not part of the training stage. This approach also reduces training times and computational resources needed for training.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"VI. CONCLUSION AND FUTURE WORK","bboxes":"[[{'page': '10', 'x': '321.94', 'y': '256.28', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '268.23', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '280.19', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '292.14', 'h': '38.91', 'w': '8.64'}], [{'page': '10', 'x': '353.40', 'y': '292.14', 'h': '209.63', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '303.92', 'h': '251.06', 'w': '8.82'}, {'page': '10', 'x': '311.98', 'y': '316.05', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '328.01', 'h': '40.11', 'w': '8.64'}]]","pages":"('10', '10')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"69bf60a6-d4ec-4964-8f63-b3ad6b7b089b","_collection_name":"Papers"},"page_content":"We have shown on the three challenging games, Othello, ConnectFour, and Rubik's Cube, that an AlphaZero-inspired MCTS planning stage boosts the performance of TD-n-tuple networks.Interestingly, this performance boost is even reached when MCTS is not part of the training stage, which leads to very large reductions in training times and computational resources.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('10', '10')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"VI. CONCLUSION AND FUTURE WORK","bboxes":"[[{'page': '10', 'x': '321.94', 'y': '339.55', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '351.51', 'h': '141.76', 'w': '8.64'}], [{'page': '10', 'x': '456.15', 'y': '351.51', 'h': '106.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '363.46', 'h': '183.86', 'w': '8.64'}], [{'page': '10', 'x': '501.15', 'y': '363.46', 'h': '61.89', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '375.42', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '387.37', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '399.33', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '411.28', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '423.24', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '435.20', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '447.15', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '459.11', 'h': '29.06', 'w': '8.64'}], [{'page': '10', 'x': '343.61', 'y': '459.11', 'h': '219.43', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '471.06', 'h': '124.56', 'w': '8.64'}]]","text":"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"414b691d-581f-4c25-beeb-d65b9c0035cb","_collection_name":"Papers"},"page_content":"The new architecture was evaluated on the three games without any game-specific changes.We reach near-perfect play for ConnectFour and 2x2x2 Rubik's Cube.For the games Othello and 3x3x3 Rubik's Cube, we observe good results and increased performance compared to our version without MCTS planning stage, but we do not reach the high-quality results of Norelli and Panconesi [21] on Othello (beats Edax level 10 where we reach only level 7) and of Agostinelli, McAleer et al. [35], [36] on 3x3x3 Rubik's Cube (they solve all scrambled cubes while we solve only cubes with up to 9 twists).Both high-performing approaches require considerably more computational resources.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.","paper_title":"On the optimal pivot path of simplex method for linear programming based on reinforcement learning","section_title":"Extracting Multiple Shortest Pivot Paths","bboxes":"[[{'page': '17', 'x': '148.71', 'y': '148.88', 'h': '328.77', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '166.81', 'h': '94.83', 'w': '8.74'}], [{'page': '17', 'x': '233.59', 'y': '166.81', 'h': '243.89', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '184.75', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '202.68', 'h': '81.04', 'w': '8.74'}], [{'page': '17', 'x': '222.69', 'y': '202.68', 'h': '254.79', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '220.61', 'h': '189.98', 'w': '8.74'}], [{'page': '17', 'x': '328.71', 'y': '220.61', 'h': '148.76', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '238.55', 'h': '77.42', 'w': '8.74'}], [{'page': '17', 'x': '216.01', 'y': '238.55', 'h': '261.47', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '256.48', 'h': '120.24', 'w': '8.74'}], [{'page': '17', 'x': '258.08', 'y': '256.48', 'h': '219.40', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '274.41', 'h': '279.13', 'w': '8.74'}], [{'page': '17', 'x': '419.27', 'y': '274.41', 'h': '58.21', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '292.34', 'h': '343.71', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '310.28', 'h': '256.24', 'w': '8.74'}], [{'page': '17', 'x': '394.26', 'y': '310.28', 'h': '83.22', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '328.21', 'h': '230.93', 'w': '8.74'}], [{'page': '17', 'x': '369.70', 'y': '328.21', 'h': '107.78', 'w': '8.74'}, {'page': '17', 'x': '133.77', 'y': '346.14', 'h': '257.76', 'w': '8.74'}]]","pages":"('17', '17')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2210.02945.pdf","section_number":"4.3.","para":"8","_id":"e6cd2bf4-d04c-40b3-a4f6-7a2b40dddc73","_collection_name":"Papers"},"page_content":"The optimal pivot paths correspond to different pivot sequences with minimum pivot iterations.Therefore, such an optimal path is not unique, which is important for the simplex method, but previous work is difficult to solve and ignores this point.Fortunately, the randomness of the MCTS rule is highly effective for finding multiple optimal paths.This randomness originates in the exploration stage.Specifically, the generation of the exploration trajectory depends on a random strategy.Thus, the estimated value brought about by limited exploration will be affected by randomness to a certain extent.Based on the randomness of the MCTS rule, our algorithm can select different actions that lead to optimal pivot paths in different execution processes.Therefore, multiple pivot sequences can be used to achieve optimization.Furthermore, we provide proof to ensure that each optimal pivot path can be found.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.","paper_title":"PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS","section_title":"BENCHMARKS","bboxes":"[[{'page': '7', 'x': '108.00', 'y': '123.83', 'h': '396.22', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '134.75', 'h': '396.06', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '145.79', 'h': '67.17', 'w': '9.03'}], [{'page': '7', 'x': '178.32', 'y': '145.79', 'h': '325.69', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '156.71', 'h': '396.30', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '167.63', 'h': '30.69', 'w': '9.03'}], [{'page': '7', 'x': '141.48', 'y': '167.63', 'h': '362.67', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '178.05', 'h': '396.06', 'w': '9.96'}, {'page': '7', 'x': '108.00', 'y': '189.59', 'h': '341.25', 'w': '9.03'}], [{'page': '7', 'x': '455.16', 'y': '189.59', 'h': '48.92', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '200.51', 'h': '396.08', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '210.93', 'h': '178.41', 'w': '9.96'}], [{'page': '7', 'x': '292.44', 'y': '211.55', 'h': '211.50', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '221.85', 'h': '329.49', 'w': '9.96'}], [{'page': '7', 'x': '442.44', 'y': '222.47', 'h': '61.43', 'w': '9.03'}, {'page': '7', 'x': '108.24', 'y': '233.39', 'h': '395.86', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '243.81', 'h': '368.49', 'w': '9.96'}], [{'page': '7', 'x': '482.40', 'y': '244.43', 'h': '21.70', 'w': '9.03'}, {'page': '7', 'x': '108.00', 'y': '255.35', 'h': '144.21', 'w': '9.03'}]]","pages":"('7', '7')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2310.14659.pdf","section_number":"4.1","para":"6","_id":"6e059989-cd59-448a-a4fe-db4f1e1d5593","_collection_name":"Papers"},"page_content":"Since there is no publicly available dataset for this problem adapted to Machine Learning, with large collections of instances sharing common features, we designed four small datasets on which we can run experiments.We generate four MCND datasets from a subset of instances of the Canad dataset (Crainic et al., 2001), well known and commonly used by the optimization community to benchmark solvers.The first two datasets, MCND-SMALL-COM40 and MCND-SMALL-COMVAR, contain instances which all share the same network (20 nodes and 230 edges) and the same arc capacities and fixed costs, but with different values for origins, destinations, volumes and routing costs.Instances of the former all involve the same number of commodities (40), while for the latter the number of commodities can also vary from 40 to 200.Dataset MCND-BIG-COM40 is generated similarly to MCND-SMALL-COM40 but upon a bigger graph containing 30 nodes and 500 arcs.Finally, MCND-BIG-COMVAR contains examples generated using either the network of MCND-SMALL-COM40 or the one of MCND-BIG-COM40, with the number of commodities varying between 40 and 200.More details can be found in Appendix D.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. MCTS Wrapper","bboxes":"[[{'page': '2', 'x': '321.94', 'y': '462.44', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '474.40', 'h': '52.35', 'w': '8.64'}], [{'page': '2', 'x': '367.70', 'y': '474.40', 'h': '195.34', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '486.35', 'h': '21.86', 'w': '8.64'}], [{'page': '2', 'x': '337.36', 'y': '486.35', 'h': '225.67', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '498.31', 'h': '205.57', 'w': '8.64'}], [{'page': '2', 'x': '522.02', 'y': '498.31', 'h': '41.02', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '510.26', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '311.98', 'y': '521.90', 'h': '251.06', 'w': '8.96'}, {'page': '2', 'x': '311.98', 'y': '534.17', 'h': '97.00', 'w': '8.64'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"3","_id":"778dc672-a89f-4a58-a142-ba642132da6c","_collection_name":"Papers"},"page_content":"Our implementation of an MCTS iteration is illustrated in Algorithm 1.It performs a single MCTS iteration for a given node.The numerical return value approximates how valuable it is to choose an action that leads to this node.Since this assessment corresponds to the view of the previous player, the algorithm negates the returned values (\u03ba = -1) in the case of 2-player games.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics\/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and\/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.","paper_title":"Reset-free Trial-and-Error Learning for Robot Damage Recovery","section_title":"Discussion and Conclusion","bboxes":"[[{'page': '15', 'x': '316.57', 'y': '694.09', 'h': '241.10', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '706.05', 'h': '251.05', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '718.00', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '729.96', 'h': '251.06', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '741.91', 'h': '32.49', 'w': '8.74'}], [{'page': '15', 'x': '348.39', 'y': '741.91', 'h': '209.27', 'w': '8.74'}, {'page': '15', 'x': '306.60', 'y': '753.87', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '654.67', 'h': '100.48', 'w': '8.74'}], [{'page': '16', 'x': '144.47', 'y': '654.67', 'h': '144.20', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '666.63', 'h': '251.05', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '678.58', 'h': '166.72', 'w': '8.74'}], [{'page': '16', 'x': '208.70', 'y': '678.58', 'h': '79.98', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '690.54', 'h': '251.06', 'w': '8.74'}, {'page': '16', 'x': '37.61', 'y': '702.49', 'h': '39.38', 'w': '8.74'}]]","pages":"('15', '16')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1610.04213.pdf","section_number":"8.","para":"3","_id":"701347e2-703c-49b3-8c29-c483d9f99719","_collection_name":"Papers"},"page_content":"In this work, we chose to use MCTS for the planning phase of our approach, because it has been successfully used in the context of RL [56,35,23] and because it makes no assumptions about the dynamics\/model of the system.This allows us to incorporate prior knowledge about the problem [20] and to use actions of any type, as we did in our work.Nevertheless, traditional samplebased planners, like RRT, could provide more accurate solutions and\/or be faster in some cases.In future work, we will investigate and experiment with different probabilistic planners.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('5', '5')","paper_title":"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization","section_title":"Experiments and Results","bboxes":"[[{'page': '5', 'x': '107.69', 'y': '137.78', 'h': '396.56', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '149.01', 'h': '24.56', 'w': '8.64'}], [{'page': '5', 'x': '135.96', 'y': '148.69', 'h': '301.51', 'w': '8.96'}], [{'page': '5', 'x': '440.86', 'y': '149.01', 'h': '63.31', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '159.91', 'h': '396.00', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '170.50', 'h': '396.00', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '181.41', 'h': '106.12', 'w': '9.65'}], [{'page': '5', 'x': '217.55', 'y': '181.41', 'h': '228.49', 'w': '8.96'}], [{'page': '5', 'x': '449.47', 'y': '181.73', 'h': '54.53', 'w': '8.64'}, {'page': '5', 'x': '108.00', 'y': '192.64', 'h': '397.75', 'w': '8.64'}], [{'page': '5', 'x': '107.69', 'y': '203.23', 'h': '396.65', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '214.14', 'h': '132.59', 'w': '8.96'}], [{'page': '5', 'x': '243.68', 'y': '214.14', 'h': '260.31', 'w': '8.96'}, {'page': '5', 'x': '108.00', 'y': '225.37', 'h': '284.58', 'w': '8.64'}]]","text":"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square\/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1807.01672.pdf","section_number":"5","para":"6","_id":"21ca484e-c984-4a38-b134-67906841bcc7","_collection_name":"Papers"},"page_content":"To validate our approach and evaluate its effectiveness, we first considered 2D and 3D BPP with 10 items.The items were generated by repeatedly dividing a square\/cube of size 10.The process for generating items randomly is presented in detail in Appendix A. The training process is as follows: at each iteration, 50 problems are generated and solved by R2 with a reward buffer of size 250, used to define the threshold r \u03b1 .All MCTS instances perform 300 simulations per move.The reshaped rewards alongside the MCTS-improved policies are stored in a dataset and used during training.The neural network is trained by gradient descent (Adam) using mini-batches of size 32, uniformly sampled from the last 500 games.At each iteration, 50 steps of gradient descent are performed.Each experiment ran on a NVIDIA Tesla V100 GPU card for up to two days.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and\/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Related work","bboxes":"[[{'page': '9', 'x': '58.93', 'y': '352.78', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '364.73', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '376.69', 'h': '117.21', 'w': '8.64'}], [{'page': '9', 'x': '169.88', 'y': '376.69', 'h': '130.14', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '388.64', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '400.60', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '412.55', 'h': '21.46', 'w': '8.64'}], [{'page': '9', 'x': '74.98', 'y': '412.55', 'h': '225.04', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '424.51', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '436.46', 'h': '122.07', 'w': '8.64'}], [{'page': '9', 'x': '176.36', 'y': '436.46', 'h': '123.67', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '448.42', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '460.37', 'h': '188.81', 'w': '8.64'}], [{'page': '9', 'x': '241.50', 'y': '460.37', 'h': '58.52', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '472.33', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '484.28', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '496.24', 'h': '80.87', 'w': '8.64'}], [{'page': '9', 'x': '133.60', 'y': '496.24', 'h': '166.43', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '508.19', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '520.15', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '532.10', 'h': '79.44', 'w': '8.64'}], [{'page': '9', 'x': '131.76', 'y': '532.10', 'h': '168.26', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '544.06', 'h': '155.22', 'w': '8.64'}], [{'page': '9', 'x': '207.87', 'y': '544.06', 'h': '92.15', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '556.01', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '567.97', 'h': '51.38', 'w': '8.64'}], [{'page': '9', 'x': '104.40', 'y': '567.97', 'h': '195.63', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '579.93', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '591.88', 'h': '183.23', 'w': '8.64'}]]","pages":"('9', '9')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"8","_id":"ece653ae-4f5a-4359-95df-81ca272e7706","_collection_name":"Papers"},"page_content":"Self-play RL has a long tradition in game learning, with Tesauro's TD-Gammon [27] being a very early TD-learning application to Backgammon.The seminal papers of Silver et al. on AlphaGo and AlphaZero [1], [3] lifted this for the games Go, chess and shogi to a new complexity and performance level.They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and\/or fewer training cycles.Thakoor et al. [28] provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.But even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).Several works of Wang et al. [29], [30], [23] focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warmstart enhancements.They test these aspects on smaller games like 6x6 Othello or 5x5 ConnectFour.The work of Chang et al. [31] covered several AlphaZero improvements applied to 6x6 Othello.van der Ree and Wiering [12] investigate TD-, Q-and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons).","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '2')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Algorithm Overview","bboxes":"[[{'page': '2', 'x': '58.93', 'y': '300.75', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '312.71', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '324.66', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '336.62', 'h': '226.56', 'w': '8.64'}], [{'page': '2', 'x': '279.70', 'y': '336.62', 'h': '20.32', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '348.57', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '360.53', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '372.48', 'h': '206.38', 'w': '8.64'}], [{'page': '2', 'x': '58.93', 'y': '538.50', 'h': '241.09', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '550.46', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '562.41', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '574.37', 'h': '29.61', 'w': '8.64'}], [{'page': '2', 'x': '81.47', 'y': '574.37', 'h': '218.55', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '586.32', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '598.28', 'h': '115.47', 'w': '8.64'}], [{'page': '2', 'x': '168.38', 'y': '598.28', 'h': '131.65', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '610.23', 'h': '251.06', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '622.19', 'h': '212.91', 'w': '8.64'}], [{'page': '2', 'x': '264.48', 'y': '622.19', 'h': '35.54', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '634.14', 'h': '47.98', 'w': '8.64'}], [{'page': '2', 'x': '100.42', 'y': '634.14', 'h': '199.60', 'w': '8.64'}, {'page': '2', 'x': '48.96', 'y': '646.10', 'h': '37.53', 'w': '8.64'}]]","text":"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"6","_id":"ad14b04c-df4f-4056-a6ac-95a2e08aabc7","_collection_name":"Papers"},"page_content":"Despite being successful on a variety of games [9], [10], this base algorithm shares one disadvantage with other deep learning algorithms that are only value-based: they base their decision on the value of the current state-action pairs.They have no planning component, no what-if scenarios to think about further consequences, like possible counter-actions of the other player(s), further own actions and so on.This is where AlphaZero's MCTS-trick comes into play: Silver et al. [1], [2] combine a deep learning RL agent with an MCTS wrapper (Fig. 1) to introduce such a planning component.They do this throughout the whole training procedure, which is better for the overall performance but also very computationally demanding.In this work, we take a simpler approach: we first train our RL agent, a TD n-tuple network, and then use the MCTS wrapping only at test time (i.e. during game play).This usage of MCTS adds a form of planning at test time.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. Related Work in N-Tuple Research","bboxes":"[[{'page': '9', 'x': '321.94', 'y': '607.56', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '619.52', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '631.47', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '643.43', 'h': '190.25', 'w': '8.64'}], [{'page': '9', 'x': '504.68', 'y': '643.43', 'h': '58.35', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '655.38', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '667.34', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '679.29', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '311.98', 'y': '691.25', 'h': '39.45', 'w': '8.64'}]]","pages":"('9', '9')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"f44fc6ea-68b1-483c-94e5-9150ebad6361","_collection_name":"Papers"},"page_content":"N-tuple networks, which are an important building block of our approach, have shown to work well in many games, e. g., in ConnectFour [4], [22], Othello [5], EinStein w\u00fcrfelt nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc.Other function approximation networks (DNN or other) could be used as well in AlphaZero-inspired RL, but n-tuple networks have the advantage that they can be trained very fast on off-the-shelf hardware.","type":"Document"}],1.8647825718,86,14.3,18.0,5,"The output comprehensively addresses the question by explaining how AlphaZero-inspired MCTS planning enhances TD-n-tuple network performance in the specified games. It effectively incorporates relevant details from the provided context, including the benefits of planning and reduced training times, making it a thorough and applicable response."],["What is the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm?"," The regular 6x7 ConnectFour game has 10^12 states and a branching factor of \u2264 7 when using alpha-beta search and the Minimax algorithm.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('4', '4')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"III. EXPERIMENTAL SETUP","bboxes":"[[{'page': '4', 'x': '321.94', 'y': '619.92', 'h': '241.09', 'w': '8.82'}, {'page': '4', 'x': '311.98', 'y': '632.06', 'h': '98.21', 'w': '8.64'}], [{'page': '4', 'x': '414.24', 'y': '632.06', 'h': '148.79', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '644.01', 'h': '34.60', 'w': '8.64'}], [{'page': '4', 'x': '350.29', 'y': '643.69', 'h': '152.20', 'w': '8.96'}, {'page': '4', 'x': '502.50', 'y': '642.12', 'h': '7.94', 'w': '6.12'}, {'page': '4', 'x': '514.66', 'y': '644.01', 'h': '48.38', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '655.65', 'h': '194.38', 'w': '8.96'}, {'page': '4', 'x': '506.36', 'y': '654.07', 'h': '6.77', 'w': '6.12'}, {'page': '4', 'x': '516.47', 'y': '655.97', 'h': '46.56', 'w': '8.64'}, {'page': '4', 'x': '311.98', 'y': '667.92', 'h': '80.20', 'w': '8.64'}]]","text":"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \u2264 7. It is a solved game: The 1 st player wins if playing perfectly.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"2","_id":"8c21f839-c634-4e86-9ea7-b4e44fbf28bc","_collection_name":"Papers"},"page_content":"2) ConnectFour: (Four in a Row) is another board game with quite simple rules.Fig. 3(b) shows a typical end game position.The regular 6x7 ConnectFour has 10 12 states and a branching factor \u2264 7. It is a solved game: The 1 st player wins if playing perfectly.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('7', '7')","paper_title":"On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games","section_title":"The Approximation Guarantee","bboxes":"[[{'page': '7', 'x': '108.00', 'y': '410.49', 'h': '190.29', 'w': '9.96'}], [{'page': '7', 'x': '301.08', 'y': '410.49', 'h': '73.15', 'w': '9.96'}, {'page': '7', 'x': '377.04', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '386.40', 'y': '410.49', 'h': '62.26', 'w': '9.96'}, {'page': '7', 'x': '451.44', 'y': '410.49', 'h': '6.64', 'w': '17.04'}, {'page': '7', 'x': '460.92', 'y': '410.95', 'h': '43.14', 'w': '9.10'}]]","text":"Lemma 5. Let \u01eb > 0 and \u03c0 be a strategy profile.If for each player i \u2208 [n], each state s \u2208 S and each","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2109.01795.pdf","section_number":"5.1","para":"1","_id":"0548ecd0-5330-4a0b-a7ad-d48853603e96","_collection_name":"Papers"},"page_content":"Lemma 5. Let \u01eb > 0 and \u03c0 be a strategy profile.If for each player i \u2208 [n], each state s \u2208 S and each","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Motivation","bboxes":"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]","text":"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"4","_id":"412b28e4-f9e3-4960-bdfe-c029c812cab4","_collection_name":"Papers"},"page_content":"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('22', '22')","paper_title":"From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains","section_title":"Algorithm 2 Logistic Bernoulli Factory","bboxes":"[[{'page': '22', 'x': '145.68', 'y': '482.37', 'h': '170.46', 'w': '9.96'}, {'page': '22', 'x': '334.32', 'y': '480.65', 'h': '3.97', 'w': '6.97'}, {'page': '22', 'x': '330.36', 'y': '487.16', 'h': '6.12', 'w': '4.42'}, {'page': '22', 'x': '330.36', 'y': '488.35', 'h': '19.81', 'w': '8.15'}, {'page': '22', 'x': '351.84', 'y': '482.37', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '382.92', 'y': '482.37', 'h': '94.67', 'w': '10.33'}, {'page': '22', 'x': '133.80', 'y': '496.53', 'h': '273.60', 'w': '9.96'}], [{'page': '22', 'x': '411.00', 'y': '496.53', 'h': '66.61', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '508.41', 'h': '190.98', 'w': '10.33'}, {'page': '22', 'x': '336.84', 'y': '505.99', 'h': '7.06', 'w': '6.40'}, {'page': '22', 'x': '336.84', 'y': '513.91', 'h': '12.85', 'w': '6.40'}, {'page': '22', 'x': '351.96', 'y': '508.41', 'h': '107.88', 'w': '10.33'}], [{'page': '22', 'x': '463.68', 'y': '508.41', 'h': '13.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '520.41', 'h': '343.62', 'w': '9.96'}, {'page': '22', 'x': '168.60', 'y': '532.41', 'h': '29.40', 'w': '10.65'}], [{'page': '22', 'x': '199.68', 'y': '532.41', 'h': '277.76', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '547.77', 'h': '17.28', 'w': '9.96'}], [{'page': '22', 'x': '154.92', 'y': '547.77', 'h': '322.54', 'w': '9.96'}, {'page': '22', 'x': '133.80', 'y': '559.77', 'h': '326.49', 'w': '9.96'}]]","text":"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m\/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1912.09229.pdf","section_number":"None","para":"5","_id":"c49ea3dc-7fc6-44c3-8787-5a4d3197ab9c","_collection_name":"Papers"},"page_content":"We now consider the function f (p) = 1 m i=1 pi (p 1 , . . ., p m ) as in [8], where the problem of tossing such f (p)-die is named Bernoulli Race.Their proposed algorithm requires on average E[N D ] = m\/ m i=1 p i tosses of the m coins.After applying the required variable transformation, we then consider f (p) = (p 0 , . . ., pm ) and we can then employ our Dice Enterprise methodology.In this particular problem, f (p) is already a multivariate ladder and the transition matrix of the chain constructed as in Proposition 3.4 is given by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('29', '29')","paper_title":"MULTI-STRUCTURAL GAMES AND BEYOND","section_title":"The Syntactic Game","bboxes":"[[{'page': '29', 'x': '90.00', 'y': '219.82', 'h': '121.87', 'w': '10.91'}, {'page': '29', 'x': '211.92', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '214.80', 'y': '219.82', 'h': '11.96', 'w': '10.91'}, {'page': '29', 'x': '227.16', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '229.92', 'y': '219.82', 'h': '16.44', 'w': '10.91'}, {'page': '29', 'x': '246.72', 'y': '218.06', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '257.16', 'y': '219.82', 'h': '202.46', 'w': '10.91'}], [{'page': '29', 'x': '464.52', 'y': '219.82', 'h': '57.55', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '233.02', 'h': '4.92', 'w': '10.91'}, {'page': '29', 'x': '95.28', 'y': '231.26', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '101.04', 'y': '233.02', 'h': '420.90', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '245.98', 'h': '26.54', 'w': '10.91'}], [{'page': '29', 'x': '122.64', 'y': '245.98', 'h': '399.67', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '258.94', 'h': '303.19', 'w': '10.91'}, {'page': '29', 'x': '393.24', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '399.84', 'y': '258.94', 'h': '28.64', 'w': '10.91'}, {'page': '29', 'x': '428.88', 'y': '257.18', 'h': '2.30', 'w': '7.97'}, {'page': '29', 'x': '431.64', 'y': '258.94', 'h': '3.02', 'w': '10.91'}], [{'page': '29', 'x': '440.40', 'y': '258.94', 'h': '81.41', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '271.90', 'h': '279.74', 'w': '10.91'}], [{'page': '29', 'x': '374.76', 'y': '271.90', 'h': '147.05', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '284.86', 'h': '109.82', 'w': '10.91'}], [{'page': '29', 'x': '206.04', 'y': '284.86', 'h': '316.15', 'w': '10.91'}, {'page': '29', 'x': '87.24', 'y': '297.82', 'h': '326.96', 'w': '10.91'}, {'page': '29', 'x': '414.24', 'y': '302.07', 'h': '17.14', 'w': '8.19'}, {'page': '29', 'x': '435.24', 'y': '297.82', 'h': '86.57', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '310.78', 'h': '19.82', 'w': '10.91'}], [{'page': '29', 'x': '118.20', 'y': '310.78', 'h': '404.03', 'w': '10.91'}, {'page': '29', 'x': '90.00', 'y': '323.74', 'h': '342.02', 'w': '10.91'}]]","text":"Consider a node X = (A \u2032 , B \u2032 ) r \u2032 in the game tree T for a given QVT game.The counter r \u2032 \u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \"distinguish\" between A \u2032 and B \u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \"reasonable\" complexity measure of infinitary formulas in the logic L \u03c9 1 ,\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2301.13329.pdf","section_number":"6.","para":"6","_id":"5a24fce9-eab2-4c1d-8e51-1df08c9f7536","_collection_name":"Papers"},"page_content":"Consider a node X = (A \u2032 , B \u2032 ) r \u2032 in the game tree T for a given QVT game.The counter r \u2032 \u2208 N in X is the number of pebble moves Spoiler may use to close the subtree of T rooted at X.The proof of Theorem 5.6 will follow from the fact that this is exactly the number of quantifiers needed in some sense to \"distinguish\" between A \u2032 and B \u2032 .In fact, as stated earlier, we will prove a significantly stronger generalization.We first need the formal notion of a syntactic measure.Such measures were introduced to derive combinatorial games for \"reasonable\" complexity measure of infinitary formulas in the logic L \u03c9 1 ,\u03c9 [VW13, Definition 5.1].In this section, we will derive games precisely capturing many familiar complexity measures, including: quantifier count, quantifier rank, and formula size.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('4', '4')","paper_title":"Quantitative entanglement witnesses of Isotropic-and Werner-class via local measurements","section_title":"III. LOCAL DECOMPOSITION OF QEW","bboxes":"[[{'page': '4', 'x': '448.61', 'y': '52.70', 'h': '6.28', 'w': '6.31'}, {'page': '4', 'x': '317.01', 'y': '245.45', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '256.91', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '268.37', 'h': '245.08', 'w': '9.65'}, {'page': '4', 'x': '320.89', 'y': '279.82', 'h': '88.77', 'w': '9.65'}, {'page': '4', 'x': '409.66', 'y': '278.25', 'h': '3.97', 'w': '6.12'}, {'page': '4', 'x': '418.36', 'y': '279.82', 'h': '143.73', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '291.28', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '302.74', 'h': '11.79', 'w': '8.74'}], [{'page': '4', 'x': '335.32', 'y': '302.74', 'h': '226.77', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '314.19', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '325.65', 'h': '18.27', 'w': '8.74'}], [{'page': '4', 'x': '342.72', 'y': '325.65', 'h': '219.37', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '337.11', 'h': '150.19', 'w': '8.74'}], [{'page': '4', 'x': '475.20', 'y': '337.11', 'h': '86.89', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '348.57', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '360.02', 'h': '245.08', 'w': '8.74'}], [{'page': '4', 'x': '317.01', 'y': '371.48', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '382.94', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '394.39', 'h': '191.97', 'w': '8.74'}], [{'page': '4', 'x': '513.70', 'y': '394.39', 'h': '48.40', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '405.85', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '417.31', 'h': '245.08', 'w': '9.65'}], [{'page': '4', 'x': '317.01', 'y': '428.76', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '320.89', 'y': '440.22', 'h': '83.00', 'w': '9.65'}], [{'page': '4', 'x': '414.21', 'y': '440.22', 'h': '56.27', 'w': '9.65'}], [{'page': '4', 'x': '476.92', 'y': '440.22', 'h': '85.17', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '451.68', 'h': '245.08', 'w': '8.74'}, {'page': '4', 'x': '317.01', 'y': '463.14', 'h': '17.72', 'w': '8.74'}]]","text":"2) A \u2208 \u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \u03b1 \u2297 P \u03b1 , X \u03b1\u03b2 \u2297 X \u03b1\u03b2 and X \u03b1\u03b2 \u2297 X \u03b1\u03b2 from d 2 to order d (here we use \u2022 \u2022 \u2022 to represent the expectation value with respect to the state \u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \u03b1-th vertex green we are actually acquiring P \u03b1 \u2297 P \u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \u03b1 and \u03b2 we evaluate X \u03b1\u03b2 \u2297 X \u03b1\u03b2 (resp.Y \u03b1\u03b2 \u2297 Y \u03b1\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1102.3821.pdf","section_number":"None","para":"8","_id":"ee5e208b-f1e5-4fef-b482-f1a1da230bb6","_collection_name":"Papers"},"page_content":"2) A \u2208 \u03a3. Exploiting this simple property one can indeed reduce [20] the number of setup re-preparations needed to obtain the all values P \u03b1 \u2297 P \u03b1 , X \u03b1\u03b2 \u2297 X \u03b1\u03b2 and X \u03b1\u03b2 \u2297 X \u03b1\u03b2 from d 2 to order d (here we use \u2022 \u2022 \u2022 to represent the expectation value with respect to the state \u03c1).For the sake of clarity, we review this result by rederiving using a different method than that presented in [20].To do so we find it convenient to turn the problem into a graph colouring game.Let us assume that we have a graph made of d vertices and all the possible edges among them (also known as complete graph), e.g.see figures 1 and 2; we are also given one green brush to colour the vertices with, alongside with one blue and one red brushes to paint the edges of the graph.The idea is that when we paint the \u03b1-th vertex green we are actually acquiring P \u03b1 \u2297 P \u03b1 , while if we paint in red (resp.blue) the edge connecting the nodes \u03b1 and \u03b2 we evaluate X \u03b1\u03b2 \u2297 X \u03b1\u03b2 (resp.Y \u03b1\u03b2 \u2297 Y \u03b1\u03b2 ).As a rule, during a single turn the player can paint everything he wants, but not:","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('7', '7')","paper_title":"HAMILTONIAN EVOLUTIONARY GAMES","section_title":"Polymatrix games","bboxes":"[[{'page': '7', 'x': '83.39', 'y': '317.51', 'h': '293.72', 'w': '10.48'}], [{'page': '7', 'x': '385.27', 'y': '317.51', 'h': '102.86', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '331.46', 'h': '315.84', 'w': '10.48'}], [{'page': '7', 'x': '392.93', 'y': '331.46', 'h': '95.20', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '345.40', 'h': '232.98', 'w': '10.48'}], [{'page': '7', 'x': '313.82', 'y': '345.40', 'h': '174.31', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '359.35', 'h': '30.81', 'w': '10.48'}], [{'page': '7', 'x': '104.25', 'y': '359.35', 'h': '83.35', 'w': '10.48'}, {'page': '7', 'x': '187.60', 'y': '363.87', 'h': '5.44', 'w': '6.99'}, {'page': '7', 'x': '198.26', 'y': '359.35', 'h': '289.87', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '373.30', 'h': '131.49', 'w': '10.48'}], [{'page': '7', 'x': '212.09', 'y': '373.30', 'h': '276.03', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '387.25', 'h': '11.54', 'w': '10.48'}, {'page': '7', 'x': '82.97', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '87.71', 'y': '387.25', 'h': '18.98', 'w': '10.48'}], [{'page': '7', 'x': '108.69', 'y': '387.25', 'h': '12.22', 'w': '10.48'}, {'page': '7', 'x': '120.91', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '125.68', 'y': '387.25', 'h': '7.80', 'w': '10.48'}], [{'page': '7', 'x': '140.41', 'y': '387.25', 'h': '256.73', 'w': '10.48'}, {'page': '7', 'x': '397.14', 'y': '391.76', 'h': '4.23', 'w': '6.99'}, {'page': '7', 'x': '404.93', 'y': '387.25', 'h': '25.89', 'w': '10.48'}], [{'page': '7', 'x': '433.87', 'y': '387.25', 'h': '19.14', 'w': '10.48'}, {'page': '7', 'x': '453.01', 'y': '391.76', 'h': '4.26', 'w': '6.99'}, {'page': '7', 'x': '457.77', 'y': '387.25', 'h': '3.25', 'w': '10.48'}], [{'page': '7', 'x': '467.96', 'y': '387.25', 'h': '20.16', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '401.19', 'h': '324.04', 'w': '10.48'}, {'page': '7', 'x': '395.48', 'y': '405.71', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '402.74', 'y': '401.19', 'h': '3.25', 'w': '10.48'}, {'page': '7', 'x': '406.00', 'y': '407.76', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '417.84', 'y': '401.19', 'h': '70.28', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '415.88', 'h': '23.63', 'w': '10.48'}, {'page': '7', 'x': '95.07', 'y': '420.40', 'h': '6.31', 'w': '6.99'}, {'page': '7', 'x': '107.14', 'y': '415.88', 'h': '380.98', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '429.83', 'h': '255.42', 'w': '10.48'}], [{'page': '7', 'x': '336.60', 'y': '429.83', 'h': '151.53', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '443.78', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '457.73', 'h': '416.69', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '471.67', 'h': '32.52', 'w': '10.48'}], [{'page': '7', 'x': '110.92', 'y': '471.67', 'h': '377.21', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '485.62', 'h': '259.89', 'w': '10.48'}], [{'page': '7', 'x': '336.31', 'y': '485.62', 'h': '151.81', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '499.57', 'h': '128.06', 'w': '10.48'}], [{'page': '7', 'x': '205.50', 'y': '499.57', 'h': '186.74', 'w': '10.48'}, {'page': '7', 'x': '392.24', 'y': '497.95', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '410.67', 'y': '506.97', 'h': '12.54', 'w': '6.99'}, {'page': '7', 'x': '424.12', 'y': '499.57', 'h': '64.00', 'w': '10.48'}, {'page': '7', 'x': '71.43', 'y': '520.55', 'h': '26.99', 'w': '10.48'}]]","text":"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \u03b1 and \u03b2 to denote these groups.Assume that for each group \u03b1 \u2208 {1, . . ., p}, there are n \u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \u03b1,\u03b2 \u03b1,\u03b2 , where each block","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1404.5900.pdf","section_number":"3.","para":"13","_id":"80e7e135-2677-4f4d-ab27-2c32c5e25390","_collection_name":"Papers"},"page_content":"Finally we introduce the class of polymatrix replicators.Consider p different populations, or else a single population stratified in p groups.We shall use greek letters like \u03b1 and \u03b2 to denote these groups.Assume that for each group \u03b1 \u2208 {1, . . ., p}, there are n \u03b1 pure strategies for interacting with members of another group, including its own.Let us call signature of the game to the vector n = (n 1 , . . ., n p ).The total number of strategies is therefore n = n 1 + . . .+ n p .The polymatrix game is specified by a single n \u00d7 n matrix A = [ a ij ] ij with the payoff a ij for a user of strategy i, member of one group, against a user of strategy j, member of another group, possibly the same.The main difference between polymatrix games and the symmetric game, also specified by a single matrix A, is that in the polymatrix game competition is restricted to members of the same group.This means that the relative fitness of each strategy refers to the overall average pay-off of strategies within the same group.To be more precise we need to introduce some notation.We decompose A in blocks, A = A \u03b1,\u03b2 \u03b1,\u03b2 , where each block","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"where the number of defectors among the two co-players is written at the top of each column , and \u03c1 is a multiplication factor satisfying 1 < \u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.","paper_title":"Automata representation of successful strategies for social dilemmas","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '56.33', 'y': '542.41', 'h': '499.14', 'w': '8.85'}, {'page': '2', 'x': '56.69', 'y': '554.04', 'h': '83.39', 'w': '9.17'}], [{'page': '2', 'x': '143.17', 'y': '554.36', 'h': '276.03', 'w': '8.64'}], [{'page': '2', 'x': '422.28', 'y': '554.36', 'h': '133.03', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '566.00', 'h': '482.10', 'w': '9.73'}], [{'page': '2', 'x': '541.88', 'y': '566.32', 'h': '13.59', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '578.27', 'h': '498.96', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '590.23', 'h': '22.26', 'w': '8.64'}, {'page': '2', 'x': '78.95', 'y': '588.30', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '89.32', 'y': '590.23', 'h': '282.95', 'w': '8.64'}], [{'page': '2', 'x': '375.37', 'y': '590.23', 'h': '179.94', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '602.18', 'h': '498.62', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '614.14', 'h': '111.81', 'w': '8.64'}, {'page': '2', 'x': '168.50', 'y': '612.21', 'h': '43.20', 'w': '10.57'}], [{'page': '2', 'x': '71.64', 'y': '626.09', 'h': '393.33', 'w': '8.64'}], [{'page': '2', 'x': '468.05', 'y': '626.09', 'h': '88.50', 'w': '8.64'}, {'page': '2', 'x': '56.33', 'y': '638.05', 'h': '499.32', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '649.82', 'h': '111.52', 'w': '8.82'}], [{'page': '2', 'x': '171.31', 'y': '650.00', 'h': '227.28', 'w': '8.64'}, {'page': '2', 'x': '398.59', 'y': '648.07', 'h': '7.37', 'w': '6.39'}, {'page': '2', 'x': '408.96', 'y': '650.00', 'h': '146.35', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '661.96', 'h': '103.02', 'w': '8.64'}], [{'page': '2', 'x': '162.80', 'y': '661.96', 'h': '392.50', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '673.91', 'h': '109.93', 'w': '8.64'}], [{'page': '2', 'x': '169.81', 'y': '673.91', 'h': '385.49', 'w': '8.64'}, {'page': '2', 'x': '56.69', 'y': '685.87', 'h': '238.77', 'w': '8.64'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1910.02634.pdf","section_number":"None","para":"9","_id":"25eeb897-22f5-4e0a-953d-3e0221b31c04","_collection_name":"Papers"},"page_content":"where the number of defectors among the two co-players is written at the top of each column , and \u03c1 is a multiplication factor satisfying 1 < \u03c1 < 3.This is a generalization of the iterated PD game to a three-person case.As in the PD game, the only Nash equilibrium of the one-shot PG game is full defection with payoff M D,2 = 1, which is the worst for the society as a whole.For the iterated three-person PG game, it has been found that at least 256 successful strategies exist in the memory-three strategy space 27 and that no such strategy exists if the memory length is less than three.This fact immediately poses a problem on its understandability: Recall that a memory-three strategy is defined by an action table having 512 entries because the number of possible history profiles is 2 3\u00d73 = 512.The purpose of this paper is to interpret the successful strategies by representing them as automata.In the previous works, we have represented successful strategies in a 'history-based' manner so that the next action is given as a function of the history profile for the last m rounds.However, a strategy may also be defined as an automaton, 28 i.e., in a way that a player has a finite number of internal states.A player's internal state determines her next action, and it changes according to the actions taken by the players of the game.We will show that the decision mechanism behind the actions prescribed by the strategy can be understood more clearly in this 'state-based' representation.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. ConnectFour","bboxes":"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"5","_id":"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1","_collection_name":"Papers"},"page_content":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Playing from c 0 in M S , the strategy \u03c3 \u01eb behaves as follows.First it plays like strategy \u03c3 in the corresponding game from c \u2032 0 on M S \u2032 .(Function \u03bb connects the corresponding configurations in the two games.)When the game in M S \u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \u2265 s i (d(\u01eb)) then \u03c3 \u01eb henceforth plays like \u03c3 \u01eb i , which ensures to reach the target X with probability \u2265 1 -2\u01eb.Otherwise, the configuration in M S is still too small to switch to \u03c3 \u01eb i .In this case, \u03c3 \u01eb continues to play like \u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \u2032 , i.e., component k(i) is pumped.Since we know that \u03c3 on M S \u2032 will almost surely visit X \u2032 f or R, we obtain that \u03c3 \u01eb on M S will almost surely eventually visit X or some configuration \u2265 s i (d(\u01eb)) for i \u2208 J (and from there achieve to reach the target with probability \u2265 1 -2\u01eb).Since every weighted average of probabilities \u2265 1-2\u01eb is still \u2265 1-2\u01eb, we obtain P(M S , c 0 , \u03c3 \u01eb , \u2666X ) \u2265 1 -2\u01eb and thus P + (M S , c 0 , \u2666X ) = 1.","paper_title":"Qualitative Analysis of VASS-Induced MDPs \u22c6","section_title":"B.7 Proof of Lemma 6","bboxes":"[[{'page': '29', 'x': '149.76', 'y': '462.69', 'h': '253.56', 'w': '10.66'}], [{'page': '29', 'x': '406.32', 'y': '462.69', 'h': '74.26', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '474.69', 'h': '202.54', 'w': '9.96'}, {'page': '29', 'x': '337.32', 'y': '473.21', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '337.32', 'y': '479.33', 'h': '3.97', 'w': '6.97'}, {'page': '29', 'x': '346.20', 'y': '474.69', 'h': '32.19', 'w': '10.66'}, {'page': '29', 'x': '379.32', 'y': '474.69', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '386.52', 'y': '474.69', 'h': '94.05', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '486.57', 'h': '234.14', 'w': '9.96'}], [{'page': '29', 'x': '373.44', 'y': '486.57', 'h': '106.23', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '498.57', 'h': '344.97', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '510.23', 'h': '190.61', 'w': '10.87'}, {'page': '29', 'x': '325.68', 'y': '509.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '325.32', 'y': '515.33', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '329.52', 'y': '510.45', 'h': '151.46', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '522.23', 'h': '132.24', 'w': '10.18'}], [{'page': '29', 'x': '271.44', 'y': '522.45', 'h': '209.10', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '534.45', 'h': '89.93', 'w': '9.96'}, {'page': '29', 'x': '225.00', 'y': '533.09', 'h': '3.32', 'w': '6.97'}, {'page': '29', 'x': '224.64', 'y': '539.21', 'h': '2.82', 'w': '6.97'}, {'page': '29', 'x': '228.84', 'y': '534.45', 'h': '2.76', 'w': '9.96'}], [{'page': '29', 'x': '235.68', 'y': '534.45', 'h': '244.90', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '546.33', 'h': '304.71', 'w': '10.66'}, {'page': '29', 'x': '444.48', 'y': '546.33', 'h': '36.10', 'w': '9.96'}, {'page': '29', 'x': '134.52', 'y': '558.33', 'h': '86.64', 'w': '9.96'}], [{'page': '29', 'x': '224.64', 'y': '558.33', 'h': '256.38', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '570.33', 'h': '172.32', 'w': '9.96'}], [{'page': '29', 'x': '311.52', 'y': '570.33', 'h': '169.02', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '582.21', 'h': '346.02', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '594.21', 'h': '58.59', 'w': '10.65'}, {'page': '29', 'x': '194.28', 'y': '594.21', 'h': '147.72', 'w': '9.96'}], [{'page': '29', 'x': '346.20', 'y': '594.21', 'h': '133.47', 'w': '10.65'}, {'page': '29', 'x': '134.40', 'y': '606.09', 'h': '113.61', 'w': '9.96'}, {'page': '29', 'x': '248.88', 'y': '604.73', 'h': '2.29', 'w': '6.97'}, {'page': '29', 'x': '248.04', 'y': '611.21', 'h': '3.88', 'w': '6.97'}, {'page': '29', 'x': '257.28', 'y': '606.09', 'h': '223.77', 'w': '10.66'}, {'page': '29', 'x': '134.76', 'y': '617.87', 'h': '345.82', 'w': '10.87'}, {'page': '29', 'x': '134.76', 'y': '629.87', 'h': '248.52', 'w': '10.18'}], [{'page': '29', 'x': '387.72', 'y': '630.09', 'h': '93.01', 'w': '9.96'}, {'page': '29', 'x': '134.76', 'y': '640.87', 'h': '269.22', 'w': '11.06'}, {'page': '29', 'x': '404.04', 'y': '641.75', 'h': '76.50', 'w': '10.87'}, {'page': '29', 'x': '134.28', 'y': '652.61', 'h': '83.04', 'w': '11.32'}, {'page': '29', 'x': '217.80', 'y': '653.75', 'h': '86.64', 'w': '10.87'}]]","pages":"('29', '29')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1512.08824.pdf","section_number":"None","para":"9","_id":"6d58eaad-c55e-4a91-818c-2e06e60d32c0","_collection_name":"Papers"},"page_content":"Playing from c 0 in M S , the strategy \u03c3 \u01eb behaves as follows.First it plays like strategy \u03c3 in the corresponding game from c \u2032 0 on M S \u2032 .(Function \u03bb connects the corresponding configurations in the two games.)When the game in M S \u2032 reaches a configuration r i then there are two cases: If the configuration in M S is \u2265 s i (d(\u01eb)) then \u03c3 \u01eb henceforth plays like \u03c3 \u01eb i , which ensures to reach the target X with probability \u2265 1 -2\u01eb.Otherwise, the configuration in M S is still too small to switch to \u03c3 \u01eb i .In this case, \u03c3 \u01eb continues to play like \u03c3 plays from the previously visited smaller configuration in the uncolored part of M S \u2032 (see line 7 of the algorithm).This is possible, because the game is monotone and larger configurations always benefit Player 1.So the game on M S continues with a configuration that is larger (at least on component k(i)) than the corresponding game on M S \u2032 , i.e., component k(i) is pumped.Since we know that \u03c3 on M S \u2032 will almost surely visit X \u2032 f or R, we obtain that \u03c3 \u01eb on M S will almost surely eventually visit X or some configuration \u2265 s i (d(\u01eb)) for i \u2208 J (and from there achieve to reach the target with probability \u2265 1 -2\u01eb).Since every weighted average of probabilities \u2265 1-2\u01eb is still \u2265 1-2\u01eb, we obtain P(M S , c 0 , \u03c3 \u01eb , \u2666X ) \u2265 1 -2\u01eb and thus P + (M S , c 0 , \u2666X ) = 1.","type":"Document"}],1.1206381321,34,11.9,13.8,5,"The output directly answers the question about the number of states and the branching factor in a 6x7 ConnectFour game using alpha-beta search and the Minimax algorithm. It provides specific numerical values and is consistent with the context provided, demonstrating a comprehensive understanding of the topic."],["What was the computational setup for training the deep network to solve the 2x2x2 and 3x3x3 Rubik's Cube variants?"," The deep network was trained for 44 hours on a 32-core server with 3 GPUs.",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"C. Comparison with Other RL Research","bboxes":"[[{'page': '10', 'x': '321.94', 'y': '165.59', 'h': '241.09', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '177.55', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '189.50', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '201.46', 'h': '35.54', 'w': '8.64'}], [{'page': '10', 'x': '351.78', 'y': '201.46', 'h': '211.26', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '213.41', 'h': '251.06', 'w': '8.64'}, {'page': '10', 'x': '311.98', 'y': '225.37', 'h': '25.09', 'w': '8.64'}]]","pages":"('10', '10')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"9e430404-97b3-4346-b0c0-e2cdecb1b206","_collection_name":"Papers"},"page_content":"Concerning the puzzle Rubik's Cube, the deep network used by McAleer [35] and Agostinelli [36] had over 12 million weights and was trained for 44 hours on a 32-core server with 3 GPUs.Our approach with much less computational effort can solve the 2x2x2 cube completely, but the 3x3x3 cube only partly.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('11', '11')","paper_title":"Algorithmically-consistent deep learning frameworks for structural topology optimization","section_title":"Results","bboxes":"[[{'page': '11', 'x': '72.00', 'y': '319.27', 'h': '468.00', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '331.23', 'h': '86.63', 'w': '9.58'}], [{'page': '11', 'x': '163.27', 'y': '331.23', 'h': '376.74', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '343.18', 'h': '157.98', 'w': '9.58'}], [{'page': '11', 'x': '236.88', 'y': '343.18', 'h': '303.12', 'w': '9.58'}, {'page': '11', 'x': '72.00', 'y': '355.14', 'h': '40.86', 'w': '9.58'}], [{'page': '11', 'x': '115.95', 'y': '355.14', 'h': '357.39', 'w': '9.58'}]]","text":"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2012.05359.pdf","section_number":"5","para":"3","_id":"57ab9ca6-bf2c-466c-a038-7e355bc92112","_collection_name":"Papers"},"page_content":"We split both datasets (i.e., 2D and 3D geometries) into two parts for training the neural networks: training and testing dataset.Out of all data generated, we use 75% of the topology optimization data for training and the remaining 25% for testing.We use the testing dataset to evaluate the performance of all three methods.We will discuss the results for 2D and 3D topologies in the following subsections.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Motivation","bboxes":"[[{'page': '1', 'x': '321.94', 'y': '416.93', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '428.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '440.84', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '452.80', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '464.75', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '476.71', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '488.66', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '500.62', 'h': '23.52', 'w': '8.64'}], [{'page': '1', 'x': '339.85', 'y': '500.62', 'h': '223.18', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '512.57', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '524.53', 'h': '153.63', 'w': '8.64'}]]","text":"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"1","_id":"2fddfe2e-5f6e-4594-b966-24d92c3d82c7","_collection_name":"Papers"},"page_content":"The main contributions of this paper are as follows: (i) it shows for the first time -to the best of our knowledge -a coupling between trainable TD(\u03bb) n-tuple networks and MCTS planning; (ii) an AlphaZero-inspired solution, but with largely reduced computational requirements; (iii) a comparison between MCTS inside and outside of self-play training; (iv) very good results on Othello, ConnectFour and 2x2x2 Rubik's Cube.Specifically, we are able to defeat the strong Othello program Edax at level 7 with an agent trained from scratch in less than 2 hours on a standard CPU.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('6', '6')","paper_title":"Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML","section_title":"Experimental Settings","bboxes":"[[{'page': '6', 'x': '320.82', 'y': '222.23', 'h': '97.24', 'w': '8.96'}], [{'page': '6', 'x': '424.80', 'y': '222.62', 'h': '120.31', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '234.58', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '246.53', 'h': '236.25', 'w': '8.64'}], [{'page': '6', 'x': '308.86', 'y': '258.49', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '270.44', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '282.08', 'h': '236.25', 'w': '8.96'}, {'page': '6', 'x': '308.86', 'y': '294.35', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '306.31', 'h': '217.95', 'w': '8.64'}], [{'page': '6', 'x': '531.43', 'y': '306.31', 'h': '13.69', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '318.27', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '330.22', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '342.18', 'h': '77.99', 'w': '8.64'}], [{'page': '6', 'x': '389.93', 'y': '342.18', 'h': '155.19', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '354.13', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '366.09', 'h': '110.83', 'w': '8.64'}], [{'page': '6', 'x': '423.22', 'y': '366.09', 'h': '121.89', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '378.04', 'h': '236.25', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '390.00', 'h': '125.11', 'w': '8.64'}], [{'page': '6', 'x': '438.09', 'y': '390.00', 'h': '107.03', 'w': '8.64'}, {'page': '6', 'x': '308.86', 'y': '401.95', 'h': '182.68', 'w': '8.64'}]]","text":"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1903.08362.pdf","section_number":"4.1.","para":"6","_id":"a9a4072a-074f-4a06-b613-8e50c185e8a3","_collection_name":"Papers"},"page_content":"Base network settings.For two MNIST datasets, we use a two-layer fully-connected neural network of 100-100 units with ReLU activations as our initial network.For CIFAR-100 dataset, we use a modified version of AlexNet [15] which has five convolutional layers (64-128-256-256-128 depth with 5 \u00d7 5 filter size), and three fullyconnected layers (384-192-100 neurons at each layer) and the standard data augmentation is used in this dataset.For CUB-200 dataset, we use a pre-trained VGG-16 [27] model from ImageNet [5] and fine-tune it on the CUB-200 data for better initialization.We follow the setting of Liu et al. [19], which adds a global pooling layer after the final convolutional layer of the VGG-16.The fully-connected layers are changed to 512-512 and the size of the output layer is the number of classes in each task.All models and algorithms are implemented using Tensorflow [1] library.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('7', '8')","paper_title":"SOFTENED SYMBOL GROUNDING FOR NEURO-SYMBOLIC SYSTEMS","section_title":"VISUAL SUDOKU CLASSIFICATION","bboxes":"[[{'page': '7', 'x': '108.00', 'y': '659.91', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '670.65', 'h': '396.00', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '681.39', 'h': '246.77', 'w': '8.64'}], [{'page': '7', 'x': '359.04', 'y': '681.39', 'h': '144.97', 'w': '8.64'}, {'page': '7', 'x': '108.00', 'y': '692.13', 'h': '396.00', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '85.34', 'h': '57.64', 'w': '8.64'}], [{'page': '8', 'x': '169.07', 'y': '85.34', 'h': '262.10', 'w': '8.64'}], [{'page': '8', 'x': '434.59', 'y': '85.34', 'h': '69.41', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '96.08', 'h': '251.01', 'w': '8.64'}], [{'page': '8', 'x': '364.07', 'y': '96.08', 'h': '139.93', 'w': '8.64'}, {'page': '8', 'x': '108.00', 'y': '106.82', 'h': '280.72', 'w': '8.64'}]]","text":"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2403.00323.pdf","section_number":"5.2","para":"4","_id":"f49e6c09-0ea0-4e96-bf8e-32d4de6eed60","_collection_name":"Papers"},"page_content":"We next evaluate our approach on a visual Sudoku classification task (Wang et al., 2019), where the neural network recognizes the digits (i.e., MNIST images) in the Sudoku board, and the symbolic module determines whether a solution is valid for the puzzle.To evaluate the sample efficiency of our approach, we vary the size of training set by 50, 100, 300, and 500, and the size of test set is fixed to 1,000.Note that the solution space in this task is intrinsically connected.For example, one can easily obtain a new solution by permuting any two digits.Therefore, we additionally include this strategy without the projection (denoted by MCMC) as a baseline.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"C. Rubik's Cube","bboxes":"[[{'page': '6', 'x': '321.94', 'y': '510.57', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '522.52', 'h': '27.40', 'w': '8.64'}], [{'page': '6', 'x': '344.95', 'y': '522.52', 'h': '218.08', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '534.16', 'h': '251.06', 'w': '9.65'}, {'page': '6', 'x': '311.98', 'y': '546.11', 'h': '98.03', 'w': '9.65'}, {'page': '6', 'x': '410.01', 'y': '544.76', 'h': '3.49', 'w': '6.05'}, {'page': '6', 'x': '413.99', 'y': '546.43', 'h': '100.67', 'w': '8.64'}], [{'page': '6', 'x': '517.14', 'y': '546.43', 'h': '45.90', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '558.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '570.34', 'h': '234.33', 'w': '8.64'}], [{'page': '6', 'x': '550.00', 'y': '570.34', 'h': '13.03', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '582.30', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '593.93', 'h': '251.06', 'w': '8.96'}, {'page': '6', 'x': '311.98', 'y': '606.21', 'h': '59.69', 'w': '8.64'}], [{'page': '6', 'x': '375.72', 'y': '606.21', 'h': '187.32', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '618.16', 'h': '21.30', 'w': '8.64'}], [{'page': '6', 'x': '336.64', 'y': '617.98', 'h': '226.40', 'w': '8.82'}, {'page': '6', 'x': '311.98', 'y': '629.80', 'h': '135.98', 'w': '9.65'}], [{'page': '6', 'x': '450.83', 'y': '630.12', 'h': '112.21', 'w': '8.64'}, {'page': '6', 'x': '311.98', 'y': '642.07', 'h': '72.71', 'w': '8.64'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"6","_id":"0bd48af0-1080-4d25-9e89-d61d94b6d93d","_collection_name":"Papers"},"page_content":"We investigate two variants of Rubik's Cube: 2x2x2 and 3x3x3.We trained TCL agents by presenting them cubes scrambled with up to p max twists where p max = 13 for 2x2x2 and p max = 9 for 3x3x3 5 , both in half-turn metric.This covers the complete cube space for 2x2x2, but only a small subset for 3x3x3, where God's number [25] is known to be 20.We evaluate the trained agents on 200 scrambled cubes that are created by applying a given number p of scrambling twists to a solved cube.The agent now tries to solve each scrambled cube.A cube is said to be unsolved if the agent cannot reach the solved cube in e E = 20 steps.More details on our method are found in [26].","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"Logic Guided Genetic Algorithms","section_title":"Introduction","bboxes":"[[{'page': '1', 'x': '329.46', 'y': '375.30', 'h': '228.54', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '386.26', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '397.22', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '408.18', 'h': '238.50', 'w': '8.64'}], [{'page': '1', 'x': '319.50', 'y': '419.14', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '430.10', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '441.06', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '452.02', 'h': '238.50', 'w': '8.64'}, {'page': '1', 'x': '319.50', 'y': '462.98', 'h': '140.40', 'w': '8.64'}]]","text":"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2010.11328.pdf","section_number":"None","para":"1","_id":"e79804f7-d454-4216-a49e-f63822bf79ef","_collection_name":"Papers"},"page_content":"Recently, several Deep Neural Network (DNN) based methods have been proposed to address the SR problem, the most successful among them being AI Feynman 1.0 and 2.0 (Udrescu and Tegmark 2020; Udrescu et al. 2020).Briefly, both versions of AI Feynman use a combination of specialized properties common in physics equations (e.g., additive and multiplicative separability) and use DNNs to break down the problem into several smaller SR problems which can then be tractably solved.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.","paper_title":"Algorithmically-consistent deep learning frameworks for structural topology optimization","section_title":"Training Algorithms","bboxes":"[[{'page': '9', 'x': '72.00', 'y': '515.54', 'h': '202.08', 'w': '9.58'}], [{'page': '9', 'x': '278.00', 'y': '515.54', 'h': '262.01', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '527.49', 'h': '82.48', 'w': '9.58'}], [{'page': '9', 'x': '158.51', 'y': '527.49', 'h': '269.13', 'w': '9.58'}], [{'page': '9', 'x': '431.65', 'y': '527.49', 'h': '108.35', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '539.45', 'h': '129.83', 'w': '9.58'}], [{'page': '9', 'x': '204.10', 'y': '539.45', 'h': '335.90', 'w': '9.58'}, {'page': '9', 'x': '72.00', 'y': '551.40', 'h': '102.56', 'w': '9.58'}], [{'page': '10', 'x': '215.61', 'y': '181.49', 'h': '248.47', 'w': '7.60'}], [{'page': '10', 'x': '469.07', 'y': '181.49', 'h': '70.93', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '190.96', 'h': '153.06', 'w': '7.60'}], [{'page': '10', 'x': '230.10', 'y': '190.96', 'h': '205.94', 'w': '7.60'}], [{'page': '10', 'x': '441.09', 'y': '190.96', 'h': '98.91', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '200.42', 'h': '206.11', 'w': '7.60'}], [{'page': '10', 'x': '281.40', 'y': '200.42', 'h': '258.60', 'w': '7.60'}, {'page': '10', 'x': '72.00', 'y': '209.88', 'h': '126.29', 'w': '7.60'}]]","pages":"('9', '10')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2012.05359.pdf","section_number":"3.3.1","para":"9","_id":"44847082-6d11-4b98-bc58-7b3cd6fb659a","_collection_name":"Papers"},"page_content":"All three networks are trained independently.During the training phase, we use Adam [20] optimizer for all three networks.For more efficient training, we use an adaptive learning rate.The mean absolute error loss function is used for CPN.Moreover, for DPN and FDPN, the binary cross-entropy loss function is used to predict the densities.Each sample in the dataset if generated using this data generation pipeline.First we initialize the geometry (a cube with side length of 1 meter).This geometry is discretized into tetrahedrons to get the mesh.On this mesh, we define three non-collinear nodes to fix the mesh from any rigid body motion.Then we apply randomly generated boundary conditions and loading conditions with different magnitude and direction.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('17', '18')","paper_title":"THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES","section_title":"D.2 n-ARY LAYERS","bboxes":"[[{'page': '17', 'x': '108.00', 'y': '710.06', 'h': '374.35', 'w': '9.96'}, {'page': '17', 'x': '482.48', 'y': '714.73', 'h': '3.97', 'w': '6.97'}, {'page': '17', 'x': '486.95', 'y': '710.06', 'h': '17.05', 'w': '9.96'}, {'page': '17', 'x': '108.00', 'y': '723.38', 'h': '215.44', 'w': '8.64'}, {'page': '17', 'x': '327.90', 'y': '722.14', 'h': '20.48', 'w': '17.29'}, {'page': '17', 'x': '350.04', 'y': '722.39', 'h': '9.96', 'w': '9.96'}, {'page': '17', 'x': '360.01', 'y': '721.02', 'h': '25.34', 'w': '8.05'}, {'page': '17', 'x': '385.85', 'y': '723.38', 'h': '2.49', 'w': '8.64'}], [{'page': '17', 'x': '397.34', 'y': '722.39', 'h': '106.67', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '85.34', 'h': '123.98', 'w': '8.64'}], [{'page': '18', 'x': '239.23', 'y': '84.35', 'h': '181.07', 'w': '9.96'}, {'page': '18', 'x': '425.91', 'y': '84.10', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '438.98', 'y': '84.35', 'h': '65.02', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '96.69', 'h': '6.37', 'w': '9.96'}, {'page': '18', 'x': '117.18', 'y': '96.44', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '126.59', 'y': '96.69', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '139.87', 'y': '96.44', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '153.71', 'y': '95.31', 'h': '4.92', 'w': '6.97'}, {'page': '18', 'x': '159.13', 'y': '97.67', 'h': '2.49', 'w': '8.64'}], [{'page': '18', 'x': '164.61', 'y': '96.69', 'h': '54.84', 'w': '9.96'}, {'page': '18', 'x': '219.59', 'y': '101.36', 'h': '3.97', 'w': '6.97'}, {'page': '18', 'x': '224.06', 'y': '96.69', 'h': '13.73', 'w': '9.96'}, {'page': '18', 'x': '238.95', 'y': '96.44', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '247.85', 'y': '96.69', 'h': '196.00', 'w': '9.96'}, {'page': '18', 'x': '446.06', 'y': '96.44', 'h': '20.48', 'w': '17.29'}, {'page': '18', 'x': '468.19', 'y': '96.69', 'h': '9.96', 'w': '9.96'}, {'page': '18', 'x': '478.16', 'y': '95.31', 'h': '25.35', 'w': '8.05'}, {'page': '18', 'x': '108.00', 'y': '107.64', 'h': '396.00', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '118.60', 'h': '115.94', 'w': '9.96'}, {'page': '18', 'x': '229.40', 'y': '118.35', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '241.82', 'y': '118.60', 'h': '186.58', 'w': '9.96'}, {'page': '18', 'x': '433.07', 'y': '118.35', 'h': '6.64', 'w': '17.29'}, {'page': '18', 'x': '444.39', 'y': '118.60', 'h': '11.62', 'w': '9.96'}, {'page': '18', 'x': '457.67', 'y': '118.35', 'h': '13.84', 'w': '17.29'}, {'page': '18', 'x': '475.03', 'y': '119.59', 'h': '28.97', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '128.74', 'h': '41.82', 'w': '12.48'}], [{'page': '18', 'x': '156.60', 'y': '131.26', 'h': '226.47', 'w': '9.96'}, {'page': '18', 'x': '388.27', 'y': '131.01', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '400.13', 'y': '131.26', 'h': '103.87', 'w': '9.96'}, {'page': '18', 'x': '108.00', 'y': '142.22', 'h': '65.24', 'w': '9.96'}, {'page': '18', 'x': '177.48', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '188.37', 'y': '142.22', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '213.25', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '224.15', 'y': '142.22', 'h': '50.63', 'w': '9.96'}, {'page': '18', 'x': '277.92', 'y': '141.97', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '288.82', 'y': '142.22', 'h': '7.47', 'w': '9.96'}], [{'page': '18', 'x': '303.18', 'y': '143.21', 'h': '200.83', 'w': '8.64'}, {'page': '18', 'x': '109.51', 'y': '152.36', 'h': '59.88', 'w': '12.48'}, {'page': '18', 'x': '172.69', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '182.66', 'y': '154.88', 'h': '21.73', 'w': '9.96'}, {'page': '18', 'x': '206.59', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '216.56', 'y': '154.88', 'h': '63.42', 'w': '9.96'}, {'page': '18', 'x': '282.19', 'y': '154.63', 'h': '7.75', 'w': '17.29'}, {'page': '18', 'x': '292.15', 'y': '154.88', 'h': '7.47', 'w': '9.96'}]]","text":"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \u223c Discrete(\u03b1) for \u03b1 \u2208 (0, \u221e) n .If C is the log 2 (n) \u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \u223c Concrete(\u03b1, \u03bb) for some fixed temperature \u03bb \u2208 (0, \u221e) and set \u1ef8 = CX.For the binary case, this amounts to simply sampling U \u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \u03b1) -1.The corresponding Binary Concrete relaxation is \u1ef8 = 2\u03c3((log Ulog(1 -U ) + log \u03b1)\/\u03bb) -1.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/1611.00712.pdf","section_number":"None","para":"5","_id":"3b42039c-697d-41c1-bc3c-c8534474c72f","_collection_name":"Papers"},"page_content":"All our models are neural networks with layers of n-ary discrete stochastic nodes with log 2 (n)dimensional states on the corners of the hypercube {-1, 1} log 2 (n) .For a generic n-ary node sampling proceeds as follows.Sample a n-ary discrete random variable D \u223c Discrete(\u03b1) for \u03b1 \u2208 (0, \u221e) n .If C is the log 2 (n) \u00d7 n matrix, which lists the corners of the hypercube {-1, 1} log 2 (n) as columns, then we took Y = CD as downstream computation on D. The corresponding Concrete relaxation is to take X \u223c Concrete(\u03b1, \u03bb) for some fixed temperature \u03bb \u2208 (0, \u221e) and set \u1ef8 = CX.For the binary case, this amounts to simply sampling U \u223c Uniform(0, 1) and taking Y = 2H(log Ulog(1 -U ) + log \u03b1) -1.The corresponding Binary Concrete relaxation is \u1ef8 = 2\u03c3((log Ulog(1 -U ) + log \u03b1)\/\u03bb) -1.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).","paper_title":"Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge","section_title":"Overview of evaluated algorithms","bboxes":"[[{'page': '4', 'x': '62.25', 'y': '483.25', 'h': '421.80', 'w': '9.14'}], [{'page': '4', 'x': '487.76', 'y': '483.25', 'h': '76.24', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '494.79', 'h': '176.57', 'w': '9.14'}], [{'page': '4', 'x': '228.04', 'y': '494.79', 'h': '335.96', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '506.33', 'h': '516.00', 'w': '9.14'}], [{'page': '4', 'x': '48.00', 'y': '517.87', 'h': '516.00', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '529.41', 'h': '291.38', 'w': '9.14'}], [{'page': '4', 'x': '342.79', 'y': '529.41', 'h': '221.22', 'w': '9.14'}, {'page': '4', 'x': '48.00', 'y': '540.95', 'h': '250.91', 'w': '9.14'}]]","pages":"('4', '4')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2308.05862.pdf","section_number":"None","para":"4","_id":"644f09e8-fa9c-4d98-84d7-0a6761c90cd5","_collection_name":"Papers"},"page_content":"We analyzed the characteristics of the employed algorithms by participants (Fig. 1e-g, Table 4-5).All the submitted algorithms were based on deep learning.80% teams used U-Net as the main network architecture (Fig. 1e), where 45% teams used plain U-Net and others combined U-Net with popular networks, such as ResNet (23%) and Transformer [21].All the algorithms used dice loss [22] and its combination with cross-entropy loss was the most popular choice (Fig. 1f) because compound loss function has been proven to be robust [23].Adam [24] was the most frequently used optimizer followed by the stochastic gradient descent (SGD) (Fig. 1g).","type":"Document"}],0.9521965981,20,4.4,4.9,4,"The output directly answers the question about the computational setup for training the deep network, providing specific details about the training duration and hardware used. It aligns well with the context provided, which discusses the training of deep networks for solving Rubik's Cube variants. However, it could be improved by including more context about the specific networks or methods used, which would elevate it to a score of 5."],["What is the term for the near-perfect agents in ConnectFour that use alpha-beta search and extend the Minimax algorithm by pruning the search tree efficiently?"," AB and AB-DL",[{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"B. ConnectFour","bboxes":"[[{'page': '6', 'x': '58.93', 'y': '188.88', 'h': '241.09', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '200.84', 'h': '51.22', 'w': '8.64'}], [{'page': '6', 'x': '106.29', 'y': '200.84', 'h': '193.73', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '212.79', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '224.75', 'h': '120.48', 'w': '8.64'}], [{'page': '6', 'x': '174.75', 'y': '224.75', 'h': '125.27', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '236.70', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '248.66', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '260.61', 'h': '93.08', 'w': '8.64'}], [{'page': '6', 'x': '144.40', 'y': '260.61', 'h': '155.63', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '272.57', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '284.21', 'h': '150.58', 'w': '8.96'}, {'page': '6', 'x': '199.54', 'y': '282.63', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '211.20', 'y': '284.52', 'h': '88.83', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '296.16', 'h': '39.64', 'w': '8.96'}, {'page': '6', 'x': '88.60', 'y': '294.59', 'h': '9.07', 'w': '6.12'}, {'page': '6', 'x': '101.12', 'y': '296.16', 'h': '72.46', 'w': '8.96'}, {'page': '6', 'x': '173.58', 'y': '294.59', 'h': '6.77', 'w': '6.12'}, {'page': '6', 'x': '183.79', 'y': '296.48', 'h': '116.23', 'w': '8.64'}], [{'page': '6', 'x': '48.96', 'y': '308.43', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '320.39', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '332.34', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '344.30', 'h': '85.96', 'w': '8.64'}], [{'page': '6', 'x': '137.44', 'y': '344.30', 'h': '162.58', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '356.26', 'h': '251.06', 'w': '8.64'}, {'page': '6', 'x': '48.96', 'y': '368.21', 'h': '132.20', 'w': '8.64'}]]","pages":"('6', '6')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"5","_id":"e4bedff0-a76e-49a4-95b5-12fc0fb5fac1","_collection_name":"Papers"},"page_content":"ConnectFour is a non-trivial game that is not easy to master for humans.However, its medium-size complexity allows for very strong tree-based solutions when combined with a pre-computed opening book.These near-perfect agents are termed AB and AB-DL since they are based on alpha-beta search (AB) that extends the Minimax algorithm by efficiently pruning the search tree.Thill et al. [22] were able to implement alpha-beta search for ConnectFour in such a way that it plays near-perfect: It wins all games as 1 st player and wins very often as 2 nd player when the 1 st player makes a wrong move.AB and AB-DL differ in the way they react to losing states: While AB just takes a random move, AB-DL searches for the move, which postpones the loss as far (as distant) as possible (DL = distant losses).It is tougher to win against AB-DL since it will request more correct moves from the opponent and will very often punish wrong moves.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('4', '4')","paper_title":"Present-Biased Optimization *","section_title":"Our Results","bboxes":"[[{'page': '4', 'x': '72.00', 'y': '449.82', 'h': '183.75', 'w': '10.91'}], [{'page': '4', 'x': '266.66', 'y': '449.82', 'h': '257.25', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '463.37', 'h': '451.92', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '476.92', 'h': '257.38', 'w': '10.91'}, {'page': '4', 'x': '333.67', 'y': '476.64', 'h': '11.69', 'w': '18.93'}, {'page': '4', 'x': '345.36', 'y': '474.93', 'h': '4.23', 'w': '13.82'}, {'page': '4', 'x': '345.36', 'y': '482.04', 'h': '4.23', 'w': '7.97'}, {'page': '4', 'x': '350.09', 'y': '476.92', 'h': '17.58', 'w': '10.91'}], [{'page': '4', 'x': '369.49', 'y': '474.93', 'h': '15.31', 'w': '13.82'}, {'page': '4', 'x': '380.57', 'y': '482.44', 'h': '4.41', 'w': '7.97'}, {'page': '4', 'x': '385.69', 'y': '476.64', 'h': '8.48', 'w': '18.93'}, {'page': '4', 'x': '398.62', 'y': '476.92', 'h': '36.54', 'w': '10.91'}, {'page': '4', 'x': '449.20', 'y': '476.64', 'h': '8.49', 'w': '18.93'}, {'page': '4', 'x': '461.80', 'y': '476.92', 'h': '9.05', 'w': '10.91'}], [{'page': '4', 'x': '477.65', 'y': '476.92', 'h': '46.26', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '490.47', 'h': '451.57', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '504.02', 'h': '295.18', 'w': '10.91'}], [{'page': '4', 'x': '374.45', 'y': '504.02', 'h': '149.47', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '517.57', 'h': '451.91', 'w': '10.91'}, {'page': '4', 'x': '72.00', 'y': '531.11', 'h': '359.73', 'w': '10.91'}]]","text":"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2012.14736.pdf","section_number":"1.1","para":"4","_id":"c21ccac4-4cd2-470f-8c53-1633b726ac8f","_collection_name":"Papers"},"page_content":"Minimization & underestimation.In the original setting of minimization problems, with underestimation of future costs (i.e., \u03b2 < 1), we show that the cost ratio of an agent performing k steps, that is, computes a feasible solution {x * 1 , . . ., x * k }, satisfies \u2264 k.This is in contrast to the general model in [11], in which an agent can incur a cost ratio exponential in k when returning a k-edge path from the source to the target.Hence, in particular, our minimization scenarios do not produce the worst cases examples constructed in [11], i.e., obtained by considering travels from sources to targets in arbitrary weighted graphs.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.","paper_title":"Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach","section_title":"Deriving tailored policies using past graph instances:","bboxes":"[[{'page': '2', 'x': '72.00', 'y': '443.01', 'h': '452.49', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '454.96', 'h': '468.11', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '466.92', 'h': '51.77', 'w': '9.96'}], [{'page': '2', 'x': '128.20', 'y': '466.92', 'h': '408.15', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '478.87', 'h': '103.59', 'w': '9.96'}], [{'page': '2', 'x': '180.03', 'y': '478.87', 'h': '358.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '490.83', 'h': '288.82', 'w': '9.96'}], [{'page': '2', 'x': '365.25', 'y': '490.83', 'h': '172.94', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '502.78', 'h': '305.77', 'w': '9.96'}], [{'page': '2', 'x': '382.20', 'y': '502.78', 'h': '146.84', 'w': '9.96'}, {'page': '2', 'x': '72.00', 'y': '514.74', 'h': '466.95', 'w': '9.96'}]]","pages":"('2', '2')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2109.10380.pdf","section_number":"None","para":"4","_id":"6eea14ef-5fe6-41e3-8890-a98f462fa226","_collection_name":"Papers"},"page_content":"Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V could correspond to social media users whose demographic information could be used to understand their preferences.Existing algorithms are limited in considering such node features that could be leveraged to obtain better solutions.For instance, the RL agent may learn that connecting a node v with a particular set of attributes to a specific node in U would yield high returns.The proposed framework can naturally account for such attributes, going beyond simple greedy-like policies.We will show that accounting for node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('9', '10')","paper_title":"Stable Dinner Party Seating Arrangements","section_title":"Polynomial Solvability for k-Class Preferences","bboxes":"[[{'page': '9', 'x': '149.71', 'y': '357.18', 'h': '146.38', 'w': '8.80'}], [{'page': '9', 'x': '299.99', 'y': '357.18', 'h': '180.60', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '369.13', 'h': '243.34', 'w': '9.35'}], [{'page': '9', 'x': '381.42', 'y': '369.13', 'h': '99.17', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '381.09', 'h': '191.55', 'w': '8.80'}], [{'page': '9', 'x': '327.98', 'y': '381.09', 'h': '99.35', 'w': '9.71'}], [{'page': '9', 'x': '428.99', 'y': '381.09', 'h': '51.61', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '393.04', 'h': '302.37', 'w': '9.71'}], [{'page': '9', 'x': '438.54', 'y': '393.10', 'h': '42.06', 'w': '9.29'}], [{'page': '9', 'x': '134.77', 'y': '405.00', 'h': '345.83', 'w': '10.08'}, {'page': '9', 'x': '134.77', 'y': '416.95', 'h': '345.83', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '428.91', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '440.86', 'h': '272.33', 'w': '8.80'}], [{'page': '9', 'x': '412.30', 'y': '440.86', 'h': '68.29', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '452.82', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '464.78', 'h': '328.70', 'w': '8.80'}], [{'page': '9', 'x': '466.76', 'y': '464.78', 'h': '13.84', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '476.73', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '488.69', 'h': '345.82', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '500.64', 'h': '272.90', 'w': '8.80'}], [{'page': '9', 'x': '412.25', 'y': '500.64', 'h': '68.33', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '512.60', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '524.55', 'h': '667.36', 'w': '9.71'}, {'page': '9', 'x': '144.06', 'y': '536.51', 'h': '336.54', 'w': '9.71'}], [{'page': '9', 'x': '134.77', 'y': '548.46', 'h': '345.83', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '560.42', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '572.37', 'h': '345.83', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '584.33', 'h': '72.82', 'w': '8.80'}], [{'page': '9', 'x': '210.65', 'y': '584.33', 'h': '269.94', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '596.28', 'h': '36.71', 'w': '8.80'}], [{'page': '9', 'x': '175.54', 'y': '596.28', 'h': '305.06', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '608.24', 'h': '25.48', 'w': '8.80'}], [{'page': '9', 'x': '163.75', 'y': '608.24', 'h': '316.84', 'w': '9.35'}, {'page': '9', 'x': '134.77', 'y': '620.19', 'h': '345.83', 'w': '9.71'}, {'page': '9', 'x': '134.77', 'y': '632.15', 'h': '176.93', 'w': '9.35'}], [{'page': '9', 'x': '315.80', 'y': '632.15', 'h': '164.78', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '644.10', 'h': '24.43', 'w': '8.80'}], [{'page': '9', 'x': '162.44', 'y': '644.10', 'h': '162.04', 'w': '8.80'}], [{'page': '9', 'x': '327.73', 'y': '644.10', 'h': '152.86', 'w': '8.80'}, {'page': '9', 'x': '134.77', 'y': '656.06', 'h': '217.85', 'w': '8.80'}], [{'page': '10', 'x': '134.77', 'y': '118.92', 'h': '345.83', 'w': '8.81'}, {'page': '10', 'x': '134.77', 'y': '130.89', 'h': '72.29', 'w': '8.80'}]]","text":"We begin with the case of paths.For simplicity, we assume that n \u011b 3, as for n \u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \" n.For ease of writing, we will see arrangements as sequences s \" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \"class a likes class b.\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \" s n`1 \" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \u011b p b pdq `pb pf q and p e pdq pe pf q \u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \u011b p b pdq and p c pdq \u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \"or\" instead of \"and.\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \u010f i \u0103 j \u010f n the triplets ps i\u00b41 , s i , s i`1 q and ps j\u00b41 , s j , s j`1 q are long-range compatible when j \u00b4i \u0105 1 and short-range compatible when j \u00b4i \" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2305.09549.pdf","section_number":"5","para":"18","_id":"31289307-4c78-4fe6-bcd0-a2e5a491816b","_collection_name":"Papers"},"page_content":"We begin with the case of paths.For simplicity, we assume that n \u011b 3, as for n \u010f 2 any arrangement is both stable and envy-free.Assume that the agent classes are identified by the numbers 1, . . ., k and that n 1 , n 2 , . . ., n k are the number of agents of each class in our preference profile, where n 1 `. . .`nk \" n.For ease of writing, we will see arrangements as sequences s \" ps i q iPrns , where s i P rks and for any agent class j P rks the number of values j in s is n j .Moreover, for brevity, we lift agent preferences to class preferences, in order to give meaning to statements such as \"class a likes class b.\"To simplify the treatment of agents sitting at the ends of the path, we introduce two agents of a dummy class 0 with preference values 0 from and towards the other agents.We require the dummy agents to sit at the two ends of the path; i.e., s 0 \" s n`1 \" 0. In order to use a common framework for stability and envy-freeness, we define the concept of compatible triples of agent classes, as follows.First, for envyfreeness, let a, b, c, d, e, f be agent classes, then we say that triples pa, b, cq and pd, e, f q are long-range compatible if p b paq `pb pcq \u011b p b pdq `pb pf q and p e pdq pe pf q \u011b p e paq `pe pcq; intuitively, neither b wants to swap with e, nor vice-versa.Furthermore, for a, b, c, d agent classes, we say that triples pa, b, cq and pb, c, dq are short-range compatible if p b paq \u011b p b pdq and p c pdq \u011b p c paq; intuitively, if a, b, c, d are consecutive in the arrangement, then neither b wants to swap with c, nor vice-versa.For stability, we keep the same definitions but use \"or\" instead of \"and.\"Note that long-range and short-range compatibility do not imply each other.We call an arrangement s compatible if for all 1 \u010f i \u0103 j \u010f n the triplets ps i\u00b41 , s i , s i`1 q and ps j\u00b41 , s j , s j`1 q are long-range compatible when j \u00b4i \u0105 1 and short-range compatible when j \u00b4i \" 1.Note that arrangement s is envy-free (resp.stable) if and only if it is compatible.In the following, we explain how to decide the existence of a compatible arrangement.Lemma 3. Deciding whether compatible arrangements exist can be achieved in polynomial time.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('3', '3')","paper_title":"Potential-based Reward Shaping in Sokoban","section_title":"Reward Shaping","bboxes":"[[{'page': '3', 'x': '149.71', 'y': '353.63', 'h': '330.88', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '365.58', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '377.54', 'h': '59.87', 'w': '8.74'}], [{'page': '3', 'x': '197.02', 'y': '377.54', 'h': '283.57', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '389.49', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '401.45', 'h': '89.98', 'w': '8.74'}], [{'page': '3', 'x': '229.09', 'y': '401.45', 'h': '251.49', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '413.40', 'h': '294.09', 'w': '8.74'}], [{'page': '3', 'x': '431.31', 'y': '413.40', 'h': '49.28', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '425.36', 'h': '345.83', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '437.31', 'h': '345.82', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '449.27', 'h': '60.59', 'w': '8.74'}], [{'page': '3', 'x': '198.21', 'y': '449.27', 'h': '282.38', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '461.22', 'h': '201.36', 'w': '8.74'}]]","text":"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2109.05022.pdf","section_number":"2.1","para":"4","_id":"b7f445e2-861a-4d8a-8f44-abc181529169","_collection_name":"Papers"},"page_content":"Brys et al. extracted potential function from demonstrations by checking if agent's state-action pairs are in demonstrations or not and apply it to Cart Pole and Mario [2].Hussein et al. trained a neural network from demonstrations as the potential function and added it to the original reward function of DQN in grid navigation tasks [12].Grzes provided more insights and analysis for potentialbased reward shaping and extended it to multi-agent RL scenario [7].While most previous methods have focused on extracting potential functions from expert demonstrations, we investigate whether potential functions can also be extracted from a search.In our case, we use the distance function which is provided by the A* search algorithm as the potential function.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Related work","bboxes":"[[{'page': '9', 'x': '58.93', 'y': '604.16', 'h': '241.09', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '616.11', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '628.07', 'h': '251.06', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '640.02', 'h': '119.16', 'w': '9.33'}], [{'page': '9', 'x': '174.57', 'y': '640.02', 'h': '125.45', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '651.98', 'h': '225.34', 'w': '8.64'}], [{'page': '9', 'x': '278.17', 'y': '651.98', 'h': '21.85', 'w': '8.64'}, {'page': '9', 'x': '48.96', 'y': '663.93', 'h': '218.16', 'w': '8.64'}]]","pages":"('9', '9')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"2","_id":"5eb50813-b8dd-45d7-9053-156a4c823d95","_collection_name":"Papers"},"page_content":"Dawson [24] introduces a CNN-based and AlphaZeroinspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS 1000 .Young et al. [32] report on an AlphaZero implementation applied to ConnectFour.Here, training took between 21 and 77 hours of GPU time.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"text":"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.","paper_title":"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning","section_title":"Experimental setup","bboxes":"[[{'page': '4', 'x': '329.46', 'y': '156.11', 'h': '228.53', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '167.07', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '178.03', 'h': '172.99', 'w': '8.64'}], [{'page': '4', 'x': '494.99', 'y': '178.03', 'h': '63.01', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '188.98', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '199.94', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '210.90', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '221.86', 'h': '20.75', 'w': '8.64'}], [{'page': '4', 'x': '343.84', 'y': '221.86', 'h': '214.17', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '232.82', 'h': '238.50', 'w': '8.64'}], [{'page': '4', 'x': '319.50', 'y': '243.78', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '254.74', 'h': '238.50', 'w': '8.64'}, {'page': '4', 'x': '319.50', 'y': '265.70', 'h': '167.01', 'w': '8.64'}]]","pages":"('4', '4')","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2012.12062.pdf","section_number":"None","para":"3","_id":"782393e7-a352-4535-b9f3-c8cb687c936d","_collection_name":"Papers"},"page_content":"To keep the comparison among the different tested algorithms as fair as possible, we ensure that each algorithm uses the same type of hyper-parameters ranging.Specifically, we referred to the authors of QMIX, MAVEN and IQL to determine the set of hyper-parameters and have simply kept the same values for QVMix, QVMix-Max, IQV and IQV-Max.For a more thorough presentation of all used hyperparameters we refer the reader to the open-sourced code2 .As is common practice within the literature, to improve the learning speed of the algorithms, the parameters of the individual networks are shared among agents.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('15', '15')","paper_title":"A T E X template Contextually Aware Intelligent Control Agents for Heterogeneous Swarms","section_title":"Shepherding Agent Behaviours","bboxes":"[[{'page': '15', 'x': '51.02', 'y': '440.17', 'h': '36.14', 'w': '8.74'}, {'page': '15', 'x': '87.17', 'y': '438.60', 'h': '3.01', 'w': '6.12'}, {'page': '15', 'x': '87.17', 'y': '445.03', 'h': '9.53', 'w': '6.12'}, {'page': '15', 'x': '100.91', 'y': '440.17', 'h': '157.75', 'w': '8.74'}], [{'page': '15', 'x': '262.12', 'y': '440.17', 'h': '126.23', 'w': '8.74'}, {'page': '15', 'x': '51.02', 'y': '452.17', 'h': '186.30', 'w': '8.74'}]]","text":"where \u2126 t \u03b2\u03c0 is the set of \u03c0 agents \u03b2 operates on.We summarise the algorithm by Str\u00f6mbom et al. (2014) in Algorithm 2.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2211.12560.pdf","section_number":"4.1","para":"1","_id":"61fec9c2-66a5-46c6-b26d-237b26f59520","_collection_name":"Papers"},"page_content":"where \u2126 t \u03b2\u03c0 is the set of \u03c0 agents \u03b2 operates on.We summarise the algorithm by Str\u00f6mbom et al. (2014) in Algorithm 2.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('2', '2')","paper_title":"Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games","section_title":"Introduction","bboxes":"[[{'page': '2', 'x': '88.94', 'y': '75.86', 'h': '451.07', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '89.41', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '102.77', 'h': '346.67', 'w': '9.64'}], [{'page': '2', 'x': '422.40', 'y': '102.96', 'h': '117.60', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '116.50', 'h': '221.60', 'w': '9.46'}], [{'page': '2', 'x': '296.72', 'y': '116.50', 'h': '243.28', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '130.05', 'h': '468.00', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '143.60', 'h': '78.64', 'w': '9.46'}], [{'page': '2', 'x': '154.29', 'y': '143.60', 'h': '385.71', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '157.15', 'h': '121.14', 'w': '9.46'}], [{'page': '2', 'x': '198.45', 'y': '156.97', 'h': '341.56', 'w': '9.64'}, {'page': '2', 'x': '72.00', 'y': '170.52', 'h': '237.24', 'w': '9.39'}]]","text":"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2303.04865.pdf","section_number":"1","para":"4","_id":"3f9ce1b2-9d4d-4b76-97aa-36a2cc06074f","_collection_name":"Papers"},"page_content":"A promising approach for the design of local and scalable MARL algorithms in competitive settings is to exploit the network structure of practical applications to design algorithms with sample complexity that only depends on the local properties of the network instead of the global state.This approach has recently been successful in the case of cooperative MARL.For example, Qu et al. (2020); Lin et al. (2021); Zhang et al. (2023) provides a scalable localized algorithm with a sample complexity that does not depend on the number of agents.However, to this point, local algorithms that exploit network structure do not exist in the competitive MARL setting.Thus, we ask: Can we design a scalable and local algorithm with finite-time bounds for networked MARL with competitive agents?","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{"pages":"('1', '1')","paper_title":"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time","section_title":"A. Motivation","bboxes":"[[{'page': '1', 'x': '58.93', 'y': '692.03', 'h': '241.09', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '703.98', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '715.94', 'h': '145.60', 'w': '8.64'}], [{'page': '1', 'x': '198.95', 'y': '715.94', 'h': '101.07', 'w': '8.64'}, {'page': '1', 'x': '48.96', 'y': '727.89', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '225.77', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '237.73', 'h': '174.15', 'w': '8.64'}], [{'page': '1', 'x': '491.06', 'y': '237.73', 'h': '71.98', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '249.69', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '261.64', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '273.60', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '285.55', 'h': '45.11', 'w': '8.64'}], [{'page': '1', 'x': '359.83', 'y': '285.55', 'h': '203.21', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '297.51', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '309.46', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '321.42', 'h': '53.79', 'w': '8.64'}], [{'page': '1', 'x': '371.19', 'y': '321.42', 'h': '191.85', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '333.37', 'h': '251.06', 'w': '8.64'}, {'page': '1', 'x': '311.98', 'y': '345.33', 'h': '23.52', 'w': '8.64'}]]","text":"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.","file_path":"\/home\/dakoro\/Github\/Projet_Simplon\/pdfs\/2204.13307.pdf","section_number":"None","para":"4","_id":"412b28e4-f9e3-4960-bdfe-c029c812cab4","_collection_name":"Papers"},"page_content":"However, the full algorithms in [1]- [3] require huge computational resources in order to learn how to play the game of Go at world-master level.It is the purpose of this work to investigate whether some of the important elements of AlphaZero can already reach decent advances in game learning with much smaller computational efforts.For this purpose, we study several games -namely Othello, ConnectFour and Rubik's Cube -that have a lower complexity than Go yet are not easy to master for both humans and game learning algorithms.The goal is to deliver not only agents with average game playing strength but agents that learn from scratch and play almost as well as the strongest known algorithms1 for these games.We will show that this can be achieved for Othello and ConnectFour and, to some extent, also for Rubik's Cube.","type":"Document"}],0.6576969624,5,0.9,-4.3,5,"The output directly answers the question by identifying the near-perfect agents in ConnectFour as AB and AB-DL, which are based on alpha-beta search and the Minimax algorithm. This response is comprehensive and aligns perfectly with the provided context, which elaborates on the characteristics and performance of these agents."]]}